doc	unit1_toks	unit2_toks	unit1_txt	unit2_txt	s1_toks	s2_toks	unit1_sent	unit2_sent	dir	orig_label	label
D14-1051	1-3	4-15	Various studies highlighted	that topic-based approaches give a powerful spoken content representation of documents .	1-15	1-15	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	1>2	attribution	attribution
D14-1051	4-15	36-46	that topic-based approaches give a powerful spoken content representation of documents .	In this study , we propose an original and promising framework	1-15	36-65	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	1>2	bg-compare	bg-compare
D14-1051	4-15	16-27	that topic-based approaches give a powerful spoken content representation of documents .	Nonetheless , these documents may contain more than one main theme ,	1-15	16-35	Various studies highlighted that topic-based approaches give a powerful spoken content representation of documents .	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	1<2	contrast	contrast
D14-1051	16-27	28-35	Nonetheless , these documents may contain more than one main theme ,	and their automatic transcription inevitably contains errors .	16-35	16-35	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	Nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors .	1<2	joint	joint
D14-1051	36-46	47-56	In this study , we propose an original and promising framework	based on a compact representation of a textual document ,	36-65	36-65	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	1<2	bg-general	bg-general
D14-1051	36-46	57-59	In this study , we propose an original and promising framework	to solve issues	36-65	36-65	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	1<2	enablement	enablement
D14-1051	57-59	60-65	to solve issues	related to topic space granularity .	36-65	36-65	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	1<2	elab-addition	elab-addition
D14-1051	36-46	66-83	In this study , we propose an original and promising framework	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	36-65	66-83	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Firstly , various topic spaces are estimated with different numbers of classes from a Latent Dirichlet Allocation .	1<2	elab-process_step	elab-process_step
D14-1051	36-46	84-97	In this study , we propose an original and promising framework	Then , this multiple topic space representation is compacted into an elementary segment ,	36-65	84-109	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	1<2	elab-process_step	elab-process_step
D14-1051	84-97	98-100	Then , this multiple topic space representation is compacted into an elementary segment ,	called c-vector ,	84-109	84-109	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	1<2	elab-addition	elab-addition
D14-1051	84-97	101-109	Then , this multiple topic space representation is compacted into an elementary segment ,	originally developed in the context of speaker recognition .	84-109	84-109	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	Then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition .	1<2	elab-addition	elab-addition
D14-1051	110-119	120-131	Experiments are conducted on the DECODA corpus of conversations .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	110-119	120-131	Experiments are conducted on the DECODA corpus of conversations .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	1>2	manner-means	manner-means
D14-1051	36-46	120-131	In this study , we propose an original and promising framework	Results show the effectiveness of the proposed multi-view compact representation paradigm .	36-65	120-131	In this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity .	Results show the effectiveness of the proposed multi-view compact representation paradigm .	1<2	evaluation	evaluation
D14-1051	120-131	132-148	Results show the effectiveness of the proposed multi-view compact representation paradigm .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points	120-131	132-160	Results show the effectiveness of the proposed multi-view compact representation paradigm .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	1<2	exp-evidence	exp-evidence
D14-1051	132-148	149-152	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points	compared to the baseline	132-160	132-160	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	1<2	comparison	comparison
D14-1051	132-148	153-160	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points	( best single topic space configuration ) .	132-160	132-160	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	Our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .	1<2	elab-addition	elab-addition
D14-1052	1-8	9-31	This paper introduces a model of multiple-instance learning	applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	1-31	1-31	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	1<2	elab-addition	elab-addition
D14-1052	1-8	32-42	This paper introduces a model of multiple-instance learning	Each variable-length text is represented by several independent feature vectors ;	1-31	32-50	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	1<2	elab-addition	elab-addition
D14-1052	32-42	43-50	Each variable-length text is represented by several independent feature vectors ;	one word vector per sentence or paragraph .	32-50	32-50	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	Each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .	1<2	joint	joint
D14-1052	51-59	60-67	For learning from texts with known aspect ratings ,	the model performs multiple-instance regression ( MIR )	51-90	51-90	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	1>2	enablement	enablement
D14-1052	1-8	60-67	This paper introduces a model of multiple-instance learning	the model performs multiple-instance regression ( MIR )	1-31	51-90	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	1<2	elab-process_step	elab-process_step
D14-1052	60-67	68-82	the model performs multiple-instance regression ( MIR )	and assigns importance weights to each of the sentences or paragraphs of a text ,	51-90	51-90	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	1<2	joint	joint
D14-1052	60-67	83-90	the model performs multiple-instance regression ( MIR )	uncovering their contribution to the aspect ratings .	51-90	51-90	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	For learning from texts with known aspect ratings , the model performs multiple-instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .	1<2	elab-addition	elab-addition
D14-1052	1-8	91-96	This paper introduces a model of multiple-instance learning	Next , the model is used	1-31	91-114	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	1<2	elab-process_step	elab-process_step
D14-1052	91-96	97-105	Next , the model is used	to predict aspect ratings in previously unseen texts ,	91-114	91-114	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	1<2	enablement	enablement
D14-1052	97-105	106-114	to predict aspect ratings in previously unseen texts ,	demonstrating interpretability and explanatory power for its predictions .	91-114	91-114	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .	1<2	elab-addition	elab-addition
D14-1052	1-8	115-126	This paper introduces a model of multiple-instance learning	We evaluate the model on seven multi-aspect sentiment analysis data sets ,	1-31	115-154	This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	1<2	evaluation	evaluation
D14-1052	115-126	127-154	We evaluate the model on seven multi-aspect sentiment analysis data sets ,	improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	115-154	115-154	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	We evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .	1<2	result	result
D14-1053	1-6	7-16	We propose a semi-supervised bootstrapping algorithm	for analyzing China's foreign relations from the People's Daily .	1-16	1-16	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	1<2	enablement	enablement
D14-1053	1-6	17-34	We propose a semi-supervised bootstrapping algorithm	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	1-16	17-34	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Our approach addresses sentiment target clustering , subjective lexicons extraction and sentiment prediction in a unified framework .	1<2	elab-addition	elab-addition
D14-1053	35-42	43-54	Different from existing algorithms in the literature ,	time information is considered in our algorithm through a hierarchical bayesian model	35-60	35-60	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	1>2	contrast	contrast
D14-1053	1-6	43-54	We propose a semi-supervised bootstrapping algorithm	time information is considered in our algorithm through a hierarchical bayesian model	1-16	35-60	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	1<2	manner-means	manner-means
D14-1053	43-54	55-60	time information is considered in our algorithm through a hierarchical bayesian model	to guide the bootstrapping approach .	35-60	35-60	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	Different from existing algorithms in the literature , time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach .	1<2	enablement	enablement
D14-1053	1-6	61-71	We propose a semi-supervised bootstrapping algorithm	We are hopeful that our approach can facilitate quantitative political analysis	1-16	61-78	We propose a semi-supervised bootstrapping algorithm for analyzing China's foreign relations from the People's Daily .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	1<2	summary	summary
D14-1053	61-71	72-78	We are hopeful that our approach can facilitate quantitative political analysis	conducted by social scientists and politicians .	61-78	61-78	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians .	1<2	elab-addition	elab-addition
D14-1054	1-16	17-29	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Existing sentiment classification algorithms typically split a sentence as a word sequence ,	1-16	17-60	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad" and " a great deal of " .	1<2	bg-compare	bg-compare
D14-1054	17-29	30-44	Existing sentiment classification algorithms typically split a sentence as a word sequence ,	which does not effectively handle the inconsistent sentiment polarity between a phrase and the words	17-60	17-60	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad" and " a great deal of " .	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad" and " a great deal of " .	1<2	elab-addition	elab-addition
D14-1054	30-44	45-60	which does not effectively handle the inconsistent sentiment polarity between a phrase and the words	it contains , such as " not bad" and " a great deal of " .	17-60	17-60	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad" and " a great deal of " .	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad" and " a great deal of " .	1<2	elab-addition	elab-addition
D14-1054	17-29	61-64	Existing sentiment classification algorithms typically split a sentence as a word sequence ,	We address this issue	17-60	61-86	Existing sentiment classification algorithms typically split a sentence as a word sequence , which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains , such as " not bad" and " a great deal of " .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	1<2	result	result
D14-1054	61-64	65-76	We address this issue	by developing a joint segmentation and classification framework ( JSC ) ,	61-86	61-86	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	1<2	manner-means	manner-means
D14-1054	65-76	77-86	by developing a joint segmentation and classification framework ( JSC ) ,	which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	61-86	61-86	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	1<2	elab-addition	elab-addition
D14-1054	65-76	87-93	by developing a joint segmentation and classification framework ( JSC ) ,	Specifically , we use a log-linear model	61-86	87-115	We address this issue by developing a joint segmentation and classification framework ( JSC ) , which simultaneously conducts sentence segmentation and sentence-level sentiment classification .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	1<2	manner-means	manner-means
D14-1054	87-93	94-99	Specifically , we use a log-linear model	to score each segmentation candidate ,	87-115	87-115	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	1<2	enablement	enablement
D14-1054	87-93	100-109	Specifically , we use a log-linear model	and exploit the phrasal information of top-ranked segmentations as features	87-115	87-115	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	1<2	joint	joint
D14-1054	100-109	110-115	and exploit the phrasal information of top-ranked segmentations as features	to build the sentiment classifier .	87-115	87-115	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	1<2	enablement	enablement
D14-1054	87-93	116-127	Specifically , we use a log-linear model	A marginal log-likelihood objective function is devised for the segmentation model ,	87-115	116-137	Specifically , we use a log-linear model to score each segmentation candidate , and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	1<2	manner-means	manner-means
D14-1054	116-127	128-137	A marginal log-likelihood objective function is devised for the segmentation model ,	which is optimized for enhancing the sentiment classification performance .	116-137	116-137	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	1<2	elab-addition	elab-addition
D14-1054	116-127	138-142	A marginal log-likelihood objective function is devised for the segmentation model ,	The joint model is trained	116-137	138-157	A marginal log-likelihood objective function is devised for the segmentation model , which is optimized for enhancing the sentiment classification performance .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	1<2	elab-addition	elab-addition
D14-1054	138-142	143-152	The joint model is trained	only based on the annotated sentiment polarity of sentences ,	138-157	138-157	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	1<2	bg-general	bg-general
D14-1054	143-152	153-157	only based on the annotated sentiment polarity of sentences ,	without any segmentation annotations .	138-157	138-157	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	The joint model is trained only based on the annotated sentiment polarity of sentences , without any segmentation annotations .	1<2	contrast	contrast
D14-1054	158-169	170-181	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show	that , our joint model performs comparably with the state-of-the-art methods .	158-181	158-181	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	1>2	attribution	attribution
D14-1054	1-16	170-181	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	that , our joint model performs comparably with the state-of-the-art methods .	1-16	158-181	In this paper , we propose a joint segmentation and classification framework for sentiment analysis .	Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that , our joint model performs comparably with the state-of-the-art methods .	1<2	evaluation	evaluation
D14-1055	1-14	29-36	Deceptive reviews detection has attracted significant attention from both business and research communities .	the problem remains to be highly challenging .	1-14	15-36	Deceptive reviews detection has attracted significant attention from both business and research communities .	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	1>2	contrast	contrast
D14-1055	15-23	29-36	However , due to the difficulty of human labeling	the problem remains to be highly challenging .	15-36	15-36	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	1>2	exp-reason	exp-reason
D14-1055	15-23	24-28	However , due to the difficulty of human labeling	needed for supervised learning ,	15-36	15-36	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	1<2	elab-addition	elab-addition
D14-1055	29-36	37-45	the problem remains to be highly challenging .	This paper proposed a novel angle to the problem	15-36	37-54	However , due to the difficulty of human labeling needed for supervised learning , the problem remains to be highly challenging .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	1>2	bg-goal	bg-goal
D14-1055	37-45	46-54	This paper proposed a novel angle to the problem	by modeling PU ( positive unlabeled ) learning .	37-54	37-54	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	1<2	manner-means	manner-means
D14-1055	37-45	55-58	This paper proposed a novel angle to the problem	A semi-supervised model ,	37-54	55-73	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	1<2	elab-addition	elab-addition
D14-1055	55-58	59-61	A semi-supervised model ,	called mixing population	55-73	55-73	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	1<2	elab-addition	elab-addition
D14-1055	55-58	62-73	A semi-supervised model ,	and individual property PU learning ( MPIPUL ) , is proposed .	55-73	55-73	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	A semi-supervised model , called mixing population and individual property PU learning ( MPIPUL ) , is proposed .	1<2	joint	joint
D14-1055	37-45	74-86	This paper proposed a novel angle to the problem	Firstly , some reliable negative examples are identified from the unlabeled dataset .	37-54	74-86	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Firstly , some reliable negative examples are identified from the unlabeled dataset .	1<2	elab-process_step	elab-process_step
D14-1055	37-45	87-97	This paper proposed a novel angle to the problem	Secondly , some representative positive examples and negative examples are generated	37-54	87-106	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	1<2	elab-process_step	elab-process_step
D14-1055	87-97	98-106	Secondly , some representative positive examples and negative examples are generated	based on LDA ( Latent Dirichlet Allocation ) .	87-106	87-106	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	Secondly , some representative positive examples and negative examples are generated based on LDA ( Latent Dirichlet Allocation ) .	1<2	bg-general	bg-general
D14-1055	107-113	133-138	Thirdly , for the remaining unlabeled examples	two similarity weights are assigned ,	107-158	107-158	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	1>2	enablement	enablement
D14-1055	107-113	114-121	Thirdly , for the remaining unlabeled examples	( we call them spy examples ) ,	107-158	107-158	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	1<2	elab-addition	elab-addition
D14-1055	107-113	122-132	Thirdly , for the remaining unlabeled examples	which can not be explicitly identified as positive and negative ,	107-158	107-158	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	1<2	elab-addition	elab-addition
D14-1055	37-45	133-138	This paper proposed a novel angle to the problem	two similarity weights are assigned ,	37-54	107-158	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	1<2	elab-process_step	elab-process_step
D14-1055	133-138	139-146,156-158	two similarity weights are assigned ,	by which the probability of a spy example <*> are displayed .	107-158	107-158	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	1<2	elab-addition	elab-addition
D14-1055	139-146,156-158	147-155	by which the probability of a spy example <*> are displayed .	belonging to the positive class and the negative class	107-158	107-158	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	Thirdly , for the remaining unlabeled examples ( we call them spy examples ) , which can not be explicitly identified as positive and negative , two similarity weights are assigned , by which the probability of a spy example belonging to the positive class and the negative class are displayed .	1<2	elab-addition	elab-addition
D14-1055	37-45	159-175	This paper proposed a novel angle to the problem	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine )	37-54	159-181	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	1<2	elab-process_step	elab-process_step
D14-1055	159-175	176-181	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine )	to build an accurate classifier .	159-181	159-181	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	Finally , spy examples and their similarity weights are incorporated into SVM ( Support Vector Machine ) to build an accurate classifier .	1<2	enablement	enablement
D14-1055	37-45	182-190	This paper proposed a novel angle to the problem	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL	37-54	182-196	This paper proposed a novel angle to the problem by modeling PU ( positive unlabeled ) learning .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	1<2	evaluation	evaluation
D14-1055	182-190	191-196	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL	which outperforms the state-of-the-art baselines .	182-196	182-196	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines .	1<2	elab-addition	elab-addition
D14-1056	1-17	35-39	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We propose a general approach	1-17	35-48	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	We propose a general approach to automatically identify shell content of shell nouns .	1>2	bg-goal	bg-goal
D14-1056	1-17	18-23	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	These nouns themselves are unspecific ,	1-17	18-34	Shell nouns , such as fact and problem , occur frequently in all kinds of texts .	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	1<2	elab-addition	elab-addition
D14-1056	18-23	24-34	These nouns themselves are unspecific ,	and can only be interpreted together with the shell content .	18-34	18-34	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	These nouns themselves are unspecific , and can only be interpreted together with the shell content .	1<2	joint	joint
D14-1056	35-39	40-48	We propose a general approach	to automatically identify shell content of shell nouns .	35-48	35-48	We propose a general approach to automatically identify shell content of shell nouns .	We propose a general approach to automatically identify shell content of shell nouns .	1<2	enablement	enablement
D14-1056	35-39	49-53	We propose a general approach	Our approach exploits lexico-syntactic knowledge	35-48	49-59	We propose a general approach to automatically identify shell content of shell nouns .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	1<2	elab-addition	elab-addition
D14-1056	49-53	54-59	Our approach exploits lexico-syntactic knowledge	derived from the linguistics literature .	49-59	49-59	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	Our approach exploits lexico-syntactic knowledge derived from the linguistics literature .	1<2	elab-addition	elab-addition
D14-1056	35-39	60-76	We propose a general approach	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations ,	35-48	60-103	We propose a general approach to automatically identify shell content of shell nouns .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	1<2	evaluation	evaluation
D14-1056	60-76	77-103	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations ,	achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	60-103	60-103	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations , achieving accuracies in the range of 62 % ( baseline = 33 % ) to 83 % ( baseline = 74 % ) on crowd-annotated data .	1<2	result	result
D14-1057	1-9	10-21	We present a comparison of different selectional preference models	and evaluate them on an automatic verb classification task in German .	1-21	1-21	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	1<2	joint	joint
D14-1057	22-23	24-27,30-35	We find	that all the models <*> are effective for verb clustering ;	22-53	22-53	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	1>2	attribution	attribution
D14-1057	1-9	24-27,30-35	We present a comparison of different selectional preference models	that all the models <*> are effective for verb clustering ;	1-21	22-53	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	1<2	elab-aspect	elab-aspect
D14-1057	24-27,30-35	28-29	that all the models <*> are effective for verb clustering ;	we compare	22-53	22-53	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	1<2	elab-addition	elab-addition
D14-1057	1-9	36-41	We present a comparison of different selectional preference models	the best-performing model uses syntactic information	1-21	22-53	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	1<2	elab-aspect	elab-aspect
D14-1057	36-41	42-53	the best-performing model uses syntactic information	to induce nouns classes from unlabelled data in an unsupervised manner .	22-53	22-53	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	We find that all the models we compare are effective for verb clustering ; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .	1<2	enablement	enablement
D14-1057	1-9	54-57,62-68	We present a comparison of different selectional preference models	A very simple model <*> is also found to perform well .	1-21	54-68	We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .	A very simple model based on lexical preferences is also found to perform well .	1<2	elab-aspect	elab-aspect
D14-1057	54-57,62-68	58-61	A very simple model <*> is also found to perform well .	based on lexical preferences	54-68	54-68	A very simple model based on lexical preferences is also found to perform well .	A very simple model based on lexical preferences is also found to perform well .	1<2	bg-general	bg-general
D14-1058	1-6	7-15	This paper presents a novel approach	to learning to solve simple arithmetic word problems .	1-15	1-15	This paper presents a novel approach to learning to solve simple arithmetic word problems .	This paper presents a novel approach to learning to solve simple arithmetic word problems .	1<2	enablement	enablement
D14-1058	1-6	16-29	This paper presents a novel approach	Our system , ARIS , analyzes each of the sentences in the problem state-ment	1-15	16-38	This paper presents a novel approach to learning to solve simple arithmetic word problems .	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	1<2	elab-process_step	elab-process_step
D14-1058	16-29	30-38	Our system , ARIS , analyzes each of the sentences in the problem state-ment	to identify the relevant variables and their values .	16-38	16-38	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	Our system , ARIS , analyzes each of the sentences in the problem state-ment to identify the relevant variables and their values .	1<2	enablement	enablement
D14-1058	1-6	39-46	This paper presents a novel approach	ARIS then maps this information into an equation	1-15	39-64	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	1<2	elab-process_step	elab-process_step
D14-1058	39-46	47-51	ARIS then maps this information into an equation	that represents the problem ,	39-64	39-64	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	1<2	elab-addition	elab-addition
D14-1058	39-46	52-58	ARIS then maps this information into an equation	and enables its ( trivial ) solution	39-64	39-64	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	1<2	joint	joint
D14-1058	52-58	59-64	and enables its ( trivial ) solution	as shown in Figure 1 .	39-64	39-64	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	ARIS then maps this information into an equation that represents the problem , and enables its ( trivial ) solution as shown in Figure 1 .	1<2	elab-addition	elab-addition
D14-1058	1-6	65-72	This paper presents a novel approach	The paper analyzes the arithmetic-word problems "genre" ,	1-15	65-82	This paper presents a novel approach to learning to solve simple arithmetic word problems .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	1<2	elab-addition	elab-addition
D14-1058	65-72	73-77	The paper analyzes the arithmetic-word problems "genre" ,	identifying seven categories of verbs	65-82	65-82	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	1<2	elab-addition	elab-addition
D14-1058	73-77	78-82	identifying seven categories of verbs	used in such problems .	65-82	65-82	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	The paper analyzes the arithmetic-word problems "genre" , identifying seven categories of verbs used in such problems .	1<2	elab-addition	elab-addition
D14-1058	1-6	83-92	This paper presents a novel approach	ARIS learns to categorize verbs with 81.2 % accuracy ,	1-15	83-112	This paper presents a novel approach to learning to solve simple arithmetic word problems .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	1<2	evaluation	evaluation
D14-1058	83-92	93-112	ARIS learns to categorize verbs with 81.2 % accuracy ,	and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	83-112	83-112	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	ARIS learns to categorize verbs with 81.2 % accuracy , and is able to solve 77.7 % of the problems in a corpus of standard primary school test questions .	1<2	joint	joint
D14-1058	1-6	113-121	This paper presents a novel approach	We report the first learning results on this task	1-15	113-133	This paper presents a novel approach to learning to solve simple arithmetic word problems .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	1<2	elab-addition	elab-addition
D14-1058	113-121	122-126	We report the first learning results on this task	without reliance on pre-defined templates	113-133	113-133	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	1<2	elab-addition	elab-addition
D14-1058	113-121	127-133	We report the first learning results on this task	and make our data publicly available .	113-133	113-133	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available .	1<2	joint	joint
D14-1059	1-18	19-27	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We propose NaturalLI : a Natural Logic inference system	1-18	19-54	Common-sense reasoning is important for AI applications , both in NLP and many vision and robotics tasks .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	1>2	bg-goal	bg-goal
D14-1059	19-27	28-32	We propose NaturalLI : a Natural Logic inference system	for inferring common sense facts	19-54	19-54	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	1<2	enablement	enablement
D14-1059	28-32	33-40	for inferring common sense facts	- for instance , that cats have tails	19-54	19-54	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	1<2	elab-example	elab-example
D14-1059	33-40	41-44	- for instance , that cats have tails	or tomatoes are round	19-54	19-54	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	1<2	joint	joint
D14-1059	28-32	45-54	for inferring common sense facts	- from a very large database of known facts .	19-54	19-54	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	1<2	elab-addition	elab-addition
D14-1059	19-27	55-65	We propose NaturalLI : a Natural Logic inference system	In addition to being able to provide strictly valid derivations ,	19-54	55-85	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	1<2	elab-addition	elab-addition
D14-1059	55-65	66-73	In addition to being able to provide strictly valid derivations ,	the system is also able to produce derivations	55-85	55-85	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	1<2	progression	progression
D14-1059	66-73	74-79	the system is also able to produce derivations	which are only likely valid ,	55-85	55-85	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	1<2	elab-addition	elab-addition
D14-1059	66-73	80-85	the system is also able to produce derivations	accompanied by an associated confidence .	55-85	55-85	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	In addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence .	1<2	elab-addition	elab-addition
D14-1059	86-88	89-105	We both show	that our system is able to capture strict Natural Logic inferences on the FraCaS test suite ,	86-123	86-123	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	1>2	attribution	attribution
D14-1059	19-27	89-105	We propose NaturalLI : a Natural Logic inference system	that our system is able to capture strict Natural Logic inferences on the FraCaS test suite ,	19-54	86-123	We propose NaturalLI : a Natural Logic inference system for inferring common sense facts - for instance , that cats have tails or tomatoes are round - from a very large database of known facts .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	1<2	evaluation	evaluation
D14-1059	89-105	106-109	that our system is able to capture strict Natural Logic inferences on the FraCaS test suite ,	and demonstrate its ability	86-123	86-123	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	1<2	joint	joint
D14-1059	106-109	110-123	and demonstrate its ability	to predict common sense facts with 49 % recall and 91 % precision .	86-123	86-123	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	We both show that our system is able to capture strict Natural Logic inferences on the FraCaS test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .	1<2	enablement	enablement
D14-1060	1-18	19-35	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	In this paper , we investigate three issues of term translation in the context of document-informed SMT	1-18	19-111	Term translation is of great importance for statistical machine translation ( SMT ) , especially document-informed SMT .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1>2	bg-goal	bg-goal
D14-1060	19-35	36-41	In this paper , we investigate three issues of term translation in the context of document-informed SMT	and propose three corresponding models :	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	progression	progression
D14-1060	36-41	42-49	and propose three corresponding models :	( a ) a term translation disambiguation model	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-enumember	elab-enumember
D14-1060	42-49	50-63	( a ) a term translation disambiguation model	which selects desirable translations for terms in the source language with domain information ,	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-addition	elab-addition
D14-1060	36-41	64-71	and propose three corresponding models :	( b ) a term translation consistency model	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-enumember	elab-enumember
D14-1060	64-71	72-88	( b ) a term translation consistency model	that encourages consistent translations for terms with a high strength of translation consistency throughout a document ,	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-addition	elab-addition
D14-1060	36-41	89-96	and propose three corresponding models :	and ( c ) a term bracketing model	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-enumember	elab-enumember
D14-1060	89-96	97-100	and ( c ) a term bracketing model	that rewards translation hypotheses	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-addition	elab-addition
D14-1060	97-100	101-111	that rewards translation hypotheses	where bracketable source terms are translated as a whole unit .	19-111	19-111	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	1<2	elab-addition	elab-addition
D14-1060	36-41	112-120	and propose three corresponding models :	We integrate the three models into hierarchical phrase-based SMT	19-111	112-134	In this paper , we investigate three issues of term translation in the context of document-informed SMT and propose three corresponding models : ( a ) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information , ( b ) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document , and ( c ) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	1<2	elab-addition	elab-addition
D14-1060	112-120	121-134	We integrate the three models into hierarchical phrase-based SMT	and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	112-134	112-134	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	1<2	joint	joint
D14-1060	135-137	138-149	Experiment results show	that all three models can achieve significant improvements over the baseline .	135-149	135-149	Experiment results show that all three models can achieve significant improvements over the baseline .	Experiment results show that all three models can achieve significant improvements over the baseline .	1>2	attribution	attribution
D14-1060	121-134	138-149	and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	that all three models can achieve significant improvements over the baseline .	112-134	135-149	We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese-English translation tasks with large-scale training data .	Experiment results show that all three models can achieve significant improvements over the baseline .	1<2	result	result
D14-1060	138-149	150-157	that all three models can achieve significant improvements over the baseline .	Additionally , we can obtain a further improvement	135-149	150-163	Experiment results show that all three models can achieve significant improvements over the baseline .	Additionally , we can obtain a further improvement when combining the three models .	1<2	result	result
D14-1060	150-157	158-163	Additionally , we can obtain a further improvement	when combining the three models .	150-163	150-163	Additionally , we can obtain a further improvement when combining the three models .	Additionally , we can obtain a further improvement when combining the three models .	1<2	temporal	temporal
D14-1061	1-5	15-19	Inspired by previous work ,	we propose a new idea	1-31	1-31	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	1>2	bg-general	bg-general
D14-1061	1-5	6-9	Inspired by previous work ,	where decipherment is used	1-31	1-31	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	1<2	elab-addition	elab-addition
D14-1061	6-9	10-14	where decipherment is used	to improve machine translation ,	1-31	1-31	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	1<2	enablement	enablement
D14-1061	15-19	20-31	we propose a new idea	to combine word alignment and decipherment into a single learning process .	1-31	1-31	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	1<2	enablement	enablement
D14-1061	15-19	32-34	we propose a new idea	We use EM	1-31	32-56	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	1<2	manner-means	manner-means
D14-1061	32-34	35-40	We use EM	to estimate the model parameters ,	32-56	32-56	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	1<2	enablement	enablement
D14-1061	35-40	41-50	to estimate the model parameters ,	not only to maximize the probability of parallel corpus ,	32-56	32-56	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	1<2	enablement	enablement
D14-1061	41-50	51-56	not only to maximize the probability of parallel corpus ,	but also the monolingual corpus .	32-56	32-56	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	We use EM to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus .	1<2	progression	progression
D14-1061	57-60	78-94	We apply our approach	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	57-77	78-94	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	1>2	manner-means	manner-means
D14-1061	57-60	61-66	We apply our approach	to improve Malagasy-English machine translation ,	57-77	57-77	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	1<2	enablement	enablement
D14-1061	61-66	67-77	to improve Malagasy-English machine translation ,	where only a small amount of parallel data is available .	57-77	57-77	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	We apply our approach to improve Malagasy-English machine translation , where only a small amount of parallel data is available .	1<2	elab-addition	elab-addition
D14-1061	15-19	78-94	we propose a new idea	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	1-31	78-94	Inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process .	In our experiments , we observe gains of 0.9 to 2.1 Bleu over a strong baseline .	1<2	evaluation	evaluation
D14-1062	1-2,8-10	11-22	Phrase-based models <*> can be sub-optimal.	In this paper we equip phrase-based models with a latent domain variable	1-10	11-40	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	1>2	bg-compare	bg-compare
D14-1062	1-2,8-10	3-7	Phrase-based models <*> can be sub-optimal.	directly trained on mix-of-domain corpora	1-10	1-10	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal.	1<2	elab-addition	elab-addition
D14-1062	11-22	23-27	In this paper we equip phrase-based models with a latent domain variable	and present a novel method	11-40	11-40	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	1<2	progression	progression
D14-1062	23-27	28-34	and present a novel method	for adapting them to an in-domain task	11-40	11-40	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	1<2	enablement	enablement
D14-1062	28-34	35-40	for adapting them to an in-domain task	represented by a seed corpus .	11-40	11-40	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	1<2	elab-addition	elab-addition
D14-1062	11-22	41-45	In this paper we equip phrase-based models with a latent domain variable	We derive an EM algorithm	11-40	41-68	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	1<2	elab-addition	elab-addition
D14-1062	41-45	46-60	We derive an EM algorithm	which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs	41-68	41-68	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	1<2	elab-addition	elab-addition
D14-1062	46-60	61-68	which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs	reflecting their relevance for the in-domain task .	41-68	41-68	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task .	1<2	elab-addition	elab-addition
D14-1062	69-79	87-103	By embedding our latent domain phrase model in a sentence-level model	we are able to adapt all core translation components together - phrase , lexical and reordering .	69-103	69-103	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	1>2	manner-means	manner-means
D14-1062	69-79	80-86	By embedding our latent domain phrase model in a sentence-level model	and training the two in tandem ,	69-103	69-103	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	1<2	joint	joint
D14-1062	11-22	87-103	In this paper we equip phrase-based models with a latent domain variable	we are able to adapt all core translation components together - phrase , lexical and reordering .	11-40	69-103	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	By embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together - phrase , lexical and reordering .	1<2	elab-addition	elab-addition
D14-1062	11-22	104-119	In this paper we equip phrase-based models with a latent domain variable	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models ,	11-40	104-127	In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	1<2	evaluation	evaluation
D14-1062	104-119	120-127	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models ,	showing significant performance improvement in both tasks .	104-127	104-127	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .	1<2	elab-addition	elab-addition
D14-1063	1-31	68-81	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	1-31	68-81	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	But it is also possible to describe Phrase-Based Machine Translation in this framework .	1>2	contrast	contrast
D14-1063	1-31	32-43	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation	1-31	32-67	In Corpus-Based Machine Translation , the search space of the translation candidates for a given input sentence is often defined by a set of ( cycle-free ) context-free grammar rules .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	1<2	elab-addition	elab-addition
D14-1063	32-43	44-59	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation	( where the representation will be the set of the target-side half of the synchronous rules	32-67	32-67	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	1<2	elab-addition	elab-addition
D14-1063	44-59	60-67	( where the representation will be the set of the target-side half of the synchronous rules	used to parse the input sentence ) .	32-67	32-67	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation ( where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence ) .	1<2	elab-addition	elab-addition
D14-1063	68-81	82-89	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We propose a natural extension to this representation	68-81	82-106	But it is also possible to describe Phrase-Based Machine Translation in this framework .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	1>2	bg-goal	bg-goal
D14-1063	82-89	90-92	We propose a natural extension to this representation	by using lattice-rules	82-106	82-106	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	1<2	manner-means	manner-means
D14-1063	90-92	93-106	by using lattice-rules	that allow to easily encode an exponential number of variations of each rules .	82-106	82-106	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	1<2	elab-addition	elab-addition
D14-1063	107-109	110-123	We also demonstrate	how the representation of the search space has an impact on decoding efficiency ,	107-133	107-133	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	1>2	attribution	attribution
D14-1063	93-106	110-123	that allow to easily encode an exponential number of variations of each rules .	how the representation of the search space has an impact on decoding efficiency ,	82-106	107-133	We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	1<2	progression	progression
D14-1063	110-123	124-133	how the representation of the search space has an impact on decoding efficiency ,	and how it is possible to optimize this representation .	107-133	107-133	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	We also demonstrate how the representation of the search space has an impact on decoding efficiency , and how it is possible to optimize this representation .	1<2	joint	joint
D14-1064	1-10	11-23	When documents and queries are presented in different languages ,	the common approach is to translate the query into the document language .	1-23	1-23	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	1>2	temporal	temporal
D14-1064	11-23	51-59	the common approach is to translate the query into the document language .	In this paper , we introduce a novel approach	1-23	51-89	When documents and queries are presented in different languages , the common approach is to translate the query into the document language .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	1>2	bg-compare	bg-compare
D14-1064	24-33	37-50	While there are a variety of query translation approaches ,	that combining multiple methods into a single "structured query" is the most effective .	24-50	24-50	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	1>2	contrast	contrast
D14-1064	34-36	37-50	recent research suggests	that combining multiple methods into a single "structured query" is the most effective .	24-50	24-50	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	1>2	attribution	attribution
D14-1064	37-50	51-59	that combining multiple methods into a single "structured query" is the most effective .	In this paper , we introduce a novel approach	24-50	51-89	While there are a variety of query translation approaches , recent research suggests that combining multiple methods into a single "structured query" is the most effective .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	1>2	bg-goal	bg-goal
D14-1064	51-59	60-69	In this paper , we introduce a novel approach	for producing a unique combination recipe for each query ,	51-89	51-89	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	1<2	enablement	enablement
D14-1064	70-75	76-89	as it has also been shown	that the optimal combination weights differ substantially across queries and other task specifics .	51-89	51-89	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	1>2	attribution	attribution
D14-1064	60-69	76-89	for producing a unique combination recipe for each query ,	that the optimal combination weights differ substantially across queries and other task specifics .	51-89	51-89	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	1<2	exp-reason	exp-reason
D14-1064	51-59	90-101	In this paper , we introduce a novel approach	Our query-specific combination method generates statistically significant improvements over other combination strategies	51-89	90-113	In this paper , we introduce a novel approach for producing a unique combination recipe for each query , as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	1<2	evaluation	evaluation
D14-1064	90-101	102-113	Our query-specific combination method generates statistically significant improvements over other combination strategies	presented in the literature , such as uniform and task-specific weighting .	90-113	90-113	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	1<2	elab-addition	elab-addition
D14-1064	90-101	114-140	Our query-specific combination method generates statistically significant improvements over other combination strategies	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	90-113	114-140	Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting .	An in-depth empirical analysis presents insights about the effect of data size , domain differences , labeling and tuning on the end performance of our approach .	1<2	elab-addition	elab-addition
D14-1065	1-15	58-67	A major challenge in document clustering research arises from the growing amount of text data	we propose a new document clustering approach for multilingual corpora	1-20	53-108	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1>2	bg-goal	bg-goal
D14-1065	1-15	16-20	A major challenge in document clustering research arises from the growing amount of text data	written in different languages .	1-20	1-20	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	A major challenge in document clustering research arises from the growing amount of text data written in different languages .	1<2	elab-addition	elab-addition
D14-1065	21-26	58-67	Previous approaches depend on language-specific solutions	we propose a new document clustering approach for multilingual corpora	21-52	53-108	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1>2	bg-compare	bg-compare
D14-1065	21-26	27-36	Previous approaches depend on language-specific solutions	( e.g. , bilingual dictionaries , sequential machine translation )	21-52	21-52	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	1<2	elab-example	elab-example
D14-1065	21-26	37-41	Previous approaches depend on language-specific solutions	to evaluate document similarities ,	21-52	21-52	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	1<2	enablement	enablement
D14-1065	21-26	42-52	Previous approaches depend on language-specific solutions	and the required transformations may alter the original document semantics .	21-52	21-52	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	Previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics .	1<2	joint	joint
D14-1065	53-57	58-67	To cope with this issue	we propose a new document clustering approach for multilingual corpora	53-108	53-108	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1>2	enablement	enablement
D14-1065	58-67	68-78	we propose a new document clustering approach for multilingual corpora	that ( i ) exploits a large-scale multilingual knowledge base ,	53-108	53-108	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1<2	elab-aspect	elab-aspect
D14-1065	58-67	79-92	we propose a new document clustering approach for multilingual corpora	( ii ) takes advantage of the multi-topic nature of the text documents ,	53-108	53-108	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1<2	elab-aspect	elab-aspect
D14-1065	58-67	93-100	we propose a new document clustering approach for multilingual corpora	and ( iii ) employs a tensor-based model	53-108	53-108	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1<2	elab-aspect	elab-aspect
D14-1065	58-67	101-108	we propose a new document clustering approach for multilingual corpora	to deal with high dimensionality and sparseness .	53-108	53-108	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	1<2	enablement	enablement
D14-1065	58-67	109-136	we propose a new document clustering approach for multilingual corpora	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	53-108	109-136	To cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness .	Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .	1<2	evaluation	evaluation
D14-1066	1-14	15-25	In this paper we examine the lexical substitution task for the medical domain .	We adapt the current best system from the open domain ,	1-14	15-37	In this paper we examine the lexical substitution task for the medical domain .	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	1<2	manner-means	manner-means
D14-1066	15-25	26-33	We adapt the current best system from the open domain ,	which trains a single classifier for all instances	15-37	15-37	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	1<2	elab-addition	elab-addition
D14-1066	26-33	34-37	which trains a single classifier for all instances	using delexicalized features .	15-37	15-37	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	We adapt the current best system from the open domain , which trains a single classifier for all instances using delexicalized features .	1<2	manner-means	manner-means
D14-1066	1-14	38-45	In this paper we examine the lexical substitution task for the medical domain .	We show significant improvements over a strong baseline	1-14	38-54	In this paper we examine the lexical substitution task for the medical domain .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	1<2	evaluation	evaluation
D14-1066	38-45	46-54	We show significant improvements over a strong baseline	coming from a distributional thesaurus ( DT ) .	38-54	38-54	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	1<2	elab-addition	elab-addition
D14-1066	55-61	62,66-70	Whereas in the open domain system ,	features <*> show only slight improvements ,	55-93	55-93	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	1>2	elab-addition	elab-addition
D14-1066	62,66-70	73-87	features <*> show only slight improvements ,	that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit	55-93	55-93	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	1>2	contrast	contrast
D14-1066	62,66-70	63-65	features <*> show only slight improvements ,	derived from WordNet	55-93	55-93	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	1<2	elab-addition	elab-addition
D14-1066	71-72	73-87	we show	that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit	55-93	55-93	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	1>2	attribution	attribution
D14-1066	38-45	73-87	We show significant improvements over a strong baseline	that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit	38-54	55-93	We show significant improvements over a strong baseline coming from a distributional thesaurus ( DT ) .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	1<2	exp-evidence	exp-evidence
D14-1066	73-87	88-93	that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit	when used for feature generation .	55-93	55-93	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	Whereas in the open domain system , features derived from WordNet show only slight improvements , we show that its counterpart for the medical domain ( UMLS ) shows a significant additional benefit when used for feature generation .	1<2	temporal	temporal
D14-1067	1-5	6-20	This paper presents a system	which learns to answer questions on a broad range of topics from a knowledge base	1-25	1-25	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	1<2	elab-addition	elab-addition
D14-1067	6-20	21-25	which learns to answer questions on a broad range of topics from a knowledge base	using few hand-crafted features .	1-25	1-25	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	1<2	manner-means	manner-means
D14-1067	1-5	26-37	This paper presents a system	Our model learns low-dimensional embeddings of words and knowledge base constituents ;	1-25	26-50	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	1<2	manner-means	manner-means
D14-1067	26-37	38-41	Our model learns low-dimensional embeddings of words and knowledge base constituents ;	these representations are used	26-50	26-50	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	1<2	elab-addition	elab-addition
D14-1067	38-41	42-50	these representations are used	to score natural language questions against candidate answers .	26-50	26-50	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	Our model learns low-dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .	1<2	enablement	enablement
D14-1067	1-5	51-53,71-81	This paper presents a system	Training our system <*> yields competitive results on a recent benchmark of the literature .	1-25	51-81	This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	1<2	evaluation	evaluation
D14-1067	51-53,71-81	54-70	Training our system <*> yields competitive results on a recent benchmark of the literature .	using pairs of questions and structured representations of their answers , and pairs of question paraphrases ,	51-81	51-81	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .	1<2	manner-means	manner-means
D14-1068	1-11	23-33	Keyboard layout errors and homoglyphs in cross-language queries impact our ability	We present a machine learning approach to correcting these errors ,	1-22	23-40	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	1>2	bg-goal	bg-goal
D14-1068	1-11	12-17	Keyboard layout errors and homoglyphs in cross-language queries impact our ability	to correctly interpret user information needs	1-22	1-22	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	1<2	elab-addition	elab-addition
D14-1068	12-17	18-22	to correctly interpret user information needs	and offer relevant results .	1-22	1-22	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	Keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results .	1<2	joint	joint
D14-1068	23-33	34-40	We present a machine learning approach to correcting these errors ,	based largely on character-level n-gram features .	23-40	23-40	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	1<2	bg-general	bg-general
D14-1068	23-33	41-59	We present a machine learning approach to correcting these errors ,	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries	23-40	41-65	We present a machine learning approach to correcting these errors , based largely on character-level n-gram features .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	1<2	evaluation	evaluation
D14-1068	41-59	60-65	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries	that yield null search results .	41-65	41-65	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	We demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .	1<2	elab-addition	elab-addition
D14-1069	1-33	34-74	Non-linear mappings of the form P ( ngram )  andlog ( 1+P ( ngram )) log ( 1+ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	1-33	34-74	Non-linear mappings of the form P ( ngram )  andlog ( 1+P ( ngram )) log ( 1+ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The first mapping reduces classification errors by 4.0 % to 83.9 % over a test set of more than one million 65-character strings in 1366 languages , and by 2.6 % to 76.7 % over a subset of 781 languages .	1<2	elab-enumember	elab-enumember
D14-1069	1-33	75-104	Non-linear mappings of the form P ( ngram )  andlog ( 1+P ( ngram )) log ( 1+ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	1-33	75-104	Non-linear mappings of the form P ( ngram )  andlog ( 1+P ( ngram )) log ( 1+ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The second mapping improves four of the five identifiers by 10.6 % to 83.8 % on the larger corpus and 14.4 % to 76.7 % on the smaller corpus .	1<2	elab-enumember	elab-enumember
D14-1069	1-33	105-122	Non-linear mappings of the form P ( ngram )  andlog ( 1+P ( ngram )) log ( 1+ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/ralf/langid.html .	1-33	105-122	Non-linear mappings of the form P ( ngram )  andlog ( 1+P ( ngram )) log ( 1+ ) are applied to the n-gram probabilities in five trainable open-source language identifiers .	The subset corpus and the modified programs are made freely available for download at http : //www.cs.cmu.edu/ralf/langid.html .	1<2	elab-addition	elab-addition
D14-1070	1-22	48-57	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	We introduce a recursive neural network ( rnn ) model	1-22	48-68	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	1>2	bg-compare	bg-compare
D14-1070	1-22	23-26	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	These methods are ineffective	1-22	23-47	Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations .	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	1<2	elab-addition	elab-addition
D14-1070	23-26	27-34	These methods are ineffective	when question text contains very few individual words	23-47	23-47	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	1<2	temporal	temporal
D14-1070	27-34	35-40	when question text contains very few individual words	( e.g. , named entities )	23-47	23-47	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	1<2	elab-example	elab-example
D14-1070	27-34	41-47	when question text contains very few individual words	that are indicative of the answer .	23-47	23-47	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	These methods are ineffective when question text contains very few individual words ( e.g. , named entities ) that are indicative of the answer .	1<2	elab-addition	elab-addition
D14-1070	48-57	58-63	We introduce a recursive neural network ( rnn ) model	that can reason over such input	48-68	48-68	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	1<2	elab-addition	elab-addition
D14-1070	58-63	64-68	that can reason over such input	by modeling textual compositionality .	48-68	48-68	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	1<2	manner-means	manner-means
D14-1070	48-57	69-84	We introduce a recursive neural network ( rnn ) model	We apply our model , qanta , to a dataset of questions from a trivia competition	48-68	69-88	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	1<2	elab-addition	elab-addition
D14-1070	69-84	85-88	We apply our model , qanta , to a dataset of questions from a trivia competition	called quiz bowl .	69-88	69-88	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	1<2	elab-addition	elab-addition
D14-1070	89-93	94-99	Unlike previous rnn models ,	qanta learns word and phrase-level representations	89-108	89-108	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	1>2	contrast	contrast
D14-1070	69-84	94-99	We apply our model , qanta , to a dataset of questions from a trivia competition	qanta learns word and phrase-level representations	69-88	89-108	We apply our model , qanta , to a dataset of questions from a trivia competition called quiz bowl .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	1<2	elab-aspect	elab-aspect
D14-1070	94-99	100-103	qanta learns word and phrase-level representations	that combine across sentences	89-108	89-108	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	1<2	elab-addition	elab-addition
D14-1070	94-99	104-108	qanta learns word and phrase-level representations	to reason about entities .	89-108	89-108	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	Unlike previous rnn models , qanta learns word and phrase-level representations that combine across sentences to reason about entities .	1<2	enablement	enablement
D14-1070	48-57	109-113	We introduce a recursive neural network ( rnn ) model	The model outperforms multiple baselines	48-68	109-128	We introduce a recursive neural network ( rnn ) model that can reason over such input by modeling textual compositionality .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	1<2	evaluation	evaluation
D14-1070	114-122	123-128	and , when combined with information retrieval methods ,	ri-vals the best human players .	109-128	109-128	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	1>2	temporal	temporal
D14-1070	109-113	123-128	The model outperforms multiple baselines	ri-vals the best human players .	109-128	109-128	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	The model outperforms multiple baselines and , when combined with information retrieval methods , ri-vals the best human players .	1<2	joint	joint
D14-1071	1-28	47-52	Transforming a natural language ( NL ) question into a corresponding logical form ( LF ) is central to the knowledge-based question answering ( KB-QA ) task .	this paper goes one step further	1-28	29-79	Transforming a natural language ( NL ) question into a corresponding logical form ( LF ) is central to the knowledge-based question answering ( KB-QA ) task .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1>2	bg-goal	bg-goal
D14-1071	29-32	47-52	Unlike most previous methods	this paper goes one step further	29-79	29-79	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1>2	contrast	contrast
D14-1071	29-32	33-36	Unlike most previous methods	that achieve this goal	29-79	29-79	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1<2	elab-addition	elab-addition
D14-1071	33-36	37-46	that achieve this goal	based on mappings between lexicalized phrases and logical predicates ,	29-79	29-79	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1<2	bg-general	bg-general
D14-1071	47-52	53-58	this paper goes one step further	and proposes a novel embedding-based approach	29-79	29-79	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1<2	joint	joint
D14-1071	53-58	59-65	and proposes a novel embedding-based approach	that maps NL-questions into LFs for KB-QA	29-79	29-79	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1<2	elab-addition	elab-addition
D14-1071	59-65	66-79	that maps NL-questions into LFs for KB-QA	by leveraging semantic associations between lexical representations and KB-properties in the latent space .	29-79	29-79	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	1<2	manner-means	manner-means
D14-1071	80-82	83-99	Experimental results demonstrate	that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	80-99	80-99	Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	1>2	attribution	attribution
D14-1071	47-52	83-99	this paper goes one step further	that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	29-79	80-99	Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates , this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space .	Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets .	1<2	evaluation	evaluation
D14-1072	1-13	30-44	Wikipedia's link structure is a valuable resource for natural language processing tasks ,	In this paper , we study how to augment Wikipedia with additional high-precision links .	1-29	30-44	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	In this paper , we study how to augment Wikipedia with additional high-precision links .	1>2	bg-compare	bg-compare
D14-1072	1-13	14-20,25-29	Wikipedia's link structure is a valuable resource for natural language processing tasks ,	but only a fraction of the concepts <*> are annotated with hyperlinks .	1-29	1-29	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	1<2	contrast	contrast
D14-1072	14-20,25-29	21-24	but only a fraction of the concepts <*> are annotated with hyperlinks .	mentioned in each article	1-29	1-29	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	Wikipedia's link structure is a valuable resource for natural language processing tasks , but only a fraction of the concepts mentioned in each article are annotated with hyperlinks .	1<2	elab-addition	elab-addition
D14-1072	30-44	45-50	In this paper , we study how to augment Wikipedia with additional high-precision links .	We present 3W , a system	30-44	45-67	In this paper , we study how to augment Wikipedia with additional high-precision links .	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	1<2	elab-addition	elab-addition
D14-1072	45-50	51-58	We present 3W , a system	that identifies concept mentions in Wikipedia text ,	45-67	45-67	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	1<2	elab-addition	elab-addition
D14-1072	51-58	59-67	that identifies concept mentions in Wikipedia text ,	and links each mention to its referent page .	45-67	45-67	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	1<2	joint	joint
D14-1072	45-50	68-75	We present 3W , a system	3W leverages rich semantic information present in Wikipedia	45-67	68-80	We present 3W , a system that identifies concept mentions in Wikipedia text , and links each mention to its referent page .	3W leverages rich semantic information present in Wikipedia to achieve high precision .	1<2	elab-addition	elab-addition
D14-1072	68-75	76-80	3W leverages rich semantic information present in Wikipedia	to achieve high precision .	68-80	68-80	3W leverages rich semantic information present in Wikipedia to achieve high precision .	3W leverages rich semantic information present in Wikipedia to achieve high precision .	1<2	enablement	enablement
D14-1072	81-83	84-104	Our experiments demonstrate	that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	81-104	81-104	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	1>2	attribution	attribution
D14-1072	30-44	84-104	In this paper , we study how to augment Wikipedia with additional high-precision links .	that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	30-44	81-104	In this paper , we study how to augment Wikipedia with additional high-precision links .	Our experiments demonstrate that 3W can add an average of seven new links to each Wikipedia article , at a precision of 0.98 .	1<2	evaluation	evaluation
D14-1073	1-19	20-23	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR )	using relevance prediction .	1-23	1-23	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	1<2	manner-means	manner-means
D14-1073	1-19	24-37	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR )	We describe our 13 summarization methods each from one of four summarization strategies .	1-23	24-37	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We describe our 13 summarization methods each from one of four summarization strategies .	1<2	elab-process_step	elab-process_step
D14-1073	38-39	40-44	We show	how well our methods perform	38-60	38-60	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	1>2	attribution	attribution
D14-1073	1-19	40-44	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR )	how well our methods perform	1-23	38-60	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	1<2	elab-process_step	elab-process_step
D14-1073	40-44	45-53	how well our methods perform	using Farsi text from the CLEF 2008 shared-task ,	38-60	38-60	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	1<2	manner-means	manner-means
D14-1073	45-53	54-60	using Farsi text from the CLEF 2008 shared-task ,	which we translated to English automtatically .	38-60	38-60	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	We show how well our methods perform using Farsi text from the CLEF 2008 shared-task , which we translated to English automtatically .	1<2	elab-addition	elab-addition
D14-1073	1-19	61-68	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR )	We report precision/recall/F1 , accuracy and time-on-task .	1-23	61-68	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	We report precision/recall/F1 , accuracy and time-on-task .	1<2	elab-process_step	elab-process_step
D14-1073	69-70	71-81	We found	that different summarization methods perform optimally for different evaluation metrics ,	69-93	69-93	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	1>2	attribution	attribution
D14-1073	61-68	71-81	We report precision/recall/F1 , accuracy and time-on-task .	that different summarization methods perform optimally for different evaluation metrics ,	61-68	69-93	We report precision/recall/F1 , accuracy and time-on-task .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	1<2	elab-addition	elab-addition
D14-1073	71-81	82-93	that different summarization methods perform optimally for different evaluation metrics ,	but overall query biased word clouds are the best summarization strategy .	69-93	69-93	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	We found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy .	1<2	contrast	contrast
D14-1073	94-99	100-115	In our analysis , we demonstrate	that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions	94-121	94-121	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	1>2	attribution	attribution
D14-1073	1-19	100-115	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR )	that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions	1-23	94-121	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	1<2	elab-addition	elab-addition
D14-1073	100-115	116-121	that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions	as our evaluation framework does .	94-121	94-121	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	In our analysis , we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does .	1<2	comparison	comparison
D14-1073	1-19	122-127	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR )	Finally , we present our recommendations	1-23	122-135	In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( CLIR ) using relevance prediction .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	1<2	elab-process_step	elab-process_step
D14-1073	122-127	128-135	Finally , we present our recommendations	for creating muchneeded evaluation standards and datasets .	122-135	122-135	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	Finally , we present our recommendations for creating muchneeded evaluation standards and datasets .	1<2	enablement	enablement
D14-1074	1-8	9-13	We propose a model for Chinese poem generation	based on recurrent neural networks	1-26	1-26	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	1<2	bg-general	bg-general
D14-1074	9-13	14-26	based on recurrent neural networks	which we argue is ideally suited to capturing poetic content and form .	1-26	1-26	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	1<2	elab-addition	elab-addition
D14-1074	1-8	27-32	We propose a model for Chinese poem generation	Our generator jointly performs content selection	1-26	27-72	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	1<2	elab-addition	elab-addition
D14-1074	27-32	33-37	Our generator jointly performs content selection	( "what to say" )	27-72	27-72	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	1<2	elab-definition	elab-definition
D14-1074	27-32	38-40	Our generator jointly performs content selection	and surface realization	27-72	27-72	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	1<2	joint	joint
D14-1074	38-40	41-45	and surface realization	( "how to say" )	27-72	27-72	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	1<2	elab-definition	elab-definition
D14-1074	38-40	46-72	and surface realization	by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	27-72	27-72	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	Our generator jointly performs content selection ( "what to say" ) and surface realization ( "how to say" ) by learning representations of individual characters , and their combinations into one or more lines as well as how these mutually reinforce and constrain each other .	1<2	manner-means	manner-means
D14-1074	1-8	73-77	We propose a model for Chinese poem generation	Poem lines are generated incrementally	1-26	73-105	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	1<2	elab-addition	elab-addition
D14-1074	73-77	78-84	Poem lines are generated incrementally	by taking into account the entire history	73-105	73-105	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	1<2	manner-means	manner-means
D14-1074	78-84	85-91	by taking into account the entire history	of what has been generated so far	73-105	73-105	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	1<2	elab-addition	elab-addition
D14-1074	78-84	92-96	by taking into account the entire history	rather than the limited horizon	73-105	73-105	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	1<2	contrast	contrast
D14-1074	92-96	97-105	rather than the limited horizon	imposed by the previous line or lexical n-grams .	73-105	73-105	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams .	1<2	elab-addition	elab-addition
D14-1074	106-108	109-117	Experimental results show	that our model outperforms competitive Chinese poetry generation systems	106-125	106-125	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	1>2	attribution	attribution
D14-1074	1-8	109-117	We propose a model for Chinese poem generation	that our model outperforms competitive Chinese poetry generation systems	1-26	106-125	We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form .	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	1<2	evaluation	evaluation
D14-1074	109-117	118-125	that our model outperforms competitive Chinese poetry generation systems	using both automatic and manual evaluation methods .	106-125	106-125	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods .	1<2	manner-means	manner-means
D14-1075	1-11	12-15	This paper explores alternate algorithms , reward functions and feature sets	for performing multi-document summarization	1-25	1-25	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	1<2	enablement	enablement
D14-1075	12-15	16-25	for performing multi-document summarization	using reinforcement learning with a high focus on reproducibility .	1-25	1-25	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	1<2	manner-means	manner-means
D14-1075	26-27	28-33	We show	that ROUGE results can be improved	26-50	26-50	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	1>2	attribution	attribution
D14-1075	1-11	28-33	This paper explores alternate algorithms , reward functions and feature sets	that ROUGE results can be improved	1-25	26-50	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	1<2	elab-addition	elab-addition
D14-1075	28-33	34-40	that ROUGE results can be improved	using a unigram and bigram similarity metric	26-50	26-50	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	1<2	manner-means	manner-means
D14-1075	34-40	41-50	using a unigram and bigram similarity metric	when training a learner to select sentences for summarization .	26-50	26-50	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization .	1<2	temporal	temporal
D14-1075	1-11	51-57	This paper explores alternate algorithms , reward functions and feature sets	Learners are trained to summarize document clusters	1-25	51-70	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	1<2	manner-means	manner-means
D14-1075	51-57	58-64	Learners are trained to summarize document clusters	based on various algorithms and reward functions	51-70	51-70	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	1<2	bg-general	bg-general
D14-1075	51-57	65-70	Learners are trained to summarize document clusters	and then evaluated using ROUGE .	51-70	51-70	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE .	1<2	joint	joint
D14-1075	1-11	71-97	This paper explores alternate algorithms , reward functions and feature sets	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively ,	1-25	71-119	This paper explores alternate algorithms , reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility .	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	1<2	evaluation	evaluation
D14-1075	71-97	98-119	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively ,	when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	71-119	71-119	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	1<2	comparison	comparison
D14-1075	71-97	120-158	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively ,	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	71-119	120-158	Our experiments show a statistically significant improvement of 1.33 % , 1.58 % , and 2.25 % for ROUGE-1 , ROUGE-2 and ROUGE-L scores , respectively , when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset .	Furthermore query focused extensions of our approach show an improvement of 1.37 % and 2.31 % for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset .	1<2	progression	progression
D14-1076	1-9	20-26	In this paper , we focus on the problem	We propose an innovative sentence compression method	1-19	20-44	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	1>2	bg-goal	bg-goal
D14-1076	1-9	10-19	In this paper , we focus on the problem	of using sentence compression techniques to improve multi-document summarization .	1-19	1-19	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	In this paper , we focus on the problem of using sentence compression techniques to improve multi-document summarization .	1<2	elab-addition	elab-addition
D14-1076	20-26	27-35	We propose an innovative sentence compression method	by considering every node in the constituent parse tree	20-44	20-44	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	1<2	manner-means	manner-means
D14-1076	27-35	36-39	by considering every node in the constituent parse tree	and deciding its status	20-44	20-44	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	1<2	joint	joint
D14-1076	36-39	40-44	and deciding its status	- remove or retain .	20-44	20-44	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	1<2	elab-addition	elab-addition
D14-1076	20-26	45-52	We propose an innovative sentence compression method	Integer liner programming with discriminative training is used	20-44	45-57	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	Integer liner programming with discriminative training is used to solve the problem .	1<2	manner-means	manner-means
D14-1076	45-52	53-57	Integer liner programming with discriminative training is used	to solve the problem .	45-57	45-57	Integer liner programming with discriminative training is used to solve the problem .	Integer liner programming with discriminative training is used to solve the problem .	1<2	enablement	enablement
D14-1076	45-52	58-65	Integer liner programming with discriminative training is used	Under this model , we incorporate various constraints	45-57	58-75	Integer liner programming with discriminative training is used to solve the problem .	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	1<2	elab-process_step	elab-process_step
D14-1076	58-65	66-75	Under this model , we incorporate various constraints	to improve the linguistic quality of the compressed sentences .	58-75	58-75	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	Under this model , we incorporate various constraints to improve the linguistic quality of the compressed sentences .	1<2	enablement	enablement
D14-1076	45-52	76-82	Integer liner programming with discriminative training is used	Then we utilize a pipeline summarization framework	45-57	76-110	Integer liner programming with discriminative training is used to solve the problem .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	1<2	elab-process_step	elab-process_step
D14-1076	76-82	83-92	Then we utilize a pipeline summarization framework	where sentences are first compressed by our proposed compression model	76-110	76-110	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	1<2	elab-addition	elab-addition
D14-1076	83-92	93-96	where sentences are first compressed by our proposed compression model	to obtain top-n candidates	76-110	76-110	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	1<2	enablement	enablement
D14-1076	45-52	97-104	Integer liner programming with discriminative training is used	and then a sentence selection module is used	45-57	76-110	Integer liner programming with discriminative training is used to solve the problem .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	1<2	elab-process_step	elab-process_step
D14-1076	97-104	105-110	and then a sentence selection module is used	to generate the final summary .	76-110	76-110	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary .	1<2	enablement	enablement
D14-1076	111-115	116-129	Compared with state-of-the-art algorithms ,	our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	111-129	111-129	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	1>2	comparison	comparison
D14-1076	20-26	116-129	We propose an innovative sentence compression method	our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	20-44	111-129	We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status - remove or retain .	Compared with state-of-the-art algorithms , our model has similar ROUGE-2 scores but better linguistic quality on TAC data .	1<2	evaluation	evaluation
D14-1077	1-8	9-32	In this study , we analyzed the effects	of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish ,	1-41	1-41	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	1<2	elab-addition	elab-addition
D14-1077	9-32	33-41	of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish ,	which is an agglutinative and morphologically rich language .	1-41	1-41	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	1<2	elab-addition	elab-addition
D14-1077	1-8	42-50	In this study , we analyzed the effects	We constructed a manually annotated MDS data set ,	1-41	42-64	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	We constructed a manually annotated MDS data set , and to our best knowledge , reported the first results on Turkish MDS .	1<2	manner-means	manner-means
D14-1077	42-50	51-64	We constructed a manually annotated MDS data set ,	and to our best knowledge , reported the first results on Turkish MDS .	42-64	42-64	We constructed a manually annotated MDS data set , and to our best knowledge , reported the first results on Turkish MDS .	We constructed a manually annotated MDS data set , and to our best knowledge , reported the first results on Turkish MDS .	1<2	joint	joint
D14-1077	65-67	68-81	Our results show	that a simple fixed-length word truncation approach performs slightly better than no stemming ,	65-92	65-92	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	1>2	attribution	attribution
D14-1077	1-8	68-81	In this study , we analyzed the effects	that a simple fixed-length word truncation approach performs slightly better than no stemming ,	1-41	65-92	In this study , we analyzed the effects of applying different levels of stemming approaches such as fixed-length word truncation and morphological analysis for multi-document summarization ( MDS ) on Turkish , which is an agglutinative and morphologically rich language .	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	1<2	evaluation	evaluation
D14-1077	68-81	82-92	that a simple fixed-length word truncation approach performs slightly better than no stemming ,	whereas applying complex morphological analysis does not improve Turkish MDS .	65-92	65-92	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	Our results show that a simple fixed-length word truncation approach performs slightly better than no stemming , whereas applying complex morphological analysis does not improve Turkish MDS .	1<2	contrast	contrast
D14-1078	1-2,8-17	88-105	The ability <*> can give systems access to unprecedented amounts of knowledge .	that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself ,	1-17	82-120	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	1>2	bg-compare	bg-compare
D14-1078	1-2,8-17	3-7	The ability <*> can give systems access to unprecedented amounts of knowledge .	to learn from user interactions	1-17	1-17	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	1<2	enablement	enablement
D14-1078	1-2,8-17	18-31	The ability <*> can give systems access to unprecedented amounts of knowledge .	This is evident in search engines , recommender systems , and electronic commerce ,	1-17	18-44	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	1<2	elab-addition	elab-addition
D14-1078	18-31	32-37	This is evident in search engines , recommender systems , and electronic commerce ,	and it can be the key	18-44	18-44	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	1<2	progression	progression
D14-1078	32-37	38-44	and it can be the key	to solving other knowledge in-tensive tasks .	18-44	18-44	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	This is evident in search engines , recommender systems , and electronic commerce , and it can be the key to solving other knowledge in-tensive tasks .	1<2	elab-addition	elab-addition
D14-1078	1-2,8-17	45-49,54-61	The ability <*> can give systems access to unprecedented amounts of knowledge .	However , extracting the knowledge <*> is less straightforward than standard machine learning ,	1-17	45-81	The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	1<2	contrast	contrast
D14-1078	45-49,54-61	50-53	However , extracting the knowledge <*> is less straightforward than standard machine learning ,	conveyed by user interactions	45-81	45-81	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	1<2	elab-addition	elab-addition
D14-1078	45-49,54-61	62-66	However , extracting the knowledge <*> is less straightforward than standard machine learning ,	since it requires learning systems	45-81	45-81	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	1<2	exp-reason	exp-reason
D14-1078	62-66	67-81	since it requires learning systems	that explicitly account for human decision making , human motivation , and human abilities .	45-81	45-81	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	However , extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning , since it requires learning systems that explicitly account for human decision making , human motivation , and human abilities .	1<2	elab-addition	elab-addition
D14-1078	82-87	88-105	In this talk , I argue	that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself ,	82-120	82-120	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	1>2	attribution	attribution
D14-1078	88-105	106-120	that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself ,	but also the design of the interaction under an appropriate model of user behavior .	82-120	82-120	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	1<2	progression	progression
D14-1078	121-127	128-149	To this effect , the talk explores	how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms ,	121-163	121-163	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	1>2	attribution	attribution
D14-1078	88-105	128-149	that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself ,	how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms ,	82-120	121-163	In this talk , I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself , but also the design of the interaction under an appropriate model of user behavior .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	1<2	elab-addition	elab-addition
D14-1078	128-149	150-152	how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms ,	leading to systems	121-163	121-163	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	1<2	elab-addition	elab-addition
D14-1078	150-152	153-156	leading to systems	that have provable guarantees	121-163	121-163	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	1<2	elab-addition	elab-addition
D14-1078	153-156	157-163	that have provable guarantees	and that perform robustly in practice .	121-163	121-163	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	To this effect , the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms , leading to systems that have provable guarantees and that perform robustly in practice .	1<2	joint	joint
D14-1079	1-13	14-18	We provide a comparative study between neural word representations and traditional vector spaces	based on co-occurrence counts ,	1-25	1-25	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	1<2	bg-general	bg-general
D14-1079	1-13	19-25	We provide a comparative study between neural word representations and traditional vector spaces	in a number of compositional tasks .	1-25	1-25	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	1<2	elab-addition	elab-addition
D14-1079	1-13	26-31	We provide a comparative study between neural word representations and traditional vector spaces	We use three different semantic spaces	1-25	26-60	We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts , in a number of compositional tasks .	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	1<2	manner-means	manner-means
D14-1079	26-31	32-38	We use three different semantic spaces	and implement seven tensor-based compositional models ,	26-60	26-60	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	1<2	joint	joint
D14-1079	32-38	39-53	and implement seven tensor-based compositional models ,	which we then test ( together with simpler additive and multiplicative approaches ) in tasks	26-60	26-60	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	1<2	elab-addition	elab-addition
D14-1079	39-53	54-60	which we then test ( together with simpler additive and multiplicative approaches ) in tasks	involving verb disambiguation and sentence similarity .	26-60	26-60	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	1<2	elab-addition	elab-addition
D14-1079	61-65	66-70	To check their scalability ,	we additionally evaluate the spaces	61-89	61-89	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	1>2	enablement	enablement
D14-1079	26-31	66-70	We use three different semantic spaces	we additionally evaluate the spaces	26-60	61-89	We use three different semantic spaces and implement seven tensor-based compositional models , which we then test ( together with simpler additive and multiplicative approaches ) in tasks involving verb disambiguation and sentence similarity .	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	1<2	progression	progression
D14-1079	66-70	71-82	we additionally evaluate the spaces	using simple compositional methods on larger-scale tasks with less constrained language :	61-89	61-89	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	1<2	manner-means	manner-means
D14-1079	71-82	83-89	using simple compositional methods on larger-scale tasks with less constrained language :	paraphrase detection and dialogue act tagging .	61-89	61-89	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	1<2	elab-addition	elab-addition
D14-1079	66-70	90-100	we additionally evaluate the spaces	In the more constrained tasks , co-occurrence vectors are competitive ,	61-89	90-131	To check their scalability , we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language : paraphrase detection and dialogue act tagging .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	1<2	result	result
D14-1079	90-100	101-108	In the more constrained tasks , co-occurrence vectors are competitive ,	although choice of compositional method is important ;	90-131	90-131	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	1<2	contrast	contrast
D14-1079	90-100	109-121	In the more constrained tasks , co-occurrence vectors are competitive ,	on the larger-scale tasks , they are outperformed by neural word embeddings ,	90-131	90-131	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	1<2	joint	joint
D14-1079	109-121	122-131	on the larger-scale tasks , they are outperformed by neural word embeddings ,	which show robust , stable performance across the tasks .	90-131	90-131	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	In the more constrained tasks , co-occurrence vectors are competitive , although choice of compositional method is important ; on the larger-scale tasks , they are outperformed by neural word embeddings , which show robust , stable performance across the tasks .	1<2	elab-addition	elab-addition
D14-1080	1-12	62-76	Recurrent neural networks ( RNNs ) are connectionist models of sequential data	In this work we apply these deep RNNs to the task of opinion expression extraction	1-23	62-83	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	1>2	null	null
D14-1080	1-12	13-23	Recurrent neural networks ( RNNs ) are connectionist models of sequential data	that are naturally applicable to the analysis of natural language .	1-23	1-23	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	1<2	elab-addition	elab-addition
D14-1080	1-12	24-43	Recurrent neural networks ( RNNs ) are connectionist models of sequential data	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated	1-23	24-61	Recurrent neural networks ( RNNs ) are connectionist models of sequential data that are naturally applicable to the analysis of natural language .	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	1<2	elab-addition	elab-addition
D14-1080	24-43	44-49	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated	by stacking multiple layers of RNNs	24-61	24-61	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	1<2	elab-addition	elab-addition
D14-1080	24-43	50-61	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated	and shown empirically to bring a temporal hierarchy to the architecture .	24-61	24-61	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	Recently , "depth in space"  as an orthogonal notion to "depth in time"  in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture .	1<2	joint	joint
D14-1080	62-76	77-83	In this work we apply these deep RNNs to the task of opinion expression extraction	formulated as a token-level sequence-labeling task .	62-83	62-83	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	1<2	elab-addition	elab-addition
D14-1080	84-86	87-104	Experimental results show	that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	84-104	84-104	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	1>2	attribution	attribution
D14-1080	62-76	87-104	In this work we apply these deep RNNs to the task of opinion expression extraction	that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	62-83	84-104	In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task .	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	1<2	evaluation	evaluation
D14-1080	87-104	105-113	that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	Furthermore , our approach outperforms previous CRF-based baselines ,	84-104	105-153	Experimental results show that deep , narrow RNNs outperform traditional shallow , wide RNNs with the same number of parameters .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	progression	progression
D14-1080	105-113	114-120	Furthermore , our approach outperforms previous CRF-based baselines ,	including the state-of-the-art semi-Markov CRF model ,	105-153	105-153	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	elab-addition	elab-addition
D14-1080	105-113	121-123	Furthermore , our approach outperforms previous CRF-based baselines ,	and does so	105-153	105-153	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	joint	joint
D14-1080	121-123	124-133	and does so	without access to the powerful opinion lexicons and syntactic features	105-153	105-153	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	elab-addition	elab-addition
D14-1080	124-133	134-139	without access to the powerful opinion lexicons and syntactic features	relied upon by the semi-CRF ,	105-153	105-153	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	elab-addition	elab-addition
D14-1080	124-133	140-147	without access to the powerful opinion lexicons and syntactic features	as well as without the standard layer-by-layer pre-training	105-153	105-153	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	joint	joint
D14-1080	140-147	148-153	as well as without the standard layer-by-layer pre-training	typically required of RNN architectures .	105-153	105-153	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	Furthermore , our approach outperforms previous CRF-based baselines , including the state-of-the-art semi-Markov CRF model , and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF , as well as without the standard layer-by-layer pre-training typically required of RNN architectures .	1<2	elab-addition	elab-addition
D14-1081	1-12	13-30	We propose the first implementation of an infinite-order generative dependency model .	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	1-12	13-30	We propose the first implementation of an infinite-order generative dependency model .	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	1<2	elab-addition	elab-addition
D14-1081	13-30	31-51	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	13-30	31-51	The model is based on a new recursive neural network architecture , the Inside-Outside Recursive Neural Network .	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	1<2	elab-addition	elab-addition
D14-1081	31-51	52-54	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	This is achieved	31-51	52-72	This architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also top-down .	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	1<2	elab-addition	elab-addition
D14-1081	52-54	55-66	This is achieved	by computing content as well as context representations for any constituent ,	52-72	52-72	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	1<2	manner-means	manner-means
D14-1081	55-66	67-72	by computing content as well as context representations for any constituent ,	and letting these representations interact .	52-72	52-72	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	This is achieved by computing content as well as context representations for any constituent , and letting these representations interact .	1<2	joint	joint
D14-1081	73-84	85-99	Experimental results on the English section of the Universal Dependency Treebank show	that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model	73-113	73-113	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	1>2	attribution	attribution
D14-1081	1-12	85-99	We propose the first implementation of an infinite-order generative dependency model .	that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model	1-12	73-113	We propose the first implementation of an infinite-order generative dependency model .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	1<2	evaluation	evaluation
D14-1081	85-99	100-102	that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model	using counting ,	73-113	73-113	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	1<2	manner-means	manner-means
D14-1081	85-99	103-113	that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model	and tends to choose more accurate parses in k-best lists .	73-113	73-113	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	1<2	joint	joint
D14-1081	85-99	114-131	that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	73-113	114-131	Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists .	In addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .	1<2	progression	progression
D14-1082	1-6	34-42	Almost all current dependency parsers classify	In this work , we propose a novel way	1-14	34-58	Almost all current dependency parsers classify based on millions of sparse indicator features .	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	1>2	bg-compare	bg-compare
D14-1082	1-6	7-14	Almost all current dependency parsers classify	based on millions of sparse indicator features .	1-14	1-14	Almost all current dependency parsers classify based on millions of sparse indicator features .	Almost all current dependency parsers classify based on millions of sparse indicator features .	1<2	bg-general	bg-general
D14-1082	1-6	15-22	Almost all current dependency parsers classify	Not only do these features generalize poorly ,	1-14	15-33	Almost all current dependency parsers classify based on millions of sparse indicator features .	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	1<2	elab-addition	elab-addition
D14-1082	15-22	23-33	Not only do these features generalize poorly ,	but the cost of feature computation restricts parsing speed significantly .	15-33	15-33	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	Not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly .	1<2	progression	progression
D14-1082	34-42	43-58	In this work , we propose a novel way	of learning a neural network classifier for use in a greedy , transition-based dependency parser .	34-58	34-58	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	1<2	elab-addition	elab-addition
D14-1082	43-58	59-72	of learning a neural network classifier for use in a greedy , transition-based dependency parser .	Because this classifier learns and uses just a small number of dense features ,	34-58	59-98	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	1<2	exp-reason	exp-reason
D14-1082	59-72	73-78	Because this classifier learns and uses just a small number of dense features ,	it can work very fast ,	59-98	59-98	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	1<2	result	result
D14-1082	73-78	79-98	it can work very fast ,	while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	59-98	59-98	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	Because this classifier learns and uses just a small number of dense features , it can work very fast , while achiev-ing an about 2 % improvement in unlabeled and labeled attachment scores on both English and Chinese datasets .	1<2	progression	progression
D14-1082	34-42	99-124	In this work , we propose a novel way	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	34-58	99-124	In this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser .	Concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the English Penn Treebank .	1<2	evaluation	evaluation
D14-1083	1-15	52-64	Recent years have seen a surge of interest in stance classification in online debates .	We therefore examine the new task of reason classification in this paper .	1-15	52-64	Recent years have seen a surge of interest in stance classification in online debates .	We therefore examine the new task of reason classification in this paper .	1>2	bg-compare	bg-compare
D14-1083	1-15	16-28	Recent years have seen a surge of interest in stance classification in online debates .	Oftentimes , however , it is important to determine not only the stance	1-15	16-51	Recent years have seen a surge of interest in stance classification in online debates .	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	1<2	contrast	contrast
D14-1083	16-28	29-37	Oftentimes , however , it is important to determine not only the stance	expressed by an author in her debate posts ,	16-51	16-51	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	1<2	elab-addition	elab-addition
D14-1083	16-28	38-41	Oftentimes , however , it is important to determine not only the stance	but also the reasons	16-51	16-51	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	1<2	progression	progression
D14-1083	38-41	42-51	but also the reasons	behind her supporting or opposing the issue under debate .	16-51	16-51	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	Oftentimes , however , it is important to determine not only the stance expressed by an author in her debate posts , but also the reasons behind her supporting or opposing the issue under debate .	1<2	elab-addition	elab-addition
D14-1083	65-75	76-79	Given the close in-terplay between stance classification and reason classification ,	we design computational models	65-94	65-94	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	1>2	condition	condition
D14-1083	52-64	76-79	We therefore examine the new task of reason classification in this paper .	we design computational models	52-64	65-94	We therefore examine the new task of reason classification in this paper .	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	1<2	elab-addition	elab-addition
D14-1083	76-79	80-94	we design computational models	for examining how automatically computed stance information can be profitably exploited for reason classification .	65-94	65-94	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	Given the close in-terplay between stance classification and reason classification , we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification .	1<2	manner-means	manner-means
D14-1083	95-107	108-129	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate	that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	95-129	95-129	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	1>2	attribution	attribution
D14-1083	52-64	108-129	We therefore examine the new task of reason classification in this paper .	that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	52-64	95-129	We therefore examine the new task of reason classification in this paper .	Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts .	1<2	evaluation	evaluation
D14-1084	1-9	29-40	State-of-the-art Chinese zero pronoun resolution systems are supervised ,	we present a generative model for unsupervised Chinese zero pronoun resolution .	1-20	21-40	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	1>2	bg-compare	bg-compare
D14-1084	1-9	10-14	State-of-the-art Chinese zero pronoun resolution systems are supervised ,	thus relying on training data	1-20	1-20	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	1<2	result	result
D14-1084	10-14	15-20	thus relying on training data	containing manually resolved zero pronouns .	1-20	1-20	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	State-of-the-art Chinese zero pronoun resolution systems are supervised , thus relying on training data containing manually resolved zero pronouns .	1<2	elab-addition	elab-addition
D14-1084	21-28	29-40	To eliminate the reliance on annotated data ,	we present a generative model for unsupervised Chinese zero pronoun resolution .	21-40	21-40	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	1>2	enablement	enablement
D14-1084	29-40	41-51	we present a generative model for unsupervised Chinese zero pronoun resolution .	At the core of our model is a novel hypothesis :	21-40	41-71	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	1<2	elab-addition	elab-addition
D14-1084	41-51	52-55,64-66	At the core of our model is a novel hypothesis :	a probabilistic pronoun resolver <*> can be used	41-71	41-71	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	1<2	elab-addition	elab-addition
D14-1084	52-55,64-66	56-63	a probabilistic pronoun resolver <*> can be used	trained on overt pronouns in an unsupervised manner	41-71	41-71	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	1<2	elab-addition	elab-addition
D14-1084	64-66	67-71	can be used	to resolve zero pronouns .	41-71	41-71	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	At the core of our model is a novel hypothesis : a probabilistic pronoun resolver trained on overt pronouns in an unsupervised manner can be used to resolve zero pronouns .	1<2	enablement	enablement
D14-1084	72-73	74-84	Experiments demonstrate	that our unsupervised model rivals its state-of-the-art supervised counterparts in performance	72-95	72-95	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	1>2	attribution	attribution
D14-1084	29-40	74-84	we present a generative model for unsupervised Chinese zero pronoun resolution .	that our unsupervised model rivals its state-of-the-art supervised counterparts in performance	21-40	72-95	To eliminate the reliance on annotated data , we present a generative model for unsupervised Chinese zero pronoun resolution .	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	1<2	evaluation	evaluation
D14-1084	74-84	85-95	that our unsupervised model rivals its state-of-the-art supervised counterparts in performance	when resolving the Chinese zero pronouns in the OntoNotes corpus .	72-95	72-95	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	Experiments demonstrate that our unsupervised model rivals its state-of-the-art supervised counterparts in performance when resolving the Chinese zero pronouns in the OntoNotes corpus .	1<2	temporal	temporal
D14-1085	16-25	26-34	Compared to extraction or previous approaches to sentence fusion ,	sentence enhancement increases the range of possible summary sentences	16-49	16-49	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	1>2	comparison	comparison
D14-1085	1-15	26-34	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	sentence enhancement increases the range of possible summary sentences	1-15	16-49	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	1<2	elab-addition	elab-addition
D14-1085	26-34	35-49	sentence enhancement increases the range of possible summary sentences	by allowing the combination of dependency subtrees from any sentence from the source text .	16-49	16-49	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	Compared to extraction or previous approaches to sentence fusion , sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text .	1<2	manner-means	manner-means
D14-1085	50-52	53-58	Our experiments indicate	that our approach yields summary sentences	50-98	50-98	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	1>2	attribution	attribution
D14-1085	1-15	53-58	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	that our approach yields summary sentences	1-15	50-98	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	1<2	evaluation	evaluation
D14-1085	53-58	59-72	that our approach yields summary sentences	that are competitive with a sentence fusion baseline in terms of content quality ,	50-98	50-98	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	1<2	elab-addition	elab-addition
D14-1085	59-72	73-79	that are competitive with a sentence fusion baseline in terms of content quality ,	but better in terms of grammaticality ,	50-98	50-98	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	1<2	contrast	contrast
D14-1085	73-79	80-94	but better in terms of grammaticality ,	and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm	50-98	50-98	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	1<2	joint	joint
D14-1085	80-94	95-98	and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm	using distributional semantics .	50-98	50-98	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality , but better in terms of grammaticality , and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics .	1<2	manner-means	manner-means
D14-1085	99-101	102-114	We also consider	how text-to-text generation approaches to summarization can be extended beyond the source text	99-128	99-128	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	1>2	attribution	attribution
D14-1085	1-15	102-114	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	how text-to-text generation approaches to summarization can be extended beyond the source text	1-15	99-128	We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	1<2	elab-addition	elab-addition
D14-1085	115-116	117-128	by examining	how human summary writers incorporate source-text-external elements into their summary sentences .	99-128	99-128	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	1>2	attribution	attribution
D14-1085	102-114	117-128	how text-to-text generation approaches to summarization can be extended beyond the source text	how human summary writers incorporate source-text-external elements into their summary sentences .	99-128	99-128	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences .	1<2	manner-means	manner-means
D14-1086	1-8	9-15	In this paper we introduce a new game	to crowd-source natural language referring expressions .	1-15	1-15	In this paper we introduce a new game to crowd-source natural language referring expressions .	In this paper we introduce a new game to crowd-source natural language referring expressions .	1<2	enablement	enablement
D14-1086	16-22	23-35	By designing a two player game ,	we can both collect and verify referring expressions directly within the game .	16-35	16-35	By designing a two player game , we can both collect and verify referring expressions directly within the game .	By designing a two player game , we can both collect and verify referring expressions directly within the game .	1>2	manner-means	manner-means
D14-1086	9-15	23-35	to crowd-source natural language referring expressions .	we can both collect and verify referring expressions directly within the game .	1-15	16-35	In this paper we introduce a new game to crowd-source natural language referring expressions .	By designing a two player game , we can both collect and verify referring expressions directly within the game .	1<2	elab-addition	elab-addition
D14-1086	1-8	36-44	In this paper we introduce a new game	To date , the game has produced a dataset	1-15	36-61	In this paper we introduce a new game to crowd-source natural language referring expressions .	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	1<2	evaluation	evaluation
D14-1086	36-44	45-48	To date , the game has produced a dataset	containing 130,525 expressions ,	36-61	36-61	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	1<2	elab-addition	elab-addition
D14-1086	45-48	49-61	containing 130,525 expressions ,	referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	36-61	36-61	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	1<2	elab-addition	elab-addition
D14-1086	36-44	62-72	To date , the game has produced a dataset	This dataset is larger and more varied than previous REG datasets	36-61	62-83	To date , the game has produced a dataset containing 130,525 expressions , referring to 96,654 distinct objects , in 19,894 photographs of natural scenes .	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	1<2	comparison	comparison
D14-1086	62-72	73-83	This dataset is larger and more varied than previous REG datasets	and allows us to study referring expressions in real-world scenes .	62-83	62-83	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes .	1<2	joint	joint
D14-1086	1-8	84-94	In this paper we introduce a new game	We provide an in depth analysis of the resulting dataset .	1-15	84-94	In this paper we introduce a new game to crowd-source natural language referring expressions .	We provide an in depth analysis of the resulting dataset .	1<2	evaluation	evaluation
D14-1086	95-99	100-106	Based on our findings ,	we design a new optimization based model	95-119	95-119	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	1>2	bg-general	bg-general
D14-1086	1-8	100-106	In this paper we introduce a new game	we design a new optimization based model	1-15	95-119	In this paper we introduce a new game to crowd-source natural language referring expressions .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	1<2	progression	progression
D14-1086	100-106	107-110	we design a new optimization based model	for generating referring expressions	95-119	95-119	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	1<2	enablement	enablement
D14-1086	107-110	111-119	for generating referring expressions	and perform experimental evaluations on 3 test sets .	95-119	95-119	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	Based on our findings , we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets .	1<2	joint	joint
D14-1087	1-17	18-28	We propose an unsupervised approach to constructing templates from a large collection of semantic category names ,	and use the templates as the semantic representation of categories .	1-28	1-28	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	1<2	joint	joint
D14-1087	1-17	29-39	We propose an unsupervised approach to constructing templates from a large collection of semantic category names ,	The main challenge is that many terms have multiple meanings ,	1-28	29-47	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	1<2	elab-addition	elab-addition
D14-1087	29-39	40-47	The main challenge is that many terms have multiple meanings ,	resulting in a lot of wrong templates .	29-47	29-47	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	The main challenge is that many terms have multiple meanings , resulting in a lot of wrong templates .	1<2	result	result
D14-1087	1-17	48-58	We propose an unsupervised approach to constructing templates from a large collection of semantic category names ,	Statistical data and semantic knowledge are extracted from a web corpus	1-28	48-63	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	1<2	elab-addition	elab-addition
D14-1087	48-58	59-63	Statistical data and semantic knowledge are extracted from a web corpus	to improve template generation .	48-63	48-63	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	Statistical data and semantic knowledge are extracted from a web corpus to improve template generation .	1<2	enablement	enablement
D14-1087	1-17	64-75	We propose an unsupervised approach to constructing templates from a large collection of semantic category names ,	A nonlinear scoring function is proposed and demonstrated to be effective .	1-28	64-75	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	A nonlinear scoring function is proposed and demonstrated to be effective .	1<2	elab-addition	elab-addition
D14-1087	76-77	78-88	Experiments show	that our approach achieves significantly better results than baseline methods .	76-88	76-88	Experiments show that our approach achieves significantly better results than baseline methods .	Experiments show that our approach achieves significantly better results than baseline methods .	1>2	attribution	attribution
D14-1087	1-17	78-88	We propose an unsupervised approach to constructing templates from a large collection of semantic category names ,	that our approach achieves significantly better results than baseline methods .	1-28	76-88	We propose an unsupervised approach to constructing templates from a large collection of semantic category names , and use the templates as the semantic representation of categories .	Experiments show that our approach achieves significantly better results than baseline methods .	1<2	evaluation	evaluation
D14-1087	78-88	89-105	that our approach achieves significantly better results than baseline methods .	As an immediate application , we apply the extracted templates to the cleaning of a category collection	76-88	89-120	Experiments show that our approach achieves significantly better results than baseline methods .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	1<2	progression	progression
D14-1087	89-105	106-109	As an immediate application , we apply the extracted templates to the cleaning of a category collection	and see promising results	89-120	89-120	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	1<2	joint	joint
D14-1087	106-109	110-120	and see promising results	( precision improved from 81 % to 89 % ) .	89-120	89-120	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	As an immediate application , we apply the extracted templates to the cleaning of a category collection and see promising results ( precision improved from 81 % to 89 % ) .	1<2	exp-evidence	exp-evidence
D14-1088	1-12	62-66	Taxonomies are the backbone of many structured , semantic knowledge resources .	we propose a novel approach	1-12	57-104	Taxonomies are the backbone of many structured , semantic knowledge resources .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	1>2	bg-goal	bg-goal
D14-1088	13-25	62-66	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns	we propose a novel approach	13-37	57-104	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	1>2	bg-compare	bg-compare
D14-1088	13-25	26-30	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns	to extract the taxonomic relations	13-37	13-37	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	1<2	enablement	enablement
D14-1088	26-30	31-37	to extract the taxonomic relations	by matching the patterns to text .	13-37	13-37	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	1<2	manner-means	manner-means
D14-1088	13-25	38-46	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns	These approaches , however , often show low coverage	13-37	38-56	Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text .	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	1<2	contrast	contrast
D14-1088	38-46	47-56	These approaches , however , often show low coverage	due to the lack of contextual analysis across sentences .	38-56	38-56	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	These approaches , however , often show low coverage due to the lack of contextual analysis across sentences .	1<2	exp-reason	exp-reason
D14-1088	57-61	62-66	To address this issue ,	we propose a novel approach	57-104	57-104	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	1>2	enablement	enablement
D14-1088	62-66	67-76	we propose a novel approach	that collectively utilizes contextual information of terms in syntactic structures	57-104	57-104	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	1<2	elab-addition	elab-addition
D14-1088	67-76	77-94	that collectively utilizes contextual information of terms in syntactic structures	such that if the set of contexts of a term includes most of contexts of another term ,	57-104	57-104	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	1<2	elab-addition	elab-addition
D14-1088	77-94	95-104	such that if the set of contexts of a term includes most of contexts of another term ,	a subsumption relation between the two terms is inferred .	57-104	57-104	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	1<2	result	result
D14-1088	105-117	134-147	We apply this method to the task of taxonomy construction from scratch ,	that the proposed method is well complementary with previous methods of linguistic pattern matching	105-129	130-155	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	1>2	manner-means	manner-means
D14-1088	105-117	118-129	We apply this method to the task of taxonomy construction from scratch ,	where we introduce another novel graph-based algorithm for taxonomic structure induction .	105-129	105-129	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	We apply this method to the task of taxonomy construction from scratch , where we introduce another novel graph-based algorithm for taxonomic structure induction .	1<2	elab-addition	elab-addition
D14-1088	130-133	134-147	Our experiment results show	that the proposed method is well complementary with previous methods of linguistic pattern matching	130-155	130-155	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	1>2	attribution	attribution
D14-1088	62-66	134-147	we propose a novel approach	that the proposed method is well complementary with previous methods of linguistic pattern matching	57-104	130-155	To address this issue , we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term , a subsumption relation between the two terms is inferred .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	1<2	evaluation	evaluation
D14-1088	134-147	148-155	that the proposed method is well complementary with previous methods of linguistic pattern matching	and significantly improves recall and thus F-measure .	130-155	130-155	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure .	1<2	joint	joint
D14-1089	1-9	20-27	State-of-the-art fact extraction is heavily constrained by recall ,	We isolate this recall loss for NE slots	1-19	20-44	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	1>2	bg-compare	bg-compare
D14-1089	1-9	10-19	State-of-the-art fact extraction is heavily constrained by recall ,	as demonstrated by recent performance in TAC Slot Filling .	1-19	1-19	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	State-of-the-art fact extraction is heavily constrained by recall , as demonstrated by recent performance in TAC Slot Filling .	1<2	joint	joint
D14-1089	20-27	28-44	We isolate this recall loss for NE slots	by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	20-44	20-44	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	1<2	manner-means	manner-means
D14-1089	20-27	45-47	We isolate this recall loss for NE slots	Recall is critical	20-44	45-66	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	1<2	elab-addition	elab-addition
D14-1089	45-47	48-56	Recall is critical	as candidates never generated can never be recovered ,	45-66	45-66	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	1<2	exp-reason	exp-reason
D14-1089	48-56	57-66	as candidates never generated can never be recovered ,	whereas precision can always be increased in downstream processing .	45-66	45-66	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	Recall is critical as candidates never generated can never be recovered , whereas precision can always be increased in downstream processing .	1<2	progression	progression
D14-1089	20-27	67-83	We isolate this recall loss for NE slots	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	20-44	67-83	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	We provide precise , empirical confirmation of previously hypothesised sources of recall loss in slot filling .	1<2	elab-addition	elab-addition
D14-1089	84-99	102-118	While NE type constraints substantially reduce the search space with only a minor recall penalty ,	that 10 % to 39 % of slot fills will be entirely ignored by most systems .	84-118	84-118	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	1>2	contrast	contrast
D14-1089	100-101	102-118	we find	that 10 % to 39 % of slot fills will be entirely ignored by most systems .	84-118	84-118	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	1>2	attribution	attribution
D14-1089	20-27	102-118	We isolate this recall loss for NE slots	that 10 % to 39 % of slot fills will be entirely ignored by most systems .	20-44	84-118	We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers .	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	1<2	summary	summary
D14-1089	102-118	119-125	that 10 % to 39 % of slot fills will be entirely ignored by most systems .	One in six correct answers are lost	84-118	119-143	While NE type constraints substantially reduce the search space with only a minor recall penalty , we find that 10 % to 39 % of slot fills will be entirely ignored by most systems .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	1<2	progression	progression
D14-1089	119-125	126-131	One in six correct answers are lost	if coreference is not used ,	119-143	119-143	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	1<2	condition	condition
D14-1089	119-125	132-143	One in six correct answers are lost	but this can be mostly retained by simple name matching rules .	119-143	119-143	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	One in six correct answers are lost if coreference is not used , but this can be mostly retained by simple name matching rules .	1<2	contrast	contrast
D14-1090	1-7	78-89	Several state-of-the-art event extraction systems employ models	In this paper , we propose a new model for event extraction	1-35	78-102	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	1>2	bg-compare	bg-compare
D14-1090	1-7	8-20	Several state-of-the-art event extraction systems employ models	based on Support Vector Machines ( SVMs ) in a pipeline architecture ,	1-35	1-35	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	1<2	bg-general	bg-general
D14-1090	8-20	21-27	based on Support Vector Machines ( SVMs ) in a pipeline architecture ,	which fails to exploit the joint dependencies	1-35	1-35	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	1<2	elab-addition	elab-addition
D14-1090	21-27	28-35	which fails to exploit the joint dependencies	that typically exist among events and arguments .	1-35	1-35	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	1<2	elab-addition	elab-addition
D14-1090	36-44	53-61	While there have been attempts to overcome this limitation	it remains challenging to perform joint inference in MLNs	36-77	36-77	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	1>2	contrast	contrast
D14-1090	36-44	45-52	While there have been attempts to overcome this limitation	using Markov Logic Networks ( MLNs ) ,	36-77	36-77	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	1<2	manner-means	manner-means
D14-1090	1-7	53-61	Several state-of-the-art event extraction systems employ models	it remains challenging to perform joint inference in MLNs	1-35	36-77	Several state-of-the-art event extraction systems employ models based on Support Vector Machines ( SVMs ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments .	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	1<2	elab-addition	elab-addition
D14-1090	53-61	62-77	it remains challenging to perform joint inference in MLNs	when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	36-77	36-77	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	While there have been attempts to overcome this limitation using Markov Logic Networks ( MLNs ) , it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction .	1<2	elab-addition	elab-addition
D14-1090	78-89	90-98	In this paper , we propose a new model for event extraction	that combines the power of MLNs and SVMs ,	78-102	78-102	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	1<2	elab-addition	elab-addition
D14-1090	90-98	99-102	that combines the power of MLNs and SVMs ,	dwarfing their limitations .	78-102	78-102	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	1<2	elab-addition	elab-addition
D14-1090	78-89	103-113	In this paper , we propose a new model for event extraction	The key idea is to reliably learn and process high-dimensional features	78-102	103-148	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	1<2	elab-addition	elab-addition
D14-1090	103-113	114-116	The key idea is to reliably learn and process high-dimensional features	using SVMs ;	103-148	103-148	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	1<2	manner-means	manner-means
D14-1090	103-113	117-129	The key idea is to reliably learn and process high-dimensional features	encode the output of SVMs as low-dimensional , soft formulas in MLNs ;	103-148	103-148	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	1<2	joint	joint
D14-1090	117-129	130-138	encode the output of SVMs as low-dimensional , soft formulas in MLNs ;	and use the superior joint inferencing power of MLNs	103-148	103-148	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	1<2	joint	joint
D14-1090	130-138	139-148	and use the superior joint inferencing power of MLNs	to enforce joint consistency constraints over the soft formulas .	103-148	103-148	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	The key idea is to reliably learn and process high-dimensional features using SVMs ; encode the output of SVMs as low-dimensional , soft formulas in MLNs ; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas .	1<2	enablement	enablement
D14-1090	78-89	149-172	In this paper , we propose a new model for event extraction	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	78-102	149-172	In this paper , we propose a new model for event extraction that combines the power of MLNs and SVMs , dwarfing their limitations .	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	1<2	evaluation	evaluation
D14-1090	149-172	173-208	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	149-172	173-208	We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013 , 2011 and 2009 Genia shared task datasets .	Our approach yields the best F1 score to date on the BioNLP'13 ( 53.61 ) and BioNLP'11 ( 58.07 ) datasets and the second-best F1 score to date on the BioNLP'09 dataset ( 58.16 ) .	1<2	result	result
D14-1091	1-10	83-89	Stress is a useful cue for English word segmentation .	We devise Adaptor Grammar word segmentation models	1-10	83-115	Stress is a useful cue for English word segmentation .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	1>2	bg-goal	bg-goal
D14-1091	11-18	19-30	A wide range of computational models have found	that stress cues enable a 2-10 % improvement in segmentation accuracy ,	11-51	11-51	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	1>2	attribution	attribution
D14-1091	1-10	19-30	Stress is a useful cue for English word segmentation .	that stress cues enable a 2-10 % improvement in segmentation accuracy ,	1-10	11-51	Stress is a useful cue for English word segmentation .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	1<2	exp-evidence	exp-evidence
D14-1091	19-30	31-37	that stress cues enable a 2-10 % improvement in segmentation accuracy ,	depending on the kind of model ,	11-51	11-51	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	1<2	elab-addition	elab-addition
D14-1091	19-30	38-40	that stress cues enable a 2-10 % improvement in segmentation accuracy ,	by using input	11-51	11-51	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	1<2	manner-means	manner-means
D14-1091	38-40	41-46	by using input	that has been annotated with stress	11-51	11-51	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	1<2	elab-addition	elab-addition
D14-1091	41-46	47-51	that has been annotated with stress	using a pronouncing dictionary .	11-51	11-51	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	A wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary .	1<2	manner-means	manner-means
D14-1091	1-10	52-65	Stress is a useful cue for English word segmentation .	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	1-10	52-65	Stress is a useful cue for English word segmentation .	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	1<2	contrast	contrast
D14-1091	52-65	66-82	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	52-65	66-82	However , stress is neither invariably produced nor unambiguously identifiable in real speech .	Heavy syllables , i.e. those with long vowels or syllable codas , attract stress in English .	1<2	elab-example	elab-example
D14-1091	83-89	90-101	We devise Adaptor Grammar word segmentation models	that exploit either stress , or syllable weight , or both ,	83-115	83-115	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	1<2	elab-addition	elab-addition
D14-1091	90-101	102-115	that exploit either stress , or syllable weight , or both ,	and evaluate the utility of syllable weight as a cue to word boundaries .	83-115	83-115	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	1<2	joint	joint
D14-1091	116-118	119-131	Our results suggest	that syllable weight encodes largely the same information for word segmentation in English	116-137	116-137	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	1>2	attribution	attribution
D14-1091	83-89	119-131	We devise Adaptor Grammar word segmentation models	that syllable weight encodes largely the same information for word segmentation in English	83-115	116-137	We devise Adaptor Grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	1<2	evaluation	evaluation
D14-1091	119-131	132-137	that syllable weight encodes largely the same information for word segmentation in English	that annotated dictionary stress does .	116-137	116-137	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does .	1<2	elab-addition	elab-addition
D14-1092	19-26	27-35	Inspired by the "products of experts" idea ,	our joint model firstly combines two generative models ,	19-55	19-55	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	1>2	elab-addition	elab-addition
D14-1092	1-18	27-35	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	our joint model firstly combines two generative models ,	1-18	19-55	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	1<2	elab-process_step	elab-process_step
D14-1092	27-35	36-48	our joint model firstly combines two generative models ,	which are word-based hierarchical Dirichlet process model and character-based hidden Markov model ,	19-55	19-55	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	1<2	elab-enumember	elab-enumember
D14-1092	27-35	49-55	our joint model firstly combines two generative models ,	by simply multiplying their probabilities together .	19-55	19-55	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	Inspired by the "products of experts" idea , our joint model firstly combines two generative models , which are word-based hierarchical Dirichlet process model and character-based hidden Markov model , by simply multiplying their probabilities together .	1<2	manner-means	manner-means
D14-1092	56-63	83-91	Gibbs sampling is used for model inference .	by using it to initializing the Gibbs sampler .	56-63	64-91	Gibbs sampling is used for model inference .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	1>2	elab-addition	elab-addition
D14-1092	64-74	75-82	In order to further combine the strength of goodness-based model ,	we then integrated nVBE into our joint model	64-91	64-91	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	1>2	enablement	enablement
D14-1092	1-18	75-82	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	we then integrated nVBE into our joint model	1-18	64-91	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	1<2	elab-process_step	elab-process_step
D14-1092	75-82	83-91	we then integrated nVBE into our joint model	by using it to initializing the Gibbs sampler .	64-91	64-91	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	In order to further combine the strength of goodness-based model , we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler .	1<2	manner-means	manner-means
D14-1092	92-100	115-129	We conduct our experiments on PKU and MSRA datasets	that the joint model achieves much better results than all of its component models .	92-107	108-129	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	1>2	manner-means	manner-means
D14-1092	92-100	101-107	We conduct our experiments on PKU and MSRA datasets	provided by the second SIGHAN bakeoff .	92-107	92-107	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff .	1<2	elab-addition	elab-addition
D14-1092	108-114	115-129	Test results on these two datasets show	that the joint model achieves much better results than all of its component models .	108-129	108-129	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	1>2	attribution	attribution
D14-1092	1-18	115-129	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	that the joint model achieves much better results than all of its component models .	1-18	108-129	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Test results on these two datasets show that the joint model achieves much better results than all of its component models .	1<2	evaluation	evaluation
D14-1092	130-134	135-143	Statistical significance tests also show	that it is significantly better than state-of-the-art systems ,	130-148	130-148	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	1>2	attribution	attribution
D14-1092	1-18	135-143	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	that it is significantly better than state-of-the-art systems ,	1-18	130-148	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	1<2	evaluation	evaluation
D14-1092	135-143	144-148	that it is significantly better than state-of-the-art systems ,	achieving the highest F-scores .	130-148	130-148	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	Statistical significance tests also show that it is significantly better than state-of-the-art systems , achieving the highest F-scores .	1<2	elab-addition	elab-addition
D14-1092	149-152	160-166	Finally , analysis indicates	the joint model has a stronger ability	149-178	149-178	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	1>2	attribution	attribution
D14-1092	153-159	160-166	that compared with nVBE and HDP ,	the joint model has a stronger ability	149-178	149-178	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	1>2	comparison	comparison
D14-1092	1-18	160-166	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	the joint model has a stronger ability	1-18	149-178	In this paper , we propose a joint model for unsupervised Chinese word segmentation ( CWS ) .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	1<2	evaluation	evaluation
D14-1092	160-166	167-178	the joint model has a stronger ability	to solve both combinational and overlapping ambiguities in Chinese word segmentation .	149-178	149-178	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	Finally , analysis indicates that compared with nVBE and HDP , the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation .	1<2	enablement	enablement
D14-1093	1-12	29-44	Supervised methods have been the dominant approach for Chinese word segmentation .	In this paper , we study the problem of obtaining partial annotation from freely available data	1-12	29-53	Supervised methods have been the dominant approach for Chinese word segmentation .	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	1>2	bg-goal	bg-goal
D14-1093	1-12	13-17	Supervised methods have been the dominant approach for Chinese word segmentation .	The performance can drop significantly	1-12	13-28	Supervised methods have been the dominant approach for Chinese word segmentation .	The performance can drop significantly when the test domain is different from the training domain .	1<2	elab-addition	elab-addition
D14-1093	13-17	18-28	The performance can drop significantly	when the test domain is different from the training domain .	13-28	13-28	The performance can drop significantly when the test domain is different from the training domain .	The performance can drop significantly when the test domain is different from the training domain .	1<2	temporal	temporal
D14-1093	29-44	45-53	In this paper , we study the problem of obtaining partial annotation from freely available data	to help Chinese word segmentation on different domains .	29-53	29-53	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	1<2	enablement	enablement
D14-1093	29-44	54-67	In this paper , we study the problem of obtaining partial annotation from freely available data	Different sources of free annotations are transformed into a unified form of partial annotation	29-53	54-84	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	1<2	manner-means	manner-means
D14-1093	54-67	68-74	Different sources of free annotations are transformed into a unified form of partial annotation	and a variant CRF model is used	54-84	54-84	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	1<2	joint	joint
D14-1093	68-74	75-84	and a variant CRF model is used	to leverage both fully and partially annotated data consistently .	54-84	54-84	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently .	1<2	enablement	enablement
D14-1093	85-87	88-100	Experimental results show	that the Chinese word segmentation model benefits from free partially annotated data .	85-100	85-100	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	1>2	attribution	attribution
D14-1093	29-44	88-100	In this paper , we study the problem of obtaining partial annotation from freely available data	that the Chinese word segmentation model benefits from free partially annotated data .	29-53	85-100	In this paper , we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains .	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	1<2	evaluation	evaluation
D14-1093	88-100	101-110	that the Chinese word segmentation model benefits from free partially annotated data .	On the SIGHAN Bakeoff 2010 data , we achieve results	85-100	101-121	Experimental results show that the Chinese word segmentation model benefits from free partially annotated data .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	1<2	result	result
D14-1093	101-110	111-116	On the SIGHAN Bakeoff 2010 data , we achieve results	that are competitive to the best	101-121	101-121	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	1<2	elab-addition	elab-addition
D14-1093	111-116	117-121	that are competitive to the best	reported in the literature .	101-121	101-121	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	On the SIGHAN Bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .	1<2	elab-addition	elab-addition
D14-1094	1-12	50-59	Most studies on statistical Korean word spacing do not utilize the information	this paper proposes a structural SVM-based Korean word spacing method	1-25	45-70	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	1>2	bg-compare	bg-compare
D14-1094	1-12	13-17	Most studies on statistical Korean word spacing do not utilize the information	provided by the input sentence	1-25	1-25	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	1<2	elab-addition	elab-addition
D14-1094	18-19	20-25	and assume	that it was completely concatenated .	1-25	1-25	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	1>2	attribution	attribution
D14-1094	13-17	20-25	provided by the input sentence	that it was completely concatenated .	1-25	1-25	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	1<2	joint	joint
D14-1094	1-12	26-39	Most studies on statistical Korean word spacing do not utilize the information	This makes the word spacer ignore the correct spaced parts of the input sentence	1-25	26-44	Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated .	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	1<2	result	result
D14-1094	26-39	40-44	This makes the word spacer ignore the correct spaced parts of the input sentence	and erroneously alter them .	26-44	26-44	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them .	1<2	joint	joint
D14-1094	45-49	50-59	To overcome such limit ,	this paper proposes a structural SVM-based Korean word spacing method	45-70	45-70	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	1>2	enablement	enablement
D14-1094	50-59	60-70	this paper proposes a structural SVM-based Korean word spacing method	that can utilize the space information of the input sentence .	45-70	45-70	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	1<2	elab-addition	elab-addition
D14-1094	71-80	81-88	The experiment on sentences with 10 % spacing errors showed	that our method achieved 96.81 % F-score ,	71-100	71-100	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	1>2	attribution	attribution
D14-1094	50-59	81-88	this paper proposes a structural SVM-based Korean word spacing method	that our method achieved 96.81 % F-score ,	45-70	71-100	To overcome such limit , this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	1<2	evaluation	evaluation
D14-1094	81-88	89-100	that our method achieved 96.81 % F-score ,	while the basic structural SVM method only achieved 92.53 % F-score .	71-100	71-100	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	1<2	contrast	contrast
D14-1094	81-88	101-109	that our method achieved 96.81 % F-score ,	The more the input sentence was correctly spaced ,	71-100	101-116	The experiment on sentences with 10 % spacing errors showed that our method achieved 96.81 % F-score , while the basic structural SVM method only achieved 92.53 % F-score .	The more the input sentence was correctly spaced , the more accurately our method performed .	1<2	summary	summary
D14-1094	101-109	110-116	The more the input sentence was correctly spaced ,	the more accurately our method performed .	101-116	101-116	The more the input sentence was correctly spaced , the more accurately our method performed .	The more the input sentence was correctly spaced , the more accurately our method performed .	1<2	joint	joint
D14-1095	1-14	28-40	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	In this paper , we augment a state-of-the-art KWS system with sub-word units	1-14	28-56	We explore the impact of morphological segmentation on keyword spotting ( KWS ) .	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	1>2	bg-goal	bg-goal
D14-1095	15-18	19-27	Despite potential benefits ,	state-of-the-art KWS systems do not use morphological information .	15-27	15-27	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	1>2	contrast	contrast
D14-1095	19-27	28-40	state-of-the-art KWS systems do not use morphological information .	In this paper , we augment a state-of-the-art KWS system with sub-word units	15-27	28-56	Despite potential benefits , state-of-the-art KWS systems do not use morphological information .	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	1>2	bg-compare	bg-compare
D14-1095	28-40	41-48	In this paper , we augment a state-of-the-art KWS system with sub-word units	derived from supervised and unsupervised morphological segmentations ,	28-56	28-56	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	1<2	elab-addition	elab-addition
D14-1095	41-48	49-56	derived from supervised and unsupervised morphological segmentations ,	and compare with phonetic and syllabic segmentations .	28-56	28-56	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	1<2	joint	joint
D14-1095	57-59	60-68	Our experiments demonstrate	that morphemes improve overall performance of KWS systems .	57-68	57-68	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	1>2	attribution	attribution
D14-1095	28-40	60-68	In this paper , we augment a state-of-the-art KWS system with sub-word units	that morphemes improve overall performance of KWS systems .	28-56	57-68	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	1<2	evaluation	evaluation
D14-1095	60-68	69-79	that morphemes improve overall performance of KWS systems .	Syllabic units , however , rival the performance of morphological units	57-68	69-84	Our experiments demonstrate that morphemes improve overall performance of KWS systems .	Syllabic units , however , rival the performance of morphological units when used in KWS .	1<2	contrast	contrast
D14-1095	69-79	80-84	Syllabic units , however , rival the performance of morphological units	when used in KWS .	69-84	69-84	Syllabic units , however , rival the performance of morphological units when used in KWS .	Syllabic units , however , rival the performance of morphological units when used in KWS .	1<2	temporal	temporal
D14-1095	85-93	94-99	By combining morphological , phonetic and syllabic segmentations ,	we demonstrate substantial performance gains .	85-99	85-99	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	1>2	manner-means	manner-means
D14-1095	28-40	94-99	In this paper , we augment a state-of-the-art KWS system with sub-word units	we demonstrate substantial performance gains .	28-56	85-99	In this paper , we augment a state-of-the-art KWS system with sub-word units derived from supervised and unsupervised morphological segmentations , and compare with phonetic and syllabic segmentations .	By combining morphological , phonetic and syllabic segmentations , we demonstrate substantial performance gains .	1<2	summary	summary
D14-1096	1-15	16-19	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	We use parallel data	1-15	16-29	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	1<2	manner-means	manner-means
D14-1096	16-19	20-29	We use parallel data	to transfer part-of-speech information from resource-rich to resource-poor languages .	16-29	16-29	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	1<2	enablement	enablement
D14-1096	16-19	30-39	We use parallel data	Additionally , we use a small amount of annotated data	16-29	30-65	We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages .	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	1<2	progression	progression
D14-1096	30-39	40-54	Additionally , we use a small amount of annotated data	to learn to "correct" errors from projected approach such as tagset mismatch between languages ,	30-65	30-65	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	1<2	enablement	enablement
D14-1096	40-54	55-65	to learn to "correct" errors from projected approach such as tagset mismatch between languages ,	achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	30-65	30-65	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	Additionally , we use a small amount of annotated data to learn to "correct" errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance ( 91.3 % ) across 8 languages .	1<2	result	result
D14-1096	1-15	66-74	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	Our approach is based on modest data requirements ,	1-15	66-80	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	Our approach is based on modest data requirements , and uses minimum divergence classification .	1<2	elab-addition	elab-addition
D14-1096	66-74	75-80	Our approach is based on modest data requirements ,	and uses minimum divergence classification .	66-80	66-80	Our approach is based on modest data requirements , and uses minimum divergence classification .	Our approach is based on modest data requirements , and uses minimum divergence classification .	1<2	joint	joint
D14-1096	81-82	91-96	For situations	we propose an alternate method ,	81-108	81-108	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	1>2	temporal	temporal
D14-1096	81-82	83-90	For situations	where no universal tagset mapping is available ,	81-108	81-108	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	1<2	elab-addition	elab-addition
D14-1096	1-15	91-96	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	we propose an alternate method ,	1-15	81-108	In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	1<2	progression	progression
D14-1096	91-96	97-108	we propose an alternate method ,	resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	81-108	81-108	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	For situations where no universal tagset mapping is available , we propose an alternate method , resulting in state-of-the-art 85.6 % accuracy on the resource-poor language Malagasy .	1<2	result	result
D14-1097	1-10	65-77	Active learning ( AL ) consists of asking human annotators	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks	1-30	65-109	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	1>2	bg-goal	bg-goal
D14-1097	1-10	11-15	Active learning ( AL ) consists of asking human annotators	to annotate automatically selected data	1-30	1-30	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	1<2	enablement	enablement
D14-1097	11-15	16-30	to annotate automatically selected data	that are assumed to bring the most benefit in the creation of a classifier .	1-30	1-30	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	1<2	elab-addition	elab-addition
D14-1097	1-10	31-41	Active learning ( AL ) consists of asking human annotators	AL allows to learn accurate systems with much less annotated data	1-30	31-64	Active learning ( AL ) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier .	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	1<2	elab-addition	elab-addition
D14-1097	31-41	42-51	AL allows to learn accurate systems with much less annotated data	than what is required by pure supervised learning algorithms ,	31-64	31-64	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	1<2	comparison	comparison
D14-1097	31-41	52-64	AL allows to learn accurate systems with much less annotated data	hence limiting the tedious effort of annotating a large collection of data .	31-64	31-64	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms , hence limiting the tedious effort of annotating a large collection of data .	1<2	result	result
D14-1097	65-77	78-83	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks	( in a partially-labeled scenario )	65-109	65-109	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	1<2	elab-addition	elab-addition
D14-1097	65-77	84-96	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks	tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks :	65-109	65-109	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	1<2	elab-addition	elab-addition
D14-1097	84-96	97-109	tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks :	phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	65-109	65-109	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	We experimentally investigate the behavior of several AL strategies for sequence labeling tasks ( in a partially-labeled scenario ) tailored on Partially-Labeled Conditional Random Fields , on four sequence labeling tasks : phrase chunking , part-of-speech tagging , named-entity recognition , and bio-entity recognition .	1<2	elab-enumember	elab-enumember
D14-1098	1-16	17-35	In this paper , we propose novel structured language modeling methods for code mixing speech recognition	by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	1-35	1-35	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	1<2	manner-means	manner-means
D14-1098	1-16	36-42	In this paper , we propose novel structured language modeling methods for code mixing speech recognition	Code mixing data is not abundantly available	1-35	36-47	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Code mixing data is not abundantly available for training language models .	1<2	elab-addition	elab-addition
D14-1098	36-42	43-47	Code mixing data is not abundantly available	for training language models .	36-47	36-47	Code mixing data is not abundantly available for training language models .	Code mixing data is not abundantly available for training language models .	1<2	elab-addition	elab-addition
D14-1098	1-16	48-60	In this paper , we propose novel structured language modeling methods for code mixing speech recognition	Our proposed methods successfully alleviate this core problem for code mixing speech recognition	1-35	48-74	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	1<2	bg-goal	bg-goal
D14-1098	48-60	61-74	Our proposed methods successfully alleviate this core problem for code mixing speech recognition	by using bilingual data to train a structured language model with syntactic constraint .	48-74	48-74	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	Our proposed methods successfully alleviate this core problem for code mixing speech recognition by using bilingual data to train a structured language model with syntactic constraint .	1<2	manner-means	manner-means
D14-1098	75-79	80-93	Linguists and bilingual speakers found	that code switch do not happen between the functional head and its complements .	75-93	75-93	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	1>2	attribution	attribution
D14-1098	80-93	94-119	that code switch do not happen between the functional head and its complements .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	75-93	94-119	Linguists and bilingual speakers found that code switch do not happen between the functional head and its complements .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	1>2	cause	cause
D14-1098	1-16	94-119	In this paper , we propose novel structured language modeling methods for code mixing speech recognition	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	1-35	94-119	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	1<2	elab-addition	elab-addition
D14-1098	94-119	120-127	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	The constrained code switch language model is obtained	94-119	120-152	We propose to learn the code mixing language model from bilingual data with this constraint in a weighted finite state transducer ( WFST ) framework .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	1<2	elab-addition	elab-addition
D14-1098	120-127	128-138	The constrained code switch language model is obtained	by first expanding the search network with a translation model ,	120-152	120-152	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	1<2	manner-means	manner-means
D14-1098	128-138	139-152	by first expanding the search network with a translation model ,	and then using parsing to restrict paths to those permissible under the constraint .	120-152	120-152	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	The constrained code switch language model is obtained by first expanding the search network with a translation model , and then using parsing to restrict paths to those permissible under the constraint .	1<2	elab-process_step	elab-process_step
D14-1098	153-158	178-205	We implement and compare two approaches	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	153-177	178-205	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	1>2	manner-means	manner-means
D14-1098	153-158	159-165	We implement and compare two approaches	- lattice parsing enables a sequential coupling	153-177	153-177	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	1<2	elab-addition	elab-addition
D14-1098	159-165	166-177	- lattice parsing enables a sequential coupling	whereas partial parsing enables a tight coupling between parsing and filtering .	153-177	153-177	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	We implement and compare two approaches - lattice parsing enables a sequential coupling whereas partial parsing enables a tight coupling between parsing and filtering .	1<2	contrast	contrast
D14-1098	178-205	206-248	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	178-205	206-248	We tested our system on a lecture speech dataset with 16 % embedded second language , and on a lunch conversation dataset with 20 % embedded language .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	1>2	evaluation	evaluation
D14-1098	1-16	206-248	In this paper , we propose novel structured language modeling methods for code mixing speech recognition	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	1-35	206-248	In this paper , we propose novel structured language modeling methods for code mixing speech recognition by incorporating a well-known syntactic constraint for switching code , namely the Functional Head Constraint ( FHC ) .	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	1<2	evaluation	evaluation
D14-1098	206-248	249-268	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively ,	206-248	249-282	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	1<2	elab-addition	elab-addition
D14-1098	249-268	269-282	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively ,	and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	249-282	249-282	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	It outperforms the interpolated language model by 3.7 % and 5.6 % in terms of word error rate relatively , and outperforms the adapted language model by 2.6 % and 4.6 % relatively .	1<2	joint	joint
D14-1098	206-248	283-292	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	Our proposed approach avoids making early decisions on code-switch boundaries	206-248	283-298	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	1<2	elab-addition	elab-addition
D14-1098	283-292	293-298	Our proposed approach avoids making early decisions on code-switch boundaries	and is therefore more robust .	283-298	283-298	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust .	1<2	joint	joint
D14-1098	206-248	299-306	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	We address the code switch data scarcity challenge	206-248	299-314	Our language models with lattice parsing and partial parsing reduce word error rates from a baseline mixed language model by 3.8 % and 3.9 % in terms of word error rate relatively on the average on the first and second tasks respectively .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	1<2	elab-addition	elab-addition
D14-1098	299-306	307-314	We address the code switch data scarcity challenge	by using bilingual data with syntactic structure .	299-314	299-314	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	We address the code switch data scarcity challenge by using bilingual data with syntactic structure .	1<2	elab-addition	elab-addition
D14-1099	1-16	45-58	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers ,	In this paper we introduce the first such oracle , for a non-projective parser	1-21	45-63	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	1>2	null	null
D14-1099	1-16	17-21	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers ,	without sacrificing parsing efficiency .	1-21	1-21	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	1<2	elab-addition	elab-addition
D14-1099	1-16	22-31	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers ,	However , this enhancement is limited to projective parsing ,	1-21	22-44	The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers , without sacrificing parsing efficiency .	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	1<2	contrast	contrast
D14-1099	22-31	32-41	However , this enhancement is limited to projective parsing ,	and dynamic oracles have not yet been implemented for parsers	22-44	22-44	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	1<2	joint	joint
D14-1099	32-41	42-44	and dynamic oracles have not yet been implemented for parsers	supporting non-projectivity .	22-44	22-44	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	However , this enhancement is limited to projective parsing , and dynamic oracles have not yet been implemented for parsers supporting non-projectivity .	1<2	elab-addition	elab-addition
D14-1099	45-58	59-63	In this paper we introduce the first such oracle , for a non-projective parser	based on Attardi's parser .	45-63	45-63	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	1<2	bg-general	bg-general
D14-1099	64-65	66-87	We show	that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	64-87	64-87	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	1>2	attribution	attribution
D14-1099	45-58	66-87	In this paper we introduce the first such oracle , for a non-projective parser	that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	45-63	64-87	In this paper we introduce the first such oracle , for a non-projective parser based on Attardi's parser .	We show that training with this oracle improves parsing accuracy over a conventional ( static ) oracle on a wide range of datasets .	1<2	evaluation	evaluation
D14-1100	1-10,18-26	27-34	The syntactic ambiguity of a transitive verb ( Vt ) <*> has long been a problem in Chinese parsing .	In this paper , we propose a classifier	1-26	27-42	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	1>2	bg-compare	bg-compare
D14-1100	1-10,18-26	11-17	The syntactic ambiguity of a transitive verb ( Vt ) <*> has long been a problem in Chinese parsing .	followed by a noun ( N )	1-26	1-26	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	The syntactic ambiguity of a transitive verb ( Vt ) followed by a noun ( N ) has long been a problem in Chinese parsing .	1<2	elab-addition	elab-addition
D14-1100	27-34	35-42	In this paper , we propose a classifier	to resolve the ambiguity of Vt-N structures .	27-42	27-42	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	1<2	enablement	enablement
D14-1100	27-34	43-54	In this paper , we propose a classifier	The design of the classifier is based on three important guidelines ,	27-42	43-74	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	1<2	elab-addition	elab-addition
D14-1100	43-54	55-61	The design of the classifier is based on three important guidelines ,	namely , adopting linguistically motivated features ,	43-74	43-74	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	1<2	elab-enumember	elab-enumember
D14-1100	55-61	62-74	namely , adopting linguistically motivated features ,	using all available resources , and easy integration into a parsing model .	43-74	43-74	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	1<2	joint	joint
D14-1100	55-61	75-88	namely , adopting linguistically motivated features ,	The linguistically motivated features include semantic relations , context , and morphological structures ;	43-74	75-104	The design of the classifier is based on three important guidelines , namely , adopting linguistically motivated features , using all available resources , and easy integration into a parsing model .	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	1<2	elab-addition	elab-addition
D14-1100	75-88	89-104	The linguistically motivated features include semantic relations , context , and morphological structures ;	and the available resources are treebank , thesaurus , affix database , and large corpora .	75-104	75-104	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	The linguistically motivated features include semantic relations , context , and morphological structures ; and the available resources are treebank , thesaurus , affix database , and large corpora .	1<2	joint	joint
D14-1100	27-34	105-110	In this paper , we propose a classifier	We also propose two learning approaches	27-42	105-128	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	1<2	elab-addition	elab-addition
D14-1100	105-110	111-117	We also propose two learning approaches	that resolve the problem of data sparseness	105-128	105-128	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	1<2	elab-addition	elab-addition
D14-1100	111-117	118-128	that resolve the problem of data sparseness	by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	105-128	105-128	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data .	1<2	manner-means	manner-means
D14-1100	129-132	133-142	Our experiment results show	that the Vt-N classifier outperforms the current PCFG parser .	129-142	129-142	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	1>2	attribution	attribution
D14-1100	27-34	133-142	In this paper , we propose a classifier	that the Vt-N classifier outperforms the current PCFG parser .	27-42	129-142	In this paper , we propose a classifier to resolve the ambiguity of Vt-N structures .	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	1<2	evaluation	evaluation
D14-1100	133-142	143-161	that the Vt-N classifier outperforms the current PCFG parser .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	129-142	143-161	Our experiment results show that the Vt-N classifier outperforms the current PCFG parser .	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	1<2	progression	progression
D14-1100	162-167	168-180	Evaluation of the learning approaches indicates	that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	162-180	162-180	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	1>2	attribution	attribution
D14-1100	143-161	168-180	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	143-161	162-180	Furthermore , it can be easily and effectively integrated into the PCFG parser and general statistical parsing models .	Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction .	1<2	evaluation	evaluation
D14-1194	1-6	7-14	We describe a convolutional neural network	that learns feature representations for short textual posts	1-20	1-20	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	1<2	elab-addition	elab-addition
D14-1194	7-14	15-20	that learns feature representations for short textual posts	using hashtags as a supervised signal.	1-20	1-20	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	1<2	manner-means	manner-means
D14-1194	1-6	21-31	We describe a convolutional neural network	The proposed approach is trained on up to 5.5 billion words	1-20	21-36	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	1<2	elab-addition	elab-addition
D14-1194	21-31	32-36	The proposed approach is trained on up to 5.5 billion words	predicting 100,000 possible hashtags .	21-36	21-36	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags .	1<2	elab-addition	elab-addition
D14-1194	37-48	51-56,63-70	As well as strong performance on the hashtag prediction task itself ,	that its learned representation of text <*> is useful for other tasks as well .	37-70	37-70	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	1>2	joint	joint
D14-1194	49-50	51-56,63-70	we show	that its learned representation of text <*> is useful for other tasks as well .	37-70	37-70	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	1>2	attribution	attribution
D14-1194	1-6	51-56,63-70	We describe a convolutional neural network	that its learned representation of text <*> is useful for other tasks as well .	1-20	37-70	We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal.	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	1<2	evaluation	evaluation
D14-1194	51-56,63-70	57-62	that its learned representation of text <*> is useful for other tasks as well .	( ignoring the hashtag labels )	37-70	37-70	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	1<2	elab-addition	elab-addition
D14-1194	51-56,63-70	71-83	that its learned representation of text <*> is useful for other tasks as well .	To that end , we present results on a document recommendation task ,	37-70	71-92	As well as strong performance on the hashtag prediction task itself , we show that its learned representation of text ( ignoring the hashtag labels ) is useful for other tasks as well .	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	1<2	elab-addition	elab-addition
D14-1194	71-83	84-92	To that end , we present results on a document recommendation task ,	where it also outperforms a number of baselines .	71-92	71-92	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	To that end , we present results on a document recommendation task , where it also outperforms a number of baselines .	1<2	elab-addition	elab-addition
D14-1195	1-9	10-17	In this paper , we provide a new method	for decoding tree transduction based sentence compression models	1-29	1-29	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	1<2	elab-addition	elab-addition
D14-1195	1-9	18-23	In this paper , we provide a new method	augmented with language model scores ,	1-29	1-29	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	1<2	elab-addition	elab-addition
D14-1195	1-9	24-29	In this paper , we provide a new method	by jointly decoding two components .	1-29	1-29	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	1<2	manner-means	manner-means
D14-1195	1-9	30-42	In this paper , we provide a new method	In our proposed solution , rich local discriminative features can be easily integrated	1-29	30-47	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	1<2	elab-addition	elab-addition
D14-1195	30-42	43-47	In our proposed solution , rich local discriminative features can be easily integrated	without increasing computational complexity .	30-47	30-47	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity .	1<2	condition	condition
D14-1195	48-51	62-66	Utilizing an unobvious fact	we conduct efficient joint decoding	48-71	48-71	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	1>2	elab-addition	elab-addition
D14-1195	48-51	52-61	Utilizing an unobvious fact	that the resulted two components can be independently decoded ,	48-71	48-71	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	1<2	elab-addition	elab-addition
D14-1195	1-9	62-66	In this paper , we provide a new method	we conduct efficient joint decoding	1-29	48-71	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	1<2	elab-addition	elab-addition
D14-1195	62-66	67-71	we conduct efficient joint decoding	based on dual decomposition .	48-71	48-71	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	Utilizing an unobvious fact that the resulted two components can be independently decoded , we conduct efficient joint decoding based on dual decomposition .	1<2	bg-general	bg-general
D14-1195	72-74	75-82	Experimental results show	that our method outperforms traditional beam search decoding	72-88	72-88	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	1>2	attribution	attribution
D14-1195	1-9	75-82	In this paper , we provide a new method	that our method outperforms traditional beam search decoding	1-29	72-88	In this paper , we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores , by jointly decoding two components .	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	1<2	evaluation	evaluation
D14-1195	75-82	83-88	that our method outperforms traditional beam search decoding	and achieves the state-of-the-art performance .	72-88	72-88	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance .	1<2	joint	joint
D14-1196	1-9	61-85	The current state-of-the-art single-document summarization method generates a summary	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT	1-41	61-92	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	1>2	contrast	contrast
D14-1196	1-9	10-19	The current state-of-the-art single-document summarization method generates a summary	by solving a Tree Knapsack Problem ( TKP ) ,	1-41	1-41	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	1<2	manner-means	manner-means
D14-1196	10-19	20-41	by solving a Tree Knapsack Problem ( TKP ) ,	which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	1-41	1-41	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	1<2	elab-addition	elab-addition
D14-1196	1-9	42-47	The current state-of-the-art single-document summarization method generates a summary	We can obtain a gold DEP-DT	1-41	42-60	The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem ( TKP ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( DEP-DT ) of a document .	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	1<2	elab-addition	elab-addition
D14-1196	42-47	48-60	We can obtain a gold DEP-DT	by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	42-60	42-60	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree ( RST-DT ) .	1<2	manner-means	manner-means
D14-1196	61-85	99-104	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT	we propose a novel discourse parser	61-92	93-110	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	1>2	bg-compare	bg-compare
D14-1196	61-85	86-92	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT	obtained from an automatically parsed RST-DT .	61-92	61-92	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	However , there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT .	1<2	elab-addition	elab-addition
D14-1196	93-98	99-104	To improve the ROUGE score ,	we propose a novel discourse parser	93-110	93-110	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	1>2	enablement	enablement
D14-1196	99-104	105-110	we propose a novel discourse parser	that directly generates the DEP-DT .	93-110	93-110	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	1<2	elab-addition	elab-addition
D14-1196	111-114	115-128	The evaluation results showed	that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser ,	111-142	111-142	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	1>2	attribution	attribution
D14-1196	99-104	115-128	we propose a novel discourse parser	that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser ,	93-110	111-142	To improve the ROUGE score , we propose a novel discourse parser that directly generates the DEP-DT .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	1<2	evaluation	evaluation
D14-1196	115-128	129-142	that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser ,	and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	111-142	111-142	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser , and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT .	1<2	joint	joint
D14-1197	1-2	3-8	We show	that semantic relationships can be used	1-26	1-26	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	1>2	attribution	attribution
D14-1197	3-8	27-34	that semantic relationships can be used	In this paper , we present a method	1-26	27-48	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	1>2	bg-goal	bg-goal
D14-1197	3-8	9-21	that semantic relationships can be used	to improve word alignment , in addition to the lexical and syntactic features	1-26	1-26	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	1<2	enablement	enablement
D14-1197	9-21	22-26	to improve word alignment , in addition to the lexical and syntactic features	that are typically used .	1-26	1-26	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	We show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used .	1<2	elab-addition	elab-addition
D14-1197	27-34	35-39	In this paper , we present a method	based on a neural network	27-48	27-48	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	1<2	bg-general	bg-general
D14-1197	27-34	40-48	In this paper , we present a method	to automatically derive word similarity from monolingual data .	27-48	27-48	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	1<2	elab-addition	elab-addition
D14-1197	27-34	49-52	In this paper , we present a method	We present an extension	27-48	49-61	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	We present an extension to word alignment models that exploits word similarity .	1<2	elab-addition	elab-addition
D14-1197	49-52	53-56	We present an extension	to word alignment models	49-61	49-61	We present an extension to word alignment models that exploits word similarity .	We present an extension to word alignment models that exploits word similarity .	1<2	elab-addition	elab-addition
D14-1197	49-52	57-61	We present an extension	that exploits word similarity .	49-61	49-61	We present an extension to word alignment models that exploits word similarity .	We present an extension to word alignment models that exploits word similarity .	1<2	elab-addition	elab-addition
D14-1197	27-34	62-83	In this paper , we present a method	Our experiments , in both large-scale and resource-limited settings , show improvements in word alignment tasks as well as translation tasks .	27-48	62-83	In this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data .	Our experiments , in both large-scale and resource-limited settings , show improvements in word alignment tasks as well as translation tasks .	1<2	evaluation	evaluation
D14-1198	1-9	10-34	In this paper , we propose a new framework	that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation ,	1-49	1-49	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	1<2	elab-addition	elab-addition
D14-1198	10-34	35-39	that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation ,	and extracts all of them	1-49	1-49	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	1<2	joint	joint
D14-1198	35-39	40-44	and extracts all of them	using one single joint model	1-49	1-49	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	1<2	manner-means	manner-means
D14-1198	35-39	45-49	and extracts all of them	based on structured prediction .	1-49	1-49	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	1<2	bg-general	bg-general
D14-1198	1-9	50-65	In this paper , we propose a new framework	This novel formulation allows different parts of the information network fully interact with each other .	1-49	50-65	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	This novel formulation allows different parts of the information network fully interact with each other .	1<2	elab-addition	elab-addition
D14-1198	50-65	66-81	This novel formulation allows different parts of the information network fully interact with each other .	For example , many relations can now be considered as the resultant states of events .	50-65	66-81	This novel formulation allows different parts of the information network fully interact with each other .	For example , many relations can now be considered as the resultant states of events .	1<2	elab-example	elab-example
D14-1198	1-9	82-91	In this paper , we propose a new framework	Our approach achieves substantial improvements over traditional pipelined approaches ,	1-49	82-100	In this paper , we propose a new framework that unifies the output of three information extraction ( IE ) tasks - entity mentions , relations and events as an information network representation , and extracts all of them using one single joint model based on structured prediction .	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	1<2	evaluation	evaluation
D14-1198	82-91	92-100	Our approach achieves substantial improvements over traditional pipelined approaches ,	and significantly advances state-of-the-art end-to-end event argument extraction .	82-100	82-100	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	Our approach achieves substantial improvements over traditional pipelined approaches , and significantly advances state-of-the-art end-to-end event argument extraction .	1<2	joint	joint
D14-1199	1-15	16-26	The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge	but the cost of developing such systems is considerably high .	1-26	1-26	The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high .	The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high .	1>2	contrast	contrast
D14-1199	16-26	27-37	but the cost of developing such systems is considerably high .	In this article , we consider the problem of event extraction	1-26	27-68	The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high .	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	1>2	bg-compare	bg-compare
D14-1199	38-39	40-68	and show	that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	27-68	27-68	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	1>2	attribution	attribution
D14-1199	27-37	40-68	In this article , we consider the problem of event extraction	that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	27-68	27-68	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	In this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set .	1<2	joint	joint
D14-1200	1-8	9-18	This paper proposes a history-based structured learning approach	that jointly extracts entities and relations in a sentence .	1-18	1-18	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	1<2	elab-addition	elab-addition
D14-1200	1-8	19-32	This paper proposes a history-based structured learning approach	We introduce a novel simple and flexible table representation of entities and relations .	1-18	19-32	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	We introduce a novel simple and flexible table representation of entities and relations .	1<2	elab-addition	elab-addition
D14-1200	1-8	33-51	This paper proposes a history-based structured learning approach	We investigate several feature settings , search orders , and learning methods with inexact search on the table .	1-18	33-51	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	We investigate several feature settings , search orders , and learning methods with inexact search on the table .	1<2	elab-addition	elab-addition
D14-1200	52-55	56-65	The experimental results demonstrate	that a joint learning approach significantly outperforms a pipeline approach	52-79	52-79	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	1>2	attribution	attribution
D14-1200	1-8	56-65	This paper proposes a history-based structured learning approach	that a joint learning approach significantly outperforms a pipeline approach	1-18	52-79	This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	1<2	evaluation	evaluation
D14-1200	56-65	66-69	that a joint learning approach significantly outperforms a pipeline approach	by incorporating global features	52-79	52-79	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	1<2	manner-means	manner-means
D14-1200	66-69	70-79	by incorporating global features	and by selecting appropriate learning methods and search orders .	52-79	52-79	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .	1<2	joint	joint
D14-1201	1-14	44-57	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques ,	However , few studies have been reported on ORE for languages beyond English .	1-24	44-57	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	However , few studies have been reported on ORE for languages beyond English .	1>2	contrast	contrast
D14-1201	1-14	15-24	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques ,	which train individual extractors for every single relation type .	1-24	1-24	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	1<2	elab-addition	elab-addition
D14-1201	1-14	25-43	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques ,	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	1-24	25-43	Open Relation Extraction ( ORE ) overcomes the limitations of traditional IE techniques , which train individual extractors for every single relation type .	Systems such as ReVerb , PATTY , OLLIE , and Exemplar have attracted much attention on English ORE .	1<2	elab-addition	elab-addition
D14-1201	44-57	58-71	However , few studies have been reported on ORE for languages beyond English .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE ,	44-57	58-81	However , few studies have been reported on ORE for languages beyond English .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	1>2	bg-compare	bg-compare
D14-1201	58-71	72-81	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE ,	for extracting relations and semantic patterns from Chinese text .	58-81	58-81	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	1<2	elab-addition	elab-addition
D14-1201	58-71	82-91	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE ,	ZORE identifies relation candidates from automatically parsed dependency trees ,	58-81	82-107	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	1<2	elab-addition	elab-addition
D14-1201	82-91	92-107	ZORE identifies relation candidates from automatically parsed dependency trees ,	and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	82-107	82-107	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	ZORE identifies relation candidates from automatically parsed dependency trees , and then extracts relations with their semantic patterns iteratively through a novel double propagation algorithm .	1<2	progression	progression
D14-1201	58-71	108-121	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE ,	Empirical results on two data sets show the effectiveness of the proposed system .	58-81	108-121	This paper presents a syntax-based Chinese ( Zh ) ORE system , ZORE , for extracting relations and semantic patterns from Chinese text .	Empirical results on two data sets show the effectiveness of the proposed system .	1<2	evaluation	evaluation
D14-1202	1-16	17-24	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	In this paper we propose a two-stage method	1-16	17-34	Correctly predicting abbreviations given the full forms is important in many natural language processing systems .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	1>2	bg-goal	bg-goal
D14-1202	17-24	25-29	In this paper we propose a two-stage method	to find the corresponding abbreviation	17-34	17-34	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	1<2	elab-addition	elab-addition
D14-1202	17-24	30-34	In this paper we propose a two-stage method	given its full form .	17-34	17-34	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	1<2	elab-addition	elab-addition
D14-1202	17-24	35-40	In this paper we propose a two-stage method	We first use the contextual information	17-34	35-62	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	1<2	elab-process_step	elab-process_step
D14-1202	35-40	41-44	We first use the contextual information	given a large corpus	35-62	35-62	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	1<2	elab-addition	elab-addition
D14-1202	35-40	45-52	We first use the contextual information	to get abbreviation candidates for each full form	35-62	35-62	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	1<2	enablement	enablement
D14-1202	35-40	53-62	We first use the contextual information	and get a coarse-grained ranking through graph random walk .	35-62	35-62	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	1<2	joint	joint
D14-1202	53-62	63-75	and get a coarse-grained ranking through graph random walk .	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	35-62	63-75	We first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk .	This coarse-grained rank list fixes the search space inside the top-ranked candidates .	1<2	elab-addition	elab-addition
D14-1202	17-24	76-83	In this paper we propose a two-stage method	Then we use a similarity sensitive re-ranking strategy	17-34	76-102	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	1<2	elab-process_step	elab-process_step
D14-1202	76-83	84-91	Then we use a similarity sensitive re-ranking strategy	which can utilize the features of the candidates	76-102	76-102	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	1<2	elab-addition	elab-addition
D14-1202	76-83	92-96	Then we use a similarity sensitive re-ranking strategy	to give a fine-grained re-ranking	76-102	76-102	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	1<2	enablement	enablement
D14-1202	76-83	97-102	Then we use a similarity sensitive re-ranking strategy	and select the final result .	76-102	76-102	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	Then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result .	1<2	joint	joint
D14-1202	17-24	103-107	In this paper we propose a two-stage method	Our method achieves good results	17-34	103-113	In this paper we propose a two-stage method to find the corresponding abbreviation given its full form .	Our method achieves good results and outperforms the state-of-the-art systems .	1<2	evaluation	evaluation
D14-1202	103-107	108-113	Our method achieves good results	and outperforms the state-of-the-art systems .	103-113	103-113	Our method achieves good results and outperforms the state-of-the-art systems .	Our method achieves good results and outperforms the state-of-the-art systems .	1<2	joint	joint
D14-1202	103-107	114-125	Our method achieves good results	One advantage of our method is that it only needs weak supervision	103-113	114-135	Our method achieves good results and outperforms the state-of-the-art systems .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	1<2	elab-addition	elab-addition
D14-1202	114-125	126-135	One advantage of our method is that it only needs weak supervision	and can get competitive results with fewer training data .	114-135	114-135	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	1<2	joint	joint
D14-1202	114-125	136-145	One advantage of our method is that it only needs weak supervision	The candidate generation and coarse-grained ranking is totally unsupervised .	114-135	136-145	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	The candidate generation and coarse-grained ranking is totally unsupervised .	1<2	elab-addition	elab-addition
D14-1202	126-135	146-157	and can get competitive results with fewer training data .	The re-ranking phase can use a very small amount of training data	114-135	146-164	One advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	1<2	elab-addition	elab-addition
D14-1202	146-157	158-164	The re-ranking phase can use a very small amount of training data	to get a reasonably good result .	146-164	146-164	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	The re-ranking phase can use a very small amount of training data to get a reasonably good result .	1<2	enablement	enablement
D14-1203	1-7	25-35	Distant supervision has become the leading method	However , there are still many questions about the best way	1-24	25-40	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	However , there are still many questions about the best way to learn such extractors .	1>2	contrast	contrast
D14-1203	1-7	8-13	Distant supervision has become the leading method	for training large-scale relation extractors ,	1-24	1-24	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	1<2	elab-addition	elab-addition
D14-1203	1-7	14-24	Distant supervision has become the leading method	with nearly universal adoption in recent TAC knowledge-base population competitions .	1-24	1-24	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	Distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent TAC knowledge-base population competitions .	1<2	elab-addition	elab-addition
D14-1203	25-35	41-49	However , there are still many questions about the best way	In this paper we investigate four orthogonal improvements :	25-40	41-83	However , there are still many questions about the best way to learn such extractors .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	1>2	bg-compare	bg-compare
D14-1203	25-35	36-40	However , there are still many questions about the best way	to learn such extractors .	25-40	25-40	However , there are still many questions about the best way to learn such extractors .	However , there are still many questions about the best way to learn such extractors .	1<2	elab-addition	elab-addition
D14-1203	41-49	50-67	In this paper we investigate four orthogonal improvements :	integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction ,	41-83	41-83	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	1<2	elab-enumember	elab-enumember
D14-1203	50-67	68-74	integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction ,	enforcing type constraints of linked arguments ,	41-83	41-83	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	1<2	joint	joint
D14-1203	68-74	75-83	enforcing type constraints of linked arguments ,	and partitioning the model by relation type signature .	41-83	41-83	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	1<2	joint	joint
D14-1203	41-49	84-92	In this paper we investigate four orthogonal improvements :	We evaluate sentential extraction performance on two datasets :	41-83	84-125	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	1<2	evaluation	evaluation
D14-1203	84-92	93-99	We evaluate sentential extraction performance on two datasets :	the popular set of NY Times articles	84-125	84-125	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	1<2	elab-enumember	elab-enumember
D14-1203	93-99	100-108	the popular set of NY Times articles	partially annotated by Hoffmann et al. ( 2011 )	84-125	84-125	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	1<2	elab-addition	elab-addition
D14-1203	93-99	109-113	the popular set of NY Times articles	and a new dataset ,	84-125	84-125	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	1<2	joint	joint
D14-1203	109-113	114-116	and a new dataset ,	called GORECO ,	84-125	84-125	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	1<2	elab-addition	elab-addition
D14-1203	109-113	117-125	and a new dataset ,	that is comprehensively annotated for 48 common relations .	84-125	84-125	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	We evaluate sentential extraction performance on two datasets : the popular set of NY Times articles partially annotated by Hoffmann et al. ( 2011 ) and a new dataset , called GORECO , that is comprehensively annotated for 48 common relations .	1<2	elab-addition	elab-addition
D14-1203	126-127	128-139	We find	that using NEL for argument identification boosts performance over the traditional approach	126-158	126-158	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	1>2	attribution	attribution
D14-1203	41-49	128-139	In this paper we investigate four orthogonal improvements :	that using NEL for argument identification boosts performance over the traditional approach	41-83	126-158	In this paper we investigate four orthogonal improvements : integrating named entity linking ( NEL ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	1<2	evaluation	evaluation
D14-1203	128-139	140-148	that using NEL for argument identification boosts performance over the traditional approach	( named entity recognition with string match ) ,	126-158	126-158	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	1<2	elab-addition	elab-addition
D14-1203	128-139	149-158	that using NEL for argument identification boosts performance over the traditional approach	and there is further improvement from using argument types .	126-158	126-158	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	1<2	joint	joint
D14-1203	128-139	159-172	that using NEL for argument identification boosts performance over the traditional approach	Our best system boosts precision by 44 % and recall by 70 % .	126-158	159-172	We find that using NEL for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types .	Our best system boosts precision by 44 % and recall by 70 % .	1<2	exp-evidence	exp-evidence
D14-1204	1-15	16-20	We address the problem of automatically inferring the tense of events in Chinese text .	We use a new corpus	1-15	16-38	We address the problem of automatically inferring the tense of events in Chinese text .	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	1>2	bg-goal	bg-goal
D14-1204	16-20	21-32	We use a new corpus	annotated with Chinese semantic tense information and other implicit Chinese linguistic information	16-38	16-38	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	1<2	elab-addition	elab-addition
D14-1204	16-20	33-38	We use a new corpus	using a "distant annotation" method .	16-38	16-38	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	1<2	manner-means	manner-means
D14-1204	16-20	39-48	We use a new corpus	We propose three improvements over a relatively strong baseline method	16-38	39-58	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	1<2	elab-addition	elab-addition
D14-1204	39-48	49-58	We propose three improvements over a relatively strong baseline method	- a statistical learning method with extensive feature engineering .	39-58	39-58	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	1<2	elab-definition	elab-definition
D14-1204	39-48	59-79	We propose three improvements over a relatively strong baseline method	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event ,	39-58	59-85	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	1<2	elab-aspect	elab-aspect
D14-1204	59-79	80-85	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event ,	which are also inferred automatically .	59-85	59-85	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	First , we add two sources of implicit linguistic information as features - eventuality type and modality of an event , which are also inferred automatically .	1<2	elab-addition	elab-addition
D14-1204	39-48	86-104	We propose three improvements over a relatively strong baseline method	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	39-58	86-104	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	Second , we perform joint learning on semantic tense , eventuality type , and modality of an event .	1<2	elab-aspect	elab-aspect
D14-1204	39-48	105-115	We propose three improvements over a relatively strong baseline method	Third , we train artificial neural network models for this problem	39-58	105-123	We propose three improvements over a relatively strong baseline method - a statistical learning method with extensive feature engineering .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	1<2	elab-aspect	elab-aspect
D14-1204	105-115	116-123	Third , we train artificial neural network models for this problem	and compare its performance with feature-based approaches .	105-123	105-123	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	Third , we train artificial neural network models for this problem and compare its performance with feature-based approaches .	1<2	joint	joint
D14-1204	16-20	124-133	We use a new corpus	Experimental results show considerable improvements on Chinese tense inference .	16-38	124-133	We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a "distant annotation" method .	Experimental results show considerable improvements on Chinese tense inference .	1<2	evaluation	evaluation
D14-1204	124-133	134-142	Experimental results show considerable improvements on Chinese tense inference .	Our best performance reaches 68.6 % in accuracy ,	124-133	134-148	Experimental results show considerable improvements on Chinese tense inference .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	1<2	exp-evidence	exp-evidence
D14-1204	134-142	143-148	Our best performance reaches 68.6 % in accuracy ,	outperforming a strong baseline method .	134-148	134-148	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	Our best performance reaches 68.6 % in accuracy , outperforming a strong baseline method .	1<2	elab-addition	elab-addition
D14-1205	1-30	31-38	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	However , the task often suffers from errors	1-30	31-48	Populating Knowledge Base ( KB ) with new knowledge facts from reliable text re-sources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs .	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	1>2	contrast	contrast
D14-1205	31-38	49-59	However , the task often suffers from errors	In this paper , we propose a novel joint inference framework	31-48	49-103	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	1>2	bg-compare	bg-compare
D14-1205	31-38	39-48	However , the task often suffers from errors	propagating from upstream entity linkers to downstream relation extractors .	31-48	31-48	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	However , the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors .	1<2	elab-addition	elab-addition
D14-1205	49-59	60-66	In this paper , we propose a novel joint inference framework	to allow interactions between the two subtasks	49-103	49-103	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	1<2	enablement	enablement
D14-1205	60-66	67-71	to allow interactions between the two subtasks	and find an optimal assignment	49-103	49-103	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	1<2	joint	joint
D14-1205	49-59	72-80	In this paper , we propose a novel joint inference framework	by addressing the coherence among preliminary local predictions :	49-103	49-103	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	1<2	manner-means	manner-means
D14-1205	72-80	81-94	by addressing the coherence among preliminary local predictions :	whether the types of entities meet the expectations of relations explicitly or implicitly ,	49-103	49-103	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	1<2	elab-example	elab-example
D14-1205	81-94	95-103	whether the types of entities meet the expectations of relations explicitly or implicitly ,	and whether the local predictions are globally compatible .	49-103	49-103	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	1<2	joint	joint
D14-1205	49-59	104-112	In this paper , we propose a novel joint inference framework	We further measure the confidence of the extracted triples	49-103	104-123	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	1<2	elab-addition	elab-addition
D14-1205	104-112	113-123	We further measure the confidence of the extracted triples	by looking at the details of the complete extraction process .	104-123	104-123	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	We further measure the confidence of the extracted triples by looking at the details of the complete extraction process .	1<2	manner-means	manner-means
D14-1205	124-125	126-135	Experiments show	that the proposed framework can significantly reduce the error propagations	124-151	124-151	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	1>2	attribution	attribution
D14-1205	49-59	126-135	In this paper , we propose a novel joint inference framework	that the proposed framework can significantly reduce the error propagations	49-103	124-151	In this paper , we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions : whether the types of entities meet the expectations of relations explicitly or implicitly , and whether the local predictions are globally compatible .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	1<2	evaluation	evaluation
D14-1205	126-135	136-141	that the proposed framework can significantly reduce the error propagations	thus obtain more reliable facts ,	124-151	124-151	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	1<2	progression	progression
D14-1205	136-141	142-151	thus obtain more reliable facts ,	and outperforms competitive baselines with state-of-the-art relation extraction models .	124-151	124-151	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts , and outperforms competitive baselines with state-of-the-art relation extraction models .	1<2	joint	joint
D14-1206	1-22	63-82	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	1-22	63-82	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	1>2	bg-compare	bg-compare
D14-1206	1-22	23-46	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	1-22	23-46	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	In particular , genres such as marketing flyers and info-graphics often augment textual information by its color , size , positioning , etc. .	1<2	elab-example	elab-example
D14-1206	1-22	47-62	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	1-22	47-62	Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features .	As a result , traditional text-based approaches to information extraction ( IE ) could underperform .	1<2	result	result
D14-1206	63-82	83-99	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities	63-82	83-108	In this study , we present a supervised machine learning approach to IE from online commercial real estate flyers .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	1<2	evaluation	evaluation
D14-1206	83-99	100-108	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities	using a combination of textual and visual features .	83-108	83-108	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	1<2	manner-means	manner-means
D14-1206	109-110	111-129	Results show	that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	109-129	109-129	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	1>2	attribution	attribution
D14-1206	83-99	111-129	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities	that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	83-108	109-129	We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features .	Results show that the addition of visual features such as color , size , and positioning significantly increased classifier performance .	1<2	elab-addition	elab-addition
D14-1207	1-15	57-70	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	1-15	57-70	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	1>2	contrast	contrast
D14-1207	1-15	16-22	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	These time scopes specify the time periods	1-15	16-32	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	These time scopes specify the time periods when a given fact was valid in real life .	1<2	elab-addition	elab-addition
D14-1207	16-22	23-32	These time scopes specify the time periods	when a given fact was valid in real life .	16-32	16-32	These time scopes specify the time periods when a given fact was valid in real life .	These time scopes specify the time periods when a given fact was valid in real life .	1<2	temporal	temporal
D14-1207	33-36	37-41	Without temporal scope ,	many facts are under-specified ,	33-56	33-56	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	1>2	condition	condition
D14-1207	1-15	37-41	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	many facts are under-specified ,	1-15	33-56	Temporal scope adds a time dimension to facts in Knowledge Bases ( KBs ) .	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	1<2	elab-addition	elab-addition
D14-1207	37-41	42-56	many facts are under-specified ,	reducing the usefulness of the data for upper level applications such as Question Answering .	33-56	33-56	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	Without temporal scope , many facts are under-specified , reducing the usefulness of the data for upper level applications such as Question Answering .	1<2	elab-addition	elab-addition
D14-1207	57-70	71-79	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	In this paper , we present a new method	57-70	71-95	Existing methods for temporal scope inference and extraction still suffer from low accuracy .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	1>2	bg-compare	bg-compare
D14-1207	71-79	80-95	In this paper , we present a new method	that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	71-95	71-95	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	1<2	elab-addition	elab-addition
D14-1207	96-103	104-109	Through change patterns in an entity's CTP ,	we model the entity's state change	96-132	96-132	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	1>2	manner-means	manner-means
D14-1207	71-79	104-109	In this paper , we present a new method	we model the entity's state change	71-95	96-132	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	1<2	elab-addition	elab-addition
D14-1207	104-109	110-115	we model the entity's state change	brought about by real world events	96-132	96-132	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	1<2	elab-addition	elab-addition
D14-1207	110-115	116-120	brought about by real world events	that happen to the entity	96-132	96-132	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	1<2	elab-addition	elab-addition
D14-1207	116-120	121-132	that happen to the entity	( e.g , hired , fired , divorced , etc. ) .	96-132	96-132	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	1<2	elab-example	elab-example
D14-1207	104-109	133-150	we model the entity's state change	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	96-132	133-150	Through change patterns in an entity's CTP , we model the entity's state change brought about by real world events that happen to the entity ( e.g , hired , fired , divorced , etc. ) .	This leads to a new formulation of the temporal scoping problem as a state change detection problem .	1<2	elab-addition	elab-addition
D14-1207	151-153	154-167	Our experiments show	that this formulation of the problem , and the resulting solution are highly effective	151-174	151-174	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	1>2	attribution	attribution
D14-1207	71-79	154-167	In this paper , we present a new method	that this formulation of the problem , and the resulting solution are highly effective	71-95	151-174	In this paper , we present a new method that leverages temporal profiles augmented with context Contextual Temporal Profiles ( CTPs ) of entities .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	1<2	evaluation	evaluation
D14-1207	154-167	168-174	that this formulation of the problem , and the resulting solution are highly effective	for inferring temporal scope of facts .	151-174	151-174	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	Our experiments show that this formulation of the problem , and the resulting solution are highly effective for inferring temporal scope of facts .	1<2	elab-addition	elab-addition
D14-1208	1-8,26-34	56-68	Distant supervision , a paradigm of relation extraction <*> is an attractive approach for training relation extractors .	In this paper , we discuss and critically analyse a popular alignment strategy	1-34	56-75	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	1>2	bg-compare	bg-compare
D14-1208	1-8,26-34	9-13	Distant supervision , a paradigm of relation extraction <*> is an attractive approach for training relation extractors .	where training data is created	1-34	1-34	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	1<2	elab-addition	elab-addition
D14-1208	9-13	14-25	where training data is created	by aligning facts in a database with a large unannotated corpus ,	1-34	1-34	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	1<2	manner-means	manner-means
D14-1208	1-8,26-34	35-41	Distant supervision , a paradigm of relation extraction <*> is an attractive approach for training relation extractors .	Various models are proposed in recent literature	1-34	35-55	Distant supervision , a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus , is an attractive approach for training relation extractors .	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	1<2	elab-addition	elab-addition
D14-1208	35-41	42-55	Various models are proposed in recent literature	to align the facts in the database to their mentions in the corpus .	35-55	35-55	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus .	1<2	enablement	enablement
D14-1208	56-68	69-75	In this paper , we discuss and critically analyse a popular alignment strategy	called the "at least one" heuristic .	56-75	56-75	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	1<2	elab-addition	elab-addition
D14-1208	56-68	76-87	In this paper , we discuss and critically analyse a popular alignment strategy	We provide a simple , yet effective relaxation to this strategy .	56-75	76-87	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	We provide a simple , yet effective relaxation to this strategy .	1<2	elab-addition	elab-addition
D14-1208	76-87	88-102	We provide a simple , yet effective relaxation to this strategy .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems	76-87	88-121	We provide a simple , yet effective relaxation to this strategy .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	1<2	elab-addition	elab-addition
D14-1208	88-102	103-113	We formulate the inference procedures in training as integer linear programming ( ILP ) problems	and implement the relaxation to the "at least one " heuristic	88-121	88-121	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	1<2	progression	progression
D14-1208	103-113	114-121	and implement the relaxation to the "at least one " heuristic	via a soft constraint in this formulation .	88-121	88-121	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	We formulate the inference procedures in training as integer linear programming ( ILP ) problems and implement the relaxation to the "at least one " heuristic via a soft constraint in this formulation .	1<2	manner-means	manner-means
D14-1208	122-125	126-142	Empirically , we demonstrate	that this simple strategy leads to a better performance under certain settings over the existing approaches .	122-142	122-142	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	1>2	attribution	attribution
D14-1208	56-68	126-142	In this paper , we discuss and critically analyse a popular alignment strategy	that this simple strategy leads to a better performance under certain settings over the existing approaches .	56-75	122-142	In this paper , we discuss and critically analyse a popular alignment strategy called the "at least one" heuristic .	Empirically , we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .	1<2	evaluation	evaluation
D14-1209	1-11	12-29	Parameter tuning is an important problem in statistical machine translation ,	but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search ,	1-38	1-38	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	1>2	contrast	contrast
D14-1209	12-29	39-43	but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search ,	We propose a search-aware framework	1-38	39-55	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	1>2	bg-compare	bg-compare
D14-1209	12-29	30-38	but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search ,	while search errors could severely degrade translation quality .	1-38	1-38	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	Parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as MERT , MIRA and PRO are agnostic about search , while search errors could severely degrade translation quality .	1<2	contrast	contrast
D14-1209	39-43	44-49	We propose a search-aware framework	to promote promising partial translations ,	39-55	39-55	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	1<2	enablement	enablement
D14-1209	39-43	50-55	We propose a search-aware framework	preventing them from be-ing pruned .	39-55	39-55	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	1<2	elab-addition	elab-addition
D14-1209	56-58	59-67	To do so	we develop two metrics to evaluate partial derivations .	56-67	56-67	To do so we develop two metrics to evaluate partial derivations .	To do so we develop two metrics to evaluate partial derivations .	1>2	enablement	enablement
D14-1209	44-49	59-67	to promote promising partial translations ,	we develop two metrics to evaluate partial derivations .	39-55	56-67	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	To do so we develop two metrics to evaluate partial derivations .	1<2	elab-addition	elab-addition
D14-1209	39-43	68-81	We propose a search-aware framework	Our technique can be applied to all of the three above-mentioned tuning methods ,	39-55	68-99	We propose a search-aware framework to promote promising partial translations , preventing them from be-ing pruned .	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	1<2	evaluation	evaluation
D14-1209	68-81	82-99	Our technique can be applied to all of the three above-mentioned tuning methods ,	and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	68-99	68-99	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	Our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 BLEU gains over search-agnostic baselines .	1<2	joint	joint
D14-1210	1-13	21-28	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique	In this paper , we extend these techniques	1-20	21-44	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	1>2	bg-goal	bg-goal
D14-1210	1-13	14-20	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique	for improving monolingual parsing with PCFGs .	1-20	1-20	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving monolingual parsing with PCFGs .	1<2	elab-addition	elab-addition
D14-1210	21-28	29-37	In this paper , we extend these techniques	to learn latent refinements of single-category synchronous grammars ,	21-44	21-44	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	1<2	enablement	enablement
D14-1210	29-37	38-44	to learn latent refinements of single-category synchronous grammars ,	so as to improve translation performance .	21-44	21-44	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	1<2	progression	progression
D14-1210	21-28	45-53	In this paper , we extend these techniques	We compare two estimators for this latent-variable model :	21-44	45-71	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	1<2	elab-addition	elab-addition
D14-1210	45-53	54-64	We compare two estimators for this latent-variable model :	one based on EM and the other is a spectral algorithm	45-71	45-71	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	1<2	elab-enumember	elab-enumember
D14-1210	54-64	65-71	one based on EM and the other is a spectral algorithm	based on the method of moments .	45-71	45-71	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	We compare two estimators for this latent-variable model : one based on EM and the other is a spectral algorithm based on the method of moments .	1<2	bg-general	bg-general
D14-1210	21-28	72-81	In this paper , we extend these techniques	We evaluate their performance on a Chinese-English translation task .	21-44	72-81	In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars , so as to improve translation performance .	We evaluate their performance on a Chinese-English translation task .	1<2	evaluation	evaluation
D14-1210	82-84	85-97	The results indicate	that we can achieve significant gains over the baseline with both approaches ,	82-112	82-112	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	1>2	attribution	attribution
D14-1210	85-97	98-106	that we can achieve significant gains over the baseline with both approaches ,	but in particular the moments-based estimator is both faster	82-112	82-112	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	1>2	contrast	contrast
D14-1210	72-81	98-106	We evaluate their performance on a Chinese-English translation task .	but in particular the moments-based estimator is both faster	72-81	82-112	We evaluate their performance on a Chinese-English translation task .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	1<2	elab-addition	elab-addition
D14-1210	98-106	107-112	but in particular the moments-based estimator is both faster	and performs better than EM .	82-112	82-112	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM .	1<2	joint	joint
D14-1211	1-18	19-41	We investigate the interaction of power , gender , and language use in the Enron email corpus .	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	1-18	19-41	We investigate the interaction of power , gender , and language use in the Enron email corpus .	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	1>2	bg-goal	bg-goal
D14-1211	42-45	46-50	Using this data ,	we test two specific hypotheses	42-83	42-83	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1>2	manner-means	manner-means
D14-1211	19-41	46-50	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	we test two specific hypotheses	19-41	42-83	We present a freely available extension to the Enron corpus , with the gender of senders of 87 % messages reliably identified .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1<2	elab-addition	elab-addition
D14-1211	46-50	51-55	we test two specific hypotheses	drawn from the sociolinguistic literature	42-83	42-83	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1<2	elab-addition	elab-addition
D14-1211	51-55	56-61	drawn from the sociolinguistic literature	pertaining to gender and power :	42-83	42-83	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1<2	elab-addition	elab-addition
D14-1211	46-50	62-68	we test two specific hypotheses	women managers use face-saving communicative strategies ,	42-83	42-83	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1<2	elab-enumember	elab-enumember
D14-1211	62-68	69-76	women managers use face-saving communicative strategies ,	and women use language more explicitly than men	42-83	42-83	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1<2	joint	joint
D14-1211	69-76	77-83	and women use language more explicitly than men	to create and maintain social relations .	42-83	42-83	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	1<2	enablement	enablement
D14-1211	46-50	84-98	we test two specific hypotheses	We introduce the notion of "gender environment" to the computational study of written conversations ;	42-83	84-124	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	1<2	elab-process_step	elab-process_step
D14-1211	46-50	99-111	we test two specific hypotheses	we interpret this notion as the gender makeup of an email thread ,	42-83	84-124	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	1<2	elab-process_step	elab-process_step
D14-1211	112-113	114-124	and show	that some manifestations of power differ significantly between gender environments .	84-124	84-124	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	1>2	attribution	attribution
D14-1211	99-111	114-124	we interpret this notion as the gender makeup of an email thread ,	that some manifestations of power differ significantly between gender environments .	84-124	84-124	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	We introduce the notion of "gender environment" to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments .	1<2	joint	joint
D14-1211	46-50	125-151	we test two specific hypotheses	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	42-83	125-151	Using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations .	Finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .	1<2	elab-process_step	elab-process_step
D14-1212	1-10	78-87	Latent Dirichlet allocation ( LDA ) is a topic model	However , Twitter-LDA is not capable of online inference .	1-27	78-87	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	However , Twitter-LDA is not capable of online inference .	1>2	contrast	contrast
D14-1212	1-10	11-18	Latent Dirichlet allocation ( LDA ) is a topic model	that has been applied to various fields ,	1-27	1-27	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	1<2	elab-addition	elab-addition
D14-1212	11-18	19-27	that has been applied to various fields ,	including user profiling and event summarization on Twitter .	1-27	1-27	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	1<2	elab-example	elab-example
D14-1212	28-35	36-49	When LDA is applied to tweet collections ,	it generally treats all aggregated tweets of a user as a single document .	28-49	28-49	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	1>2	condition	condition
D14-1212	1-10	36-49	Latent Dirichlet allocation ( LDA ) is a topic model	it generally treats all aggregated tweets of a user as a single document .	1-27	28-49	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	When LDA is applied to tweet collections , it generally treats all aggregated tweets of a user as a single document .	1<2	elab-addition	elab-addition
D14-1212	1-10	50-51,63-65	Latent Dirichlet allocation ( LDA ) is a topic model	Twitter-LDA , <*> has been proposed	1-27	50-77	Latent Dirichlet allocation ( LDA ) is a topic model that has been applied to various fields , including user profiling and event summarization on Twitter .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	1<2	evaluation	evaluation
D14-1212	52-53	54-62	which assumes	a single tweet consists of a single topic ,	50-77	50-77	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	1>2	attribution	attribution
D14-1212	50-51,63-65	54-62	Twitter-LDA , <*> has been proposed	a single tweet consists of a single topic ,	50-77	50-77	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	1<2	elab-addition	elab-addition
D14-1212	66-68	69-77	and has shown	that it is superior in topic semantic coherence .	50-77	50-77	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	1>2	attribution	attribution
D14-1212	63-65	69-77	has been proposed	that it is superior in topic semantic coherence .	50-77	50-77	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	Twitter-LDA , which assumes a single tweet consists of a single topic , has been proposed and has shown that it is superior in topic semantic coherence .	1<2	joint	joint
D14-1212	78-87	88-100	However , Twitter-LDA is not capable of online inference .	In this study , we extend Twitter-LDA in the following two ways .	78-87	88-100	However , Twitter-LDA is not capable of online inference .	In this study , we extend Twitter-LDA in the following two ways .	1>2	bg-compare	bg-compare
D14-1212	88-100	101-111	In this study , we extend Twitter-LDA in the following two ways .	First , we model the generation process of tweets more accurately	88-100	101-125	In this study , we extend Twitter-LDA in the following two ways .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	1<2	elab-aspect	elab-aspect
D14-1212	101-111	112-125	First , we model the generation process of tweets more accurately	by estimating the ratio between topic words and general words for each user .	101-125	101-125	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	First , we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user .	1<2	manner-means	manner-means
D14-1212	88-100	126-141	In this study , we extend Twitter-LDA in the following two ways .	Second , we enable it to estimate the dynamics of user interests and topic trends online	88-100	126-157	In this study , we extend Twitter-LDA in the following two ways .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	1<2	elab-aspect	elab-aspect
D14-1212	126-141	142-151	Second , we enable it to estimate the dynamics of user interests and topic trends online	based on the topic tracking model ( TTM ) ,	126-157	126-157	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	1<2	bg-general	bg-general
D14-1212	142-151	152-157	based on the topic tracking model ( TTM ) ,	which models consumer purchase behaviors .	126-157	126-157	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	Second , we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model ( TTM ) , which models consumer purchase behaviors .	1<2	elab-addition	elab-addition
D14-1213	1-15	47-80	Self-disclosure , the act of revealing one-self to others , is an important social behavior	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	1-24	47-80	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	1>2	bg-goal	bg-goal
D14-1213	1-15	16-19	Self-disclosure , the act of revealing one-self to others , is an important social behavior	that strengthens interpersonal relationships	1-24	1-24	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	1<2	elab-addition	elab-addition
D14-1213	16-19	20-24	that strengthens interpersonal relationships	and increases social support .	1-24	1-24	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	1<2	joint	joint
D14-1213	25-34	35-46	Although there are many social science studies of self-disclosure ,	they are based on manual coding of small datasets and questionnaires .	25-46	25-46	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	1>2	contrast	contrast
D14-1213	1-15	35-46	Self-disclosure , the act of revealing one-self to others , is an important social behavior	they are based on manual coding of small datasets and questionnaires .	1-24	25-46	Self-disclosure , the act of revealing one-self to others , is an important social behavior that strengthens interpersonal relationships and increases social support .	Although there are many social science studies of self-disclosure , they are based on manual coding of small datasets and questionnaires .	1<2	elab-addition	elab-addition
D14-1213	47-80	81-90	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We use a longitudinal dataset of 17 million tweets ,	47-80	81-120	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	1<2	elab-addition	elab-addition
D14-1213	81-90	91-96	We use a longitudinal dataset of 17 million tweets ,	all of which occurred in conversations	81-120	81-120	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	1<2	elab-addition	elab-addition
D14-1213	91-96	97-103	all of which occurred in conversations	that consist of five or more tweets	81-120	81-120	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	1<2	elab-addition	elab-addition
D14-1213	97-103	104-110	that consist of five or more tweets	directly replying to the previous tweet ,	81-120	81-120	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	1<2	elab-addition	elab-addition
D14-1213	97-103	111-120	that consist of five or more tweets	and from dyads with twenty of more conversations each .	81-120	81-120	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	We use a longitudinal dataset of 17 million tweets , all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet , and from dyads with twenty of more conversations each .	1<2	joint	joint
D14-1213	47-80	121-138	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA )	47-80	121-149	We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations , a semi-supervised machine learning algorithm , and a computational analysis of the effects of self-disclosure on subsequent conversations .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	1<2	elab-addition	elab-addition
D14-1213	121-138	139-149	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA )	for automatically classifying the level of self-disclosure for each tweet .	121-149	121-149	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	1<2	elab-addition	elab-addition
D14-1213	121-138	150-155	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA )	We take the results of SDTM	121-149	150-165	We develop self-disclosure topic model ( SDTM ) , a variant of latent Dirichlet allocation ( LDA ) for automatically classifying the level of self-disclosure for each tweet .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	1<2	evaluation	evaluation
D14-1213	150-155	156-165	We take the results of SDTM	and analyze the effects of self-disclosure on subsequent conversations .	150-165	150-165	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	1<2	joint	joint
D14-1213	150-155	166-179	We take the results of SDTM	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure ,	150-165	166-201	We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	1<2	elab-addition	elab-addition
D14-1213	166-179	180-186,189-201	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure ,	and the analysis of the longitudinal data <*> uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	166-201	166-201	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	1<2	joint	joint
D14-1213	180-186,189-201	187-188	and the analysis of the longitudinal data <*> uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	using SDTM	166-201	166-201	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	Our model significantly outperforms several comparable methods on classifying the level of self-disclosure , and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between self-disclosure and conversation frequency and length .	1<2	manner-means	manner-means
D14-1214	1-8	60-71	Social media websites provide a platform for anyone	This paper demonstrates the feasibility of accurately extracting major life events .	1-20	60-71	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	This paper demonstrates the feasibility of accurately extracting major life events .	1>2	bg-compare	bg-compare
D14-1214	1-8	9-12	Social media websites provide a platform for anyone	to describe significant events	1-20	1-20	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	1<2	elab-addition	elab-addition
D14-1214	9-12	13-20	to describe significant events	taking place in their lives in realtime .	1-20	1-20	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	1<2	elab-addition	elab-addition
D14-1214	1-8	21-37	Social media websites provide a platform for anyone	Currently , the majority of personal news and life events are published in a textual format ,	1-20	21-59	Social media websites provide a platform for anyone to describe significant events taking place in their lives in realtime .	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	1<2	elab-addition	elab-addition
D14-1214	21-37	38-41	Currently , the majority of personal news and life events are published in a textual format ,	motivating information extraction systems	21-59	21-59	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	1<2	elab-addition	elab-addition
D14-1214	38-41	42-51	motivating information extraction systems	that can provide a structured representations of major life events	21-59	21-59	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	1<2	elab-addition	elab-addition
D14-1214	42-51	52-59	that can provide a structured representations of major life events	( weddings , graduation , etc. ) .	21-59	21-59	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	Currently , the majority of personal news and life events are published in a textual format , motivating information extraction systems that can provide a structured representations of major life events ( weddings , graduation , etc. ) .	1<2	elab-example	elab-example
D14-1214	60-71	72-81	This paper demonstrates the feasibility of accurately extracting major life events .	Our system extracts a fine-grained description of users' life events	60-71	72-87	This paper demonstrates the feasibility of accurately extracting major life events .	Our system extracts a fine-grained description of users' life events based on their published tweets .	1<2	elab-addition	elab-addition
D14-1214	72-81	82-87	Our system extracts a fine-grained description of users' life events	based on their published tweets .	72-87	72-87	Our system extracts a fine-grained description of users' life events based on their published tweets .	Our system extracts a fine-grained description of users' life events based on their published tweets .	1<2	bg-general	bg-general
D14-1214	88-90	91-103	We are optimistic	that our system can help Twitter users more easily grasp information from users	88-121	88-121	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	1>2	attribution	attribution
D14-1214	60-71	91-103	This paper demonstrates the feasibility of accurately extracting major life events .	that our system can help Twitter users more easily grasp information from users	60-71	88-121	This paper demonstrates the feasibility of accurately extracting major life events .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	1<2	evaluation	evaluation
D14-1214	91-103	104-108	that our system can help Twitter users more easily grasp information from users	they take interest in following	88-121	88-121	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	1<2	elab-addition	elab-addition
D14-1214	91-103	109-121	that our system can help Twitter users more easily grasp information from users	and also facilitate many downstream applications , for example realtime friend recommendation .	88-121	88-121	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	We are optimistic that our system can help Twitter users more easily grasp information from users they take interest in following and also facilitate many downstream applications , for example realtime friend recommendation .	1<2	joint	joint
D14-1215	1-5	42-56	Comparisons are common linguistic devices	In this paper we propose a computational study of figurative comparisons , or similes .	1-14	42-56	Comparisons are common linguistic devices used to indicate the likeness of two things .	In this paper we propose a computational study of figurative comparisons , or similes .	1>2	bg-goal	bg-goal
D14-1215	1-5	6-14	Comparisons are common linguistic devices	used to indicate the likeness of two things .	1-14	1-14	Comparisons are common linguistic devices used to indicate the likeness of two things .	Comparisons are common linguistic devices used to indicate the likeness of two things .	1<2	elab-addition	elab-addition
D14-1215	6-14	15-25	used to indicate the likeness of two things .	Often , this likeness is not meant in the literal sense	1-14	15-41	Comparisons are common linguistic devices used to indicate the likeness of two things .	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	1<2	elab-addition	elab-addition
D14-1215	26-36	37-41	for example , I slept like a log does not imply	that logs actually sleep .	15-41	15-41	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	1>2	attribution	attribution
D14-1215	15-25	37-41	Often , this likeness is not meant in the literal sense	that logs actually sleep .	15-41	15-41	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	Often , this likeness is not meant in the literal sense for example , I slept like a log does not imply that logs actually sleep .	1<2	elab-example	elab-example
D14-1215	42-56	57-66	In this paper we propose a computational study of figurative comparisons , or similes .	Our starting point is a new large dataset of comparisons	42-56	57-75	In this paper we propose a computational study of figurative comparisons , or similes .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	1<2	elab-process_step	elab-process_step
D14-1215	57-66	67-70	Our starting point is a new large dataset of comparisons	extracted from product reviews	57-75	57-75	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	1<2	elab-addition	elab-addition
D14-1215	67-70	71-75	extracted from product reviews	and annotated for figurativeness .	57-75	57-75	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness .	1<2	joint	joint
D14-1215	42-56	76-79	In this paper we propose a computational study of figurative comparisons , or similes .	We use this dataset	42-56	76-96	In this paper we propose a computational study of figurative comparisons , or similes .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	1<2	elab-process_step	elab-process_step
D14-1215	76-79	80-87	We use this dataset	to characterize figurative language in naturally occurring comparisons	76-96	76-96	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	1<2	enablement	enablement
D14-1215	80-87	88-96	to characterize figurative language in naturally occurring comparisons	and reveal linguistic patterns indicative of this phenomenon .	76-96	76-96	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon .	1<2	joint	joint
D14-1215	42-56	97-100	In this paper we propose a computational study of figurative comparisons , or similes .	We operationalize these insights	42-56	97-121	In this paper we propose a computational study of figurative comparisons , or similes .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	1<2	elab-process_step	elab-process_step
D14-1215	97-100	101-114	We operationalize these insights	and apply them to a new task with high relevance to text understanding :	97-121	97-121	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	1<2	joint	joint
D14-1215	101-114	115-121	and apply them to a new task with high relevance to text understanding :	distinguishing between figurative and literal comparisons .	97-121	97-121	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	We operationalize these insights and apply them to a new task with high relevance to text understanding : distinguishing between figurative and literal comparisons .	1<2	elab-definition	elab-definition
D14-1215	42-56	122-127	In this paper we propose a computational study of figurative comparisons , or similes .	Finally , we apply this framework	42-56	122-162	In this paper we propose a computational study of figurative comparisons , or similes .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	elab-process_step	elab-process_step
D14-1215	122-127	128-132	Finally , we apply this framework	to explore the social context	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	enablement	enablement
D14-1215	128-132	133-139	to explore the social context	in which figurative language is produced ,	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	elab-addition	elab-addition
D14-1215	140	141-148	showing	that similes are more likely to accompany opinions	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1>2	attribution	attribution
D14-1215	133-139	141-148	in which figurative language is produced ,	that similes are more likely to accompany opinions	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	elab-addition	elab-addition
D14-1215	141-148	149-152	that similes are more likely to accompany opinions	showing extreme sentiment ,	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	joint	joint
D14-1215	149-152	153-159	showing extreme sentiment ,	and that they are uncommon in reviews	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	joint	joint
D14-1215	153-159	160-162	and that they are uncommon in reviews	deemed helpful .	122-162	122-162	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	Finally , we apply this framework to explore the social context in which figurative language is produced , showing that similes are more likely to accompany opinions showing extreme sentiment , and that they are uncommon in reviews deemed helpful .	1<2	elab-addition	elab-addition
D14-1216	1-13	14-30,41-52	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our starting point is that words in a given text segment , such as a paragraph , <*> are less likely to be a part of an idiomatic expression .	1-13	14-52	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	1<2	elab-aspect	elab-aspect
D14-1216	14-30,41-52	31-40	Our starting point is that words in a given text segment , such as a paragraph , <*> are less likely to be a part of an idiomatic expression .	that are high-ranking representatives of a common topic of discussion	14-52	14-52	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	1<2	elab-addition	elab-addition
D14-1216	14-30,41-52	53-68	Our starting point is that words in a given text segment , such as a paragraph , <*> are less likely to be a part of an idiomatic expression .	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective	14-52	53-87	Our starting point is that words in a given text segment , such as a paragraph , that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression .	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	1<2	elab-addition	elab-addition
D14-1216	53-68	69-82	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective	and therefore , we incorporate a simple analysis of the intensity of the emotions	53-87	53-87	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	1<2	progression	progression
D14-1216	69-82	83-87	and therefore , we incorporate a simple analysis of the intensity of the emotions	expressed by the contexts .	53-87	53-87	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	Our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts .	1<2	elab-addition	elab-addition
D14-1216	1-13	88-100	We describe an algorithm for automatic classification of idiomatic and literal expressions .	We investigate the bag of words topic representation of one to three paragraphs	1-13	88-117	We describe an algorithm for automatic classification of idiomatic and literal expressions .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	1<2	elab-aspect	elab-aspect
D14-1216	88-100	101-103	We investigate the bag of words topic representation of one to three paragraphs	containing an expression	88-117	88-117	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	1<2	elab-addition	elab-addition
D14-1216	101-103	104-111	containing an expression	that should be classified as idiomatic or literal	88-117	88-117	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	1<2	elab-addition	elab-addition
D14-1216	101-103	112-117	containing an expression	( a target phrase ) .	88-117	88-117	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	1<2	elab-addition	elab-addition
D14-1216	88-100	118-122	We investigate the bag of words topic representation of one to three paragraphs	We extract topics from paragraphs	88-117	118-149	We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	1<2	elab-addition	elab-addition
D14-1216	118-122	123-127	We extract topics from paragraphs	containing idioms and from paragraphs	118-149	118-149	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	1<2	elab-addition	elab-addition
D14-1216	123-127	128-129	containing idioms and from paragraphs	containing literals	118-149	118-149	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	1<2	elab-addition	elab-addition
D14-1216	118-122	130-149	We extract topics from paragraphs	using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	118-149	118-149	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .	1<2	manner-means	manner-means
D14-1216	150-158	161-169	Since idiomatic expressions exhibit the property of non-compositionality ,	that they usually present different semantics than the words	150-175	150-175	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	1>2	cause	cause
D14-1216	159-160	161-169	we assume	that they usually present different semantics than the words	150-175	150-175	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	1>2	attribution	attribution
D14-1216	1-13	161-169	We describe an algorithm for automatic classification of idiomatic and literal expressions .	that they usually present different semantics than the words	1-13	150-175	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	1<2	elab-aspect	elab-aspect
D14-1216	161-169	170-175	that they usually present different semantics than the words	used in the local topic .	150-175	150-175	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	1<2	elab-addition	elab-addition
D14-1216	161-169	176-193	that they usually present different semantics than the words	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	150-175	176-193	Since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic .	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	1<2	elab-addition	elab-addition
D14-1216	176-193	194-205	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	Thus , this topic representation allows us to differentiate idioms from literals	176-193	194-210	We treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	1<2	result	result
D14-1216	194-205	206-210	Thus , this topic representation allows us to differentiate idioms from literals	using local semantic contexts .	194-210	194-210	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	Thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts .	1<2	elab-addition	elab-addition
D14-1216	1-13	211-215	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our results are encouraging .	1-13	211-215	We describe an algorithm for automatic classification of idiomatic and literal expressions .	Our results are encouraging .	1<2	evaluation	evaluation
D14-1217	1-21	34-45	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We present a representation for common sense spatial knowledge and an approach	1-21	34-53	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	1>2	bg-goal	bg-goal
D14-1217	1-21	22-33	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We apply our approach to the task of text-to-3D scene generation .	1-21	22-33	We address the grounding of natural language to concrete spatial constraints , and inference of implicit pragmatics in 3D environments .	We apply our approach to the task of text-to-3D scene generation .	1<2	elab-addition	elab-addition
D14-1217	34-45	46-53	We present a representation for common sense spatial knowledge and an approach	to extract it from 3D scene data .	34-53	34-53	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	1<2	elab-addition	elab-addition
D14-1217	34-45	54-66	We present a representation for common sense spatial knowledge and an approach	In text-to-3D scene generation , a user provides as input natural language text	34-53	54-82	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	1<2	elab-addition	elab-addition
D14-1217	54-66	67-75	In text-to-3D scene generation , a user provides as input natural language text	from which we extract explicit constraints on the objects	54-82	54-82	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	1<2	elab-addition	elab-addition
D14-1217	67-75	76-82	from which we extract explicit constraints on the objects	that should appear in the scene .	54-82	54-82	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	In text-to-3D scene generation , a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene .	1<2	elab-addition	elab-addition
D14-1217	83-91	92-101	The main innovation of this work is to show	how to augment these explicit constraints with learned spatial knowledge	83-115	83-115	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	1>2	attribution	attribution
D14-1217	34-45	92-101	We present a representation for common sense spatial knowledge and an approach	how to augment these explicit constraints with learned spatial knowledge	34-53	83-115	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	1<2	elab-addition	elab-addition
D14-1217	92-101	102-115	how to augment these explicit constraints with learned spatial knowledge	to infer missing objects and likely layouts for the objects in the scene .	83-115	83-115	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene .	1<2	enablement	enablement
D14-1217	116-117	118-126	We demonstrate	that spatial knowledge is useful for interpreting natural language	116-137	116-137	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	1>2	attribution	attribution
D14-1217	34-45	118-126	We present a representation for common sense spatial knowledge and an approach	that spatial knowledge is useful for interpreting natural language	34-53	116-137	We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	1<2	evaluation	evaluation
D14-1217	116-117	127-137	We demonstrate	and show examples of learned knowledge and generated 3D scenes .	116-137	116-137	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes .	1<2	joint	joint
D14-1218	1-14	15-28	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	To solve the challenge of ordering a set of sentences into coherent order ,	1-14	15-48	Coherence is what makes a multi-sentence text meaningful , both logically and syntactically .	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	1>2	elab-definition	elab-definition
D14-1218	15-28	29-38	To solve the challenge of ordering a set of sentences into coherent order ,	existing approaches focus mostly on defining and using sophisticated features	15-48	15-48	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	1>2	enablement	enablement
D14-1218	29-38	49-55,64-66	existing approaches focus mostly on defining and using sophisticated features	But both argumentation semantics and cross-sentence syntax <*> are very hard	15-48	49-69	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	1>2	comparison	comparison
D14-1218	29-38	39-48	existing approaches focus mostly on defining and using sophisticated features	to capture the cross-sentence argumentation logic and syntactic relationships .	15-48	15-48	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	To solve the challenge of ordering a set of sentences into coherent order , existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships .	1<2	enablement	enablement
D14-1218	49-55,64-66	70-83	But both argumentation semantics and cross-sentence syntax <*> are very hard	In this paper , we introduce a neural network model for the coherence task	49-69	70-89	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	1>2	bg-compare	bg-compare
D14-1218	49-55,64-66	56-63	But both argumentation semantics and cross-sentence syntax <*> are very hard	( such as coreference and tense rules )	49-69	49-69	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	1<2	elab-example	elab-example
D14-1218	64-66	67-69	are very hard	to formalize .	49-69	49-69	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	But both argumentation semantics and cross-sentence syntax ( such as coreference and tense rules ) are very hard to formalize .	1<2	elab-addition	elab-addition
D14-1218	70-83	84-89	In this paper , we introduce a neural network model for the coherence task	based on distributed sentence representation .	70-89	70-89	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	1<2	bg-general	bg-general
D14-1218	70-83	90-100	In this paper , we introduce a neural network model for the coherence task	The proposed approach learns a syntactico-semantic representation for sentences automatically ,	70-89	90-108	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	1<2	elab-addition	elab-addition
D14-1218	90-100	101-108	The proposed approach learns a syntactico-semantic representation for sentences automatically ,	using either recurrent or recursive neural networks .	90-108	90-108	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	The proposed approach learns a syntactico-semantic representation for sentences automatically , using either recurrent or recursive neural networks .	1<2	manner-means	manner-means
D14-1218	70-83	109-117	In this paper , we introduce a neural network model for the coherence task	The architecture obviated the need for feature engineering ,	70-89	109-137	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	1<2	elab-addition	elab-addition
D14-1218	109-117	118-122	The architecture obviated the need for feature engineering ,	and learns sentence representations ,	109-137	109-137	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	1<2	joint	joint
D14-1218	118-122	123-132	and learns sentence representations ,	which are to some extent able to capture the "rules"	109-137	109-137	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	1<2	elab-addition	elab-addition
D14-1218	123-132	133-137	which are to some extent able to capture the "rules"	governing coherent sentence structure .	109-137	109-137	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	The architecture obviated the need for feature engineering , and learns sentence representations , which are to some extent able to capture the "rules" governing coherent sentence structure .	1<2	elab-addition	elab-addition
D14-1218	70-83	138-143	In this paper , we introduce a neural network model for the coherence task	The proposed approach outperforms existing baselines	70-89	138-154	In this paper , we introduce a neural network model for the coherence task based on distributed sentence representation .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	1<2	evaluation	evaluation
D14-1218	138-143	144-154	The proposed approach outperforms existing baselines	and generates the state-of-art performance in standard coherence evaluation tasks .	138-154	138-154	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	The proposed approach outperforms existing baselines and generates the state-of-art performance in standard coherence evaluation tasks .	1<2	joint	joint
D14-1219	1-9	10-13	In this paper , we present a discriminative approach	for reranking discourse trees	1-21	1-21	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	1<2	elab-addition	elab-addition
D14-1219	10-13	14-21	for reranking discourse trees	generated by an existing probabilistic discourse parser .	1-21	1-21	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	1<2	elab-addition	elab-addition
D14-1219	1-9	22-30	In this paper , we present a discriminative approach	The reranker relies on tree kernels ( TKs )	1-21	22-42	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	1<2	elab-addition	elab-addition
D14-1219	22-30	31-42	The reranker relies on tree kernels ( TKs )	to capture the global dependencies between discourse units in a tree .	22-42	22-42	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	1<2	enablement	enablement
D14-1219	22-30	43-54	The reranker relies on tree kernels ( TKs )	In particular , we design new computational structures of discourse trees ,	22-42	43-65	The reranker relies on tree kernels ( TKs ) to capture the global dependencies between discourse units in a tree .	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	1<2	elab-addition	elab-addition
D14-1219	43-54	55-60	In particular , we design new computational structures of discourse trees ,	which combined with standard TKs ,	43-65	43-65	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	1<2	elab-addition	elab-addition
D14-1219	55-60	61-65	which combined with standard TKs ,	originate novel discourse TKs .	43-65	43-65	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	In particular , we design new computational structures of discourse trees , which combined with standard TKs , originate novel discourse TKs .	1<2	joint	joint
D14-1219	66-69	70-94	The empirical evaluation shows	that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % ,	66-109	66-109	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	1>2	attribution	attribution
D14-1219	1-9	70-94	In this paper , we present a discriminative approach	that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % ,	1-21	66-109	In this paper , we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser .	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	1<2	evaluation	evaluation
D14-1219	70-94	95-109	that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % ,	which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	66-109	66-109	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77 % to 82.15 % , a relative error reduction of 11.8 % , which in turn pushes the state-of-the-art document-level accuracy from 55.8 % to 57.3 % .	1<2	elab-addition	elab-addition
D14-1220	1-7	29-40	Text-level discourse parsing remains a challenge :	In this paper , we propose a recursive model for discourse parsing	1-28	29-54	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	1>2	bg-compare	bg-compare
D14-1220	1-7	8-11	Text-level discourse parsing remains a challenge :	most approaches employ features	1-28	1-28	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	1<2	elab-addition	elab-addition
D14-1220	8-11	12-23	most approaches employ features	that fail to capture the intentional , semantic , and syntactic aspects	1-28	1-28	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	1<2	elab-addition	elab-addition
D14-1220	12-23	24-28	that fail to capture the intentional , semantic , and syntactic aspects	that govern discourse coherence .	1-28	1-28	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	Text-level discourse parsing remains a challenge : most approaches employ features that fail to capture the intentional , semantic , and syntactic aspects that govern discourse coherence .	1<2	elab-addition	elab-addition
D14-1220	29-40	41-54	In this paper , we propose a recursive model for discourse parsing	that jointly models distributed representations for clauses , sentences , and entire discourses .	29-54	29-54	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	1<2	elab-addition	elab-addition
D14-1220	29-40	55-76	In this paper , we propose a recursive model for discourse parsing	The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically , .	29-54	55-76	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically , .	1<2	elab-addition	elab-addition
D14-1220	29-40	77-82	In this paper , we propose a recursive model for discourse parsing	The proposed framework obtains comparable performance	29-54	77-94	In this paper , we propose a recursive model for discourse parsing that jointly models distributed representations for clauses , sentences , and entire discourses .	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	1<2	evaluation	evaluation
D14-1220	77-82	83-87	The proposed framework obtains comparable performance	regarding standard discours-ing parsing evaluations	77-94	77-94	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	1<2	elab-addition	elab-addition
D14-1220	83-87	88-94	regarding standard discours-ing parsing evaluations	when compared against current state-of-art systems .	77-94	77-94	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	The proposed framework obtains comparable performance regarding standard discours-ing parsing evaluations when compared against current state-of-art systems .	1<2	condition	condition
D14-1221	1-10	11-27	We present a novel method for coreference resolution error analysis	which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	1-27	1-27	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	1<2	elab-addition	elab-addition
D14-1221	1-10	28-34	We present a novel method for coreference resolution error analysis	Our analysis highlights differences between the systems	1-27	28-52	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	1<2	elab-addition	elab-addition
D14-1221	35-36	37-52	and identifies	that the majority of recall errors for nouns and names are shared by all systems .	28-52	28-52	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	1>2	attribution	attribution
D14-1221	28-34	37-52	Our analysis highlights differences between the systems	that the majority of recall errors for nouns and names are shared by all systems .	28-52	28-52	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems .	1<2	joint	joint
D14-1221	1-10	53-72	We present a novel method for coreference resolution error analysis	We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties .	1-27	53-72	We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems .	We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties .	1<2	evaluation	evaluation
D14-1222	1-14	15-29	Bridging resolution plays an important role in establishing ( local ) entity coherence .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution ,	1-14	15-53	Bridging resolution plays an important role in establishing ( local ) entity coherence .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	1>2	bg-goal	bg-goal
D14-1222	15-29	30-43	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution ,	where bridging anaphors are not limited to definite NPs and semantic relations between anaphors	15-53	15-53	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	1<2	elab-addition	elab-addition
D14-1222	30-43	44-53	where bridging anaphors are not limited to definite NPs and semantic relations between anaphors	and their antecedents are not restricted to meronymic relations .	15-53	15-53	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	1<2	joint	joint
D14-1222	15-29	54-59	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution ,	The system consists of eight rules	15-53	54-68	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	The system consists of eight rules which target different relations based on linguistic insights .	1<2	elab-addition	elab-addition
D14-1222	54-59	60-63	The system consists of eight rules	which target different relations	54-68	54-68	The system consists of eight rules which target different relations based on linguistic insights .	The system consists of eight rules which target different relations based on linguistic insights .	1<2	elab-addition	elab-addition
D14-1222	60-63	64-68	which target different relations	based on linguistic insights .	54-68	54-68	The system consists of eight rules which target different relations based on linguistic insights .	The system consists of eight rules which target different relations based on linguistic insights .	1<2	bg-general	bg-general
D14-1222	15-29	69-88	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution ,	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	15-53	69-88	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Our rule-based system significantly outperforms a reimplementation of a previous rule-based system ( Vieira and Poesio , 2000 ) .	1<2	evaluation	evaluation
D14-1222	15-29	89-97	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution ,	Furthermore , it performs better than a learning-based approach	15-53	89-110	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	1<2	evaluation	evaluation
D14-1222	89-97	98-110	Furthermore , it performs better than a learning-based approach	which has access to the same knowledge resources as the rule-based system .	89-110	89-110	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	Furthermore , it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system .	1<2	elab-addition	elab-addition
D14-1222	15-29	111-131	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution ,	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	15-53	111-131	This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution , where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations .	Additionally , incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system .	1<2	evaluation	evaluation
D14-1223	1-10	11-26	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) ,	modern dialog systems tend to have visual rendering on the device screen as an additional modality	1-35	1-35	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	1>2	comparison	comparison
D14-1223	11-26	61-71	modern dialog systems tend to have visual rendering on the device screen as an additional modality	On-screen item identification and resolution in utterances is one critical problem	1-35	61-80	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	1>2	contrast	contrast
D14-1223	11-26	27-35	modern dialog systems tend to have visual rendering on the device screen as an additional modality	to communicate the system's response to the user .	1-35	1-35	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	1<2	enablement	enablement
D14-1223	11-26	36-46	modern dialog systems tend to have visual rendering on the device screen as an additional modality	Visual display of the system's response not only changes human behavior	1-35	36-60	Unlike traditional over-the-phone spoken dialog systems ( SDSs ) , modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user .	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	1<2	elab-addition	elab-addition
D14-1223	36-46	47-51	Visual display of the system's response not only changes human behavior	when interacting with devices ,	36-60	36-60	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	1<2	temporal	temporal
D14-1223	36-46	52-60	Visual display of the system's response not only changes human behavior	but also creates new research areas in SDSs .	36-60	36-60	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	Visual display of the system's response not only changes human behavior when interacting with devices , but also creates new research areas in SDSs .	1<2	joint	joint
D14-1223	61-71	81-88	On-screen item identification and resolution in utterances is one critical problem	We pose the problem as a classification task	61-80	81-101	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	1>2	bg-compare	bg-compare
D14-1223	61-71	72-80	On-screen item identification and resolution in utterances is one critical problem	to achieve a natural and accurate human-machine communication .	61-80	61-80	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication .	1<2	elab-addition	elab-addition
D14-1223	81-88	89-101	We pose the problem as a classification task	to correctly identify intended on-screen item ( s ) from user utterances .	81-101	81-101	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	1<2	enablement	enablement
D14-1223	102-115	116-131	Using syntactic , semantic as well as context features from the display screen ,	our model can resolve different types of referring expressions with up to 90 % accuracy .	102-131	102-131	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	1>2	manner-means	manner-means
D14-1223	81-88	116-131	We pose the problem as a classification task	our model can resolve different types of referring expressions with up to 90 % accuracy .	81-101	102-131	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	Using syntactic , semantic as well as context features from the display screen , our model can resolve different types of referring expressions with up to 90 % accuracy .	1<2	evaluation	evaluation
D14-1223	132-137	138-150	In the experiments we also show	that the proposed model is robust to domain and screen layout changes .	132-150	132-150	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	1>2	attribution	attribution
D14-1223	81-88	138-150	We pose the problem as a classification task	that the proposed model is robust to domain and screen layout changes .	81-101	132-150	We pose the problem as a classification task to correctly identify intended on-screen item ( s ) from user utterances .	In the experiments we also show that the proposed model is robust to domain and screen layout changes .	1<2	evaluation	evaluation
D14-1224	1-14	15-37	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme	to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes ,	1-50	1-50	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	1<2	elab-addition	elab-addition
D14-1224	15-37	38-50	to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes ,	largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	1-50	1-50	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	1<2	elab-addition	elab-addition
D14-1224	1-14	51-56	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme	In particular , connectives are employed	1-50	51-89	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	1<2	elab-addition	elab-addition
D14-1224	51-56	57-73	In particular , connectives are employed	to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse ,	51-89	51-89	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	1<2	enablement	enablement
D14-1224	51-56	74-89	In particular , connectives are employed	while the nuclei of discourse units are globally determined with reference to the dependency theory .	51-89	51-89	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	In particular , connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse , while the nuclei of discourse units are globally determined with reference to the dependency theory .	1<2	joint	joint
D14-1224	90-95	96-109	Guided by the CDT scheme ,	we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	90-109	90-109	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	1>2	bg-general	bg-general
D14-1224	1-14	96-109	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme	we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	1-50	90-109	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	Guided by the CDT scheme , we manually annotate a Chinese Discourse Treebank ( CDTB ) of 500 documents .	1<2	elab-addition	elab-addition
D14-1224	1-14	110-132	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme	Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus .	1-50	110-132	In this paper , we propose a Connective-driven Dependency Tree ( CDT ) scheme to represent the discourse rhetorical structure in Chinese language , with elementary discourse units as leaf nodes and connectives as non-leaf nodes , largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory .	Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus .	1<2	evaluation	evaluation
D14-1225	1-11	12-18	We propose a novel search-based approach for greedy coreference resolution ,	where the mentions are processed in order	1-25	1-25	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	1<2	elab-addition	elab-addition
D14-1225	12-18	19-25	where the mentions are processed in order	and added to previous coreference clusters .	1-25	1-25	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	1<2	joint	joint
D14-1225	1-11	26-35	We propose a novel search-based approach for greedy coreference resolution ,	Our method is distinguished by the use of two functions	1-25	26-67	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	1<2	elab-addition	elab-addition
D14-1225	26-35	36-41	Our method is distinguished by the use of two functions	to make each coreference decision :	26-67	26-67	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	1<2	enablement	enablement
D14-1225	26-35	42-44	Our method is distinguished by the use of two functions	a pruning function	26-67	26-67	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	1<2	elab-enumember	elab-enumember
D14-1225	42-44	45-53	a pruning function	that prunes bad coreference decisions from further consideration ,	26-67	26-67	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	1<2	elab-addition	elab-addition
D14-1225	42-44	54-57	a pruning function	and a scoring function	26-67	26-67	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	1<2	joint	joint
D14-1225	54-57	58-67	and a scoring function	that then selects the best among the remaining decisions .	26-67	26-67	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	Our method is distinguished by the use of two functions to make each coreference decision : a pruning function that prunes bad coreference decisions from further consideration , and a scoring function that then selects the best among the remaining decisions .	1<2	elab-addition	elab-addition
D14-1225	1-11	68-78	We propose a novel search-based approach for greedy coreference resolution ,	Our framework reduces learning of these functions to rank learning ,	1-25	68-85	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	1<2	evaluation	evaluation
D14-1225	68-78	79-85	Our framework reduces learning of these functions to rank learning ,	which helps leverage powerful off-the-shelf rank-learners .	68-85	68-85	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	Our framework reduces learning of these functions to rank learning , which helps leverage powerful off-the-shelf rank-learners .	1<2	evaluation	evaluation
D14-1225	86-87	88-99	We show	that our Prune-and-Score approach is superior to using a single scoring function	86-115	86-115	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	1>2	attribution	attribution
D14-1225	1-11	88-99	We propose a novel search-based approach for greedy coreference resolution ,	that our Prune-and-Score approach is superior to using a single scoring function	1-25	86-115	We propose a novel search-based approach for greedy coreference resolution , where the mentions are processed in order and added to previous coreference clusters .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	1<2	evaluation	evaluation
D14-1225	88-99	100-103	that our Prune-and-Score approach is superior to using a single scoring function	to make both decisions	86-115	86-115	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	1<2	enablement	enablement
D14-1225	88-99	104-112	that our Prune-and-Score approach is superior to using a single scoring function	and outperforms several state-of-the-art approaches on multiple benchmark corpora	86-115	86-115	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	1<2	joint	joint
D14-1225	104-112	113-115	and outperforms several state-of-the-art approaches on multiple benchmark corpora	including OntoNotes .	86-115	86-115	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes .	1<2	elab-example	elab-example
D14-1226	1-11	27-28,46-60	A typical discussion thread in an online forum spans multiple pages	A user <*> may not want to read all the previous posts but only a few selected posts	1-26	27-71	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	1>2	contrast	contrast
D14-1226	1-11	12-16	A typical discussion thread in an online forum spans multiple pages	involving participation from multiple users	1-26	1-26	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	1<2	elab-addition	elab-addition
D14-1226	1-11	17-26	A typical discussion thread in an online forum spans multiple pages	and thus , may contain multiple view-points and solutions .	1-26	1-26	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	A typical discussion thread in an online forum spans multiple pages involving participation from multiple users and thus , may contain multiple view-points and solutions .	1<2	progression	progression
D14-1226	27-28,46-60	72-78	A user <*> may not want to read all the previous posts but only a few selected posts	This paper describes an extractive summarization technique	27-71	72-96	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	1>2	bg-compare	bg-compare
D14-1226	27-28,46-60	29-34	A user <*> may not want to read all the previous posts but only a few selected posts	interested in the topic of discussion	27-71	27-71	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	1<2	elab-addition	elab-addition
D14-1226	29-34	35-45	interested in the topic of discussion	or having a problem similar to being discussed in the thread	27-71	27-71	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	1<2	joint	joint
D14-1226	46-60	61-71	may not want to read all the previous posts but only a few selected posts	that provide her a concise summary of the ongoing discussion .	27-71	27-71	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	A user interested in the topic of discussion or having a problem similar to being discussed in the thread may not want to read all the previous posts but only a few selected posts that provide her a concise summary of the ongoing discussion .	1<2	elab-addition	elab-addition
D14-1226	72-78	79-89	This paper describes an extractive summarization technique	that uses textual features and dialog act information of individual messages	72-96	72-96	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	1<2	elab-addition	elab-addition
D14-1226	79-89	90-96	that uses textual features and dialog act information of individual messages	to select a subset of posts .	72-96	72-96	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	1<2	enablement	enablement
D14-1226	72-78	97-100	This paper describes an extractive summarization technique	Proposed approach is evaluated	72-96	97-107	This paper describes an extractive summarization technique that uses textual features and dialog act information of individual messages to select a subset of posts .	Proposed approach is evaluated using two real life forum datasets .	1<2	evaluation	evaluation
D14-1226	97-100	101-107	Proposed approach is evaluated	using two real life forum datasets .	97-107	97-107	Proposed approach is evaluated using two real life forum datasets .	Proposed approach is evaluated using two real life forum datasets .	1<2	manner-means	manner-means
P14-1124	1-8	9-18	We aim to improve spoken term detection performance	by incorporating contextual information beyond traditional N-gram language models .	1-18	1-18	We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models .	We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models .	1<2	manner-means	manner-means
P14-1124	19-31	32-53	Instead of taking a broad view of topic context in spoken documents ,	variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	19-53	19-53	Instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	Instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	1>2	elab-addition	elab-addition
P14-1124	1-8	32-53	We aim to improve spoken term detection performance	variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	1-18	19-53	We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models .	Instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	1<2	elab-addition	elab-addition
P14-1124	54-55	66-81	We show	we are more likely to find additional instances of that term in the same document .	54-81	54-81	We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document .	We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document .	1>2	attribution	attribution
P14-1124	56-65	66-81	that given the detection of one instance of a term	we are more likely to find additional instances of that term in the same document .	54-81	54-81	We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document .	We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document .	1>2	condition	condition
P14-1124	32-53	66-81	variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	we are more likely to find additional instances of that term in the same document .	19-53	54-81	Instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents .	We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document .	1<2	elab-addition	elab-addition
P14-1124	1-8	82-87	We aim to improve spoken term detection performance	We leverage this burstiness of keywords	1-18	82-104	We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models .	We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits .	1<2	elab-addition	elab-addition
P14-1124	82-87	88-97	We leverage this burstiness of keywords	by taking the most confident keyword hypothesis in each document	82-104	82-104	We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits .	We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits .	1<2	manner-means	manner-means
P14-1124	88-97	98-104	by taking the most confident keyword hypothesis in each document	and interpolating with lower scoring hits .	82-104	82-104	We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits .	We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits .	1<2	joint	joint
P14-1124	1-8	105-110	We aim to improve spoken term detection performance	We then develop a principled approach	1-18	105-121	We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models .	We then develop a principled approach to select interpolation weights using only the ASR training data .	1<2	elab-addition	elab-addition
P14-1124	105-110	111-114	We then develop a principled approach	to select interpolation weights	105-121	105-121	We then develop a principled approach to select interpolation weights using only the ASR training data .	We then develop a principled approach to select interpolation weights using only the ASR training data .	1<2	elab-addition	elab-addition
P14-1124	111-114	115-121	to select interpolation weights	using only the ASR training data .	105-121	105-121	We then develop a principled approach to select interpolation weights using only the ASR training data .	We then develop a principled approach to select interpolation weights using only the ASR training data .	1<2	manner-means	manner-means
P14-1124	122-125	126-143	Using this re-weighting approach	we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program .	122-143	122-143	Using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program .	Using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program .	1>2	manner-means	manner-means
P14-1124	105-110	126-143	We then develop a principled approach	we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program .	105-121	122-143	We then develop a principled approach to select interpolation weights using only the ASR training data .	Using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program .	1<2	evaluation	evaluation
P14-1125	1-17	26-39	Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words ,	In this paper , we investigate the problem of character-level Chinese dependency parsing ,	1-25	26-45	Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words , enabling character-level analysis of Chinese syntactic structures .	In this paper , we investigate the problem of character-level Chinese dependency parsing , building dependency trees over characters .	1>2	bg-goal	bg-goal
P14-1125	1-17	18-25	Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words ,	enabling character-level analysis of Chinese syntactic structures .	1-25	1-25	Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words , enabling character-level analysis of Chinese syntactic structures .	Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words , enabling character-level analysis of Chinese syntactic structures .	1<2	elab-addition	elab-addition
P14-1125	26-39	40-45	In this paper , we investigate the problem of character-level Chinese dependency parsing ,	building dependency trees over characters .	26-45	26-45	In this paper , we investigate the problem of character-level Chinese dependency parsing , building dependency trees over characters .	In this paper , we investigate the problem of character-level Chinese dependency parsing , building dependency trees over characters .	1<2	elab-addition	elab-addition
P14-1125	26-39	46-51	In this paper , we investigate the problem of character-level Chinese dependency parsing ,	Character-level information can benefit downstream applications	26-45	46-65	In this paper , we investigate the problem of character-level Chinese dependency parsing , building dependency trees over characters .	Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving word-level dependency parsing accuracies .	1<2	elab-addition	elab-addition
P14-1125	46-51	52-58	Character-level information can benefit downstream applications	by offering flexible granularities for word segmentation	46-65	46-65	Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving word-level dependency parsing accuracies .	Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving word-level dependency parsing accuracies .	1<2	manner-means	manner-means
P14-1125	46-51	59-65	Character-level information can benefit downstream applications	while improving word-level dependency parsing accuracies .	46-65	46-65	Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving word-level dependency parsing accuracies .	Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving word-level dependency parsing accuracies .	1<2	joint	joint
P14-1125	26-39	66-80	In this paper , we investigate the problem of character-level Chinese dependency parsing ,	We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing .	26-45	66-80	In this paper , we investigate the problem of character-level Chinese dependency parsing , building dependency trees over characters .	We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing .	1<2	elab-addition	elab-addition
P14-1125	66-80	81-94	We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing .	Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods .	66-80	81-94	We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing .	Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods .	1<2	evaluation	evaluation
P14-1126	1-5	6-12	We present a novel approach	for inducing unsupervised dependency parsers for languages	1-28	1-28	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	1<2	elab-addition	elab-addition
P14-1126	13-19	20-28	that have no labeled training data ,	but have translated text in a resource-rich language .	1-28	1-28	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	1>2	contrast	contrast
P14-1126	6-12	20-28	for inducing unsupervised dependency parsers for languages	but have translated text in a resource-rich language .	1-28	1-28	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	1<2	elab-addition	elab-addition
P14-1126	1-5	29-36	We present a novel approach	We train probabilistic parsing models for resource-poor languages	1-28	29-47	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization .	1<2	elab-addition	elab-addition
P14-1126	29-36	37-47	We train probabilistic parsing models for resource-poor languages	by transferring cross-lingual knowledge from resource-rich language with entropy regularization .	29-47	29-47	We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization .	We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization .	1<2	manner-means	manner-means
P14-1126	1-5	48-59	We present a novel approach	Our method can be used as a purely monolingual dependency parser ,	1-28	48-80	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	Our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making it applicable to a wide range of resource-poor languages .	1<2	elab-addition	elab-addition
P14-1126	48-59	60-68	Our method can be used as a purely monolingual dependency parser ,	requiring no human translations for the test data ,	48-80	48-80	Our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making it applicable to a wide range of resource-poor languages .	Our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making it applicable to a wide range of resource-poor languages .	1<2	elab-addition	elab-addition
P14-1126	60-68	69-80	requiring no human translations for the test data ,	thus making it applicable to a wide range of resource-poor languages .	48-80	48-80	Our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making it applicable to a wide range of resource-poor languages .	Our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making it applicable to a wide range of resource-poor languages .	1<2	result	result
P14-1126	1-5	81-87	We present a novel approach	We perform experiments on three Data sets	1-28	81-108	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	We perform experiments on three Data sets  Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks , across ten languages .	1<2	elab-addition	elab-addition
P14-1126	81-87	88-104	We perform experiments on three Data sets	 Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks ,	81-108	81-108	We perform experiments on three Data sets  Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks , across ten languages .	We perform experiments on three Data sets  Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks , across ten languages .	1<2	elab-enumember	elab-enumember
P14-1126	81-87	105-108	We perform experiments on three Data sets	across ten languages .	81-108	81-108	We perform experiments on three Data sets  Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks , across ten languages .	We perform experiments on three Data sets  Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks , across ten languages .	1<2	elab-addition	elab-addition
P14-1126	1-5	109-119	We present a novel approach	We obtain state-of-the art performance of all the three data sets	1-28	109-130	We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language .	We obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems .	1<2	evaluation	evaluation
P14-1126	109-119	120-130	We obtain state-of-the art performance of all the three data sets	when compared with previously studied unsupervised and projected parsing systems .	109-130	109-130	We obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems .	We obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems .	1<2	condition	condition
P14-1127	1-5	6-10	We present a novel way	of generating unseen words ,	1-29	1-29	We present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	We present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	1<2	elab-addition	elab-addition
P14-1127	1-5	11-29	We present a novel way	which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	1-29	1-29	We present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	We present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	1<2	elab-addition	elab-addition
P14-1127	1-5	30-38	We present a novel way	We test our vocabulary generator on seven low-resource languages	1-29	30-52	We present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set .	1<2	elab-addition	elab-addition
P14-1127	30-38	39-52	We test our vocabulary generator on seven low-resource languages	by measuring the decrease in out-of-vocabulary word rate on a held-out test set .	30-52	30-52	We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set .	We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set .	1<2	manner-means	manner-means
P14-1127	1-5	53-54,57-62	We present a novel way	The languages <*> have very different morphological properties ;	1-29	53-77	We present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages .	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	1<2	evaluation	evaluation
P14-1127	53-54,57-62	55-56	The languages <*> have very different morphological properties ;	we study	53-77	53-77	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	1<2	elab-addition	elab-addition
P14-1127	63-64	65-68	we show	how our results differ	53-77	53-77	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	1>2	attribution	attribution
P14-1127	53-54,57-62	65-68	The languages <*> have very different morphological properties ;	how our results differ	53-77	53-77	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	1<2	elab-addition	elab-addition
P14-1127	65-68	69-77	how our results differ	depending on the morphological complexity of the language .	53-77	53-77	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	1<2	bg-general	bg-general
P14-1127	65-68	78-105	how our results differ	In our best result ( on Assamese ) , our approach can predict 29 % of the token-based out-of-vocabulary with a small amount of unlabeled training data .	53-77	78-105	The languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language .	In our best result ( on Assamese ) , our approach can predict 29 % of the token-based out-of-vocabulary with a small amount of unlabeled training data .	1<2	elab-addition	elab-addition
P14-1128	1-16	17-24	This study investigates on building a better Chinese word segmentation model for statistical machine translation .	It aims at leveraging word boundary information ,	1-16	17-38	This study investigates on building a better Chinese word segmentation model for statistical machine translation .	It aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model .	1<2	elab-addition	elab-addition
P14-1128	17-24	25-31	It aims at leveraging word boundary information ,	automatically learned by bilingual character-based alignments ,	17-38	17-38	It aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model .	It aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model .	1<2	elab-addition	elab-addition
P14-1128	17-24	32-38	It aims at leveraging word boundary information ,	to induce a preferable segmentation model .	17-38	17-38	It aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model .	It aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model .	1<2	enablement	enablement
P14-1128	1-16	39-49	This study investigates on building a better Chinese word segmentation model for statistical machine translation .	We propose dealing with the induced word boundaries as soft constraints	1-16	39-77	This study investigates on building a better Chinese word segmentation model for statistical machine translation .	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	1<2	elab-addition	elab-addition
P14-1128	39-49	50-60	We propose dealing with the induced word boundaries as soft constraints	to bias the continuous learning of a supervised CRFs model ,	39-77	39-77	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	1<2	enablement	enablement
P14-1128	50-60	61-77	to bias the continuous learning of a supervised CRFs model ,	trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	39-77	39-77	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	1<2	elab-addition	elab-addition
P14-1128	39-49	78-90	We propose dealing with the induced word boundaries as soft constraints	The induced word boundary information is encoded as a graph propagation constraint .	39-77	78-90	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	The induced word boundary information is encoded as a graph propagation constraint .	1<2	elab-addition	elab-addition
P14-1128	39-49	91-96	We propose dealing with the induced word boundaries as soft constraints	The constrained model induction is accomplished	39-77	91-102	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	The constrained model induction is accomplished by using posterior regularization algorithm .	1<2	elab-addition	elab-addition
P14-1128	91-96	97-102	The constrained model induction is accomplished	by using posterior regularization algorithm .	91-102	91-102	The constrained model induction is accomplished by using posterior regularization algorithm .	The constrained model induction is accomplished by using posterior regularization algorithm .	1<2	manner-means	manner-means
P14-1128	103-111	112-124	The experiments on a Chinese-to-English machine translation task reveal	that the proposed model can bring positive segmentation effects to translation quality .	103-124	103-124	The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality .	The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality .	1>2	attribution	attribution
P14-1128	39-49	112-124	We propose dealing with the induced word boundaries as soft constraints	that the proposed model can bring positive segmentation effects to translation quality .	39-77	103-124	We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) .	The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality .	1<2	evaluation	evaluation
P14-1129	1-5	21-37	Recent work has shown success	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	1-20	21-47	Recent work has shown success in using neural network language models ( NNLMs ) as features in MT systems .	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	1>2	bg-compare	bg-compare
P14-1129	1-5	6-20	Recent work has shown success	in using neural network language models ( NNLMs ) as features in MT systems .	1-20	1-20	Recent work has shown success in using neural network language models ( NNLMs ) as features in MT systems .	Recent work has shown success in using neural network language models ( NNLMs ) as features in MT systems .	1<2	elab-addition	elab-addition
P14-1129	21-37	38-47	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	which augments the NNLM with a source context window .	21-47	21-47	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	1<2	elab-addition	elab-addition
P14-1129	21-37	48-52	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	Our model is purely lexicalized	21-47	48-61	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	Our model is purely lexicalized and can be integrated into any MT decoder .	1<2	elab-addition	elab-addition
P14-1129	48-52	53-61	Our model is purely lexicalized	and can be integrated into any MT decoder .	48-61	48-61	Our model is purely lexicalized and can be integrated into any MT decoder .	Our model is purely lexicalized and can be integrated into any MT decoder .	1<2	joint	joint
P14-1129	21-37	62-69	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	We also present several variations of the NNJM	21-47	62-75	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	We also present several variations of the NNJM which provide significant additive improvements .	1<2	elab-addition	elab-addition
P14-1129	62-69	70-75	We also present several variations of the NNJM	which provide significant additive improvements .	62-75	62-75	We also present several variations of the NNJM which provide significant additive improvements .	We also present several variations of the NNJM which provide significant additive improvements .	1<2	elab-addition	elab-addition
P14-1129	76-82	83-88	Although the model is quite simple ,	it yields strong empirical results .	76-88	76-88	Although the model is quite simple , it yields strong empirical results .	Although the model is quite simple , it yields strong empirical results .	1>2	contrast	contrast
P14-1129	21-37	83-88	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	it yields strong empirical results .	21-47	76-88	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	Although the model is quite simple , it yields strong empirical results .	1<2	evaluation	evaluation
P14-1129	83-88	89-112	it yields strong empirical results .	On the NIST OpenMT12 Arabic-English condition , the NNJM features produce a gain of +3.0 BLEU on top of a powerful , feature-rich baseline	76-88	89-119	Although the model is quite simple , it yields strong empirical results .	On the NIST OpenMT12 Arabic-English condition , the NNJM features produce a gain of +3.0 BLEU on top of a powerful , feature-rich baseline which already includes a target-only NNLM .	1<2	exp-evidence	exp-evidence
P14-1129	89-112	113-119	On the NIST OpenMT12 Arabic-English condition , the NNJM features produce a gain of +3.0 BLEU on top of a powerful , feature-rich baseline	which already includes a target-only NNLM .	89-119	89-119	On the NIST OpenMT12 Arabic-English condition , the NNJM features produce a gain of +3.0 BLEU on top of a powerful , feature-rich baseline which already includes a target-only NNLM .	On the NIST OpenMT12 Arabic-English condition , the NNJM features produce a gain of +3.0 BLEU on top of a powerful , feature-rich baseline which already includes a target-only NNLM .	1<2	elab-addition	elab-addition
P14-1129	83-88	120-143	it yields strong empirical results .	The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang's (2007) original Hiero implementation .	76-88	120-143	Although the model is quite simple , it yields strong empirical results .	The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang's (2007) original Hiero implementation .	1<2	exp-evidence	exp-evidence
P14-1129	21-37	144-150	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	Additionally , we describe two novel techniques	21-47	144-164	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	Additionally , we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding .	1<2	elab-addition	elab-addition
P14-1129	144-150	151-164	Additionally , we describe two novel techniques	for overcoming the historically high cost of using NNLM-style models in MT decoding .	144-164	144-164	Additionally , we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding .	Additionally , we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding .	1<2	enablement	enablement
P14-1129	144-150	165-176	Additionally , we describe two novel techniques	These techniques speed up NNJM computation by a factor of 10,000x ,	144-164	165-187	Additionally , we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding .	These techniques speed up NNJM computation by a factor of 10,000x , making the model as fast as a standard back-off LM .	1<2	elab-addition	elab-addition
P14-1129	165-176	177-187	These techniques speed up NNJM computation by a factor of 10,000x ,	making the model as fast as a standard back-off LM .	165-187	165-187	These techniques speed up NNJM computation by a factor of 10,000x , making the model as fast as a standard back-off LM .	These techniques speed up NNJM computation by a factor of 10,000x , making the model as fast as a standard back-off LM .	1<2	elab-addition	elab-addition
P14-1129	21-37	188-199	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	This work was supported by DARPA/I2O Contract HR0011-12-C-0014 under the BOLT program	21-47	188-209	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	This work was supported by DARPA/I2O Contract HR0011-12-C-0014 under the BOLT program ( Approved for Public Release , Distribution Unlimited ) .	1<2	summary	summary
P14-1129	188-199	200-209	This work was supported by DARPA/I2O Contract HR0011-12-C-0014 under the BOLT program	( Approved for Public Release , Distribution Unlimited ) .	188-209	188-209	This work was supported by DARPA/I2O Contract HR0011-12-C-0014 under the BOLT program ( Approved for Public Release , Distribution Unlimited ) .	This work was supported by DARPA/I2O Contract HR0011-12-C-0014 under the BOLT program ( Approved for Public Release , Distribution Unlimited ) .	1<2	elab-addition	elab-addition
P14-1129	21-37	210-216,221-225	Here , we present a novel formulation for a neural network joint model ( NNJM ) ,	The views , opinions , and/or findings <*> are those of the author	21-47	210-256	Here , we present a novel formulation for a neural network joint model ( NNJM ) , which augments the NNLM with a source context window .	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	1<2	summary	summary
P14-1129	210-216,221-225	217-220	The views , opinions , and/or findings <*> are those of the author	contained in this article	210-256	210-256	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	1<2	elab-addition	elab-addition
P14-1129	221-225	226-238	are those of the author	and should not be interpreted as representing the official views or policies ,	210-256	210-256	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	1<2	joint	joint
P14-1129	226-238	239-256	and should not be interpreted as representing the official views or policies ,	either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	210-256	210-256	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	The views , opinions , and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies , either expressed or implied , of the Defense Advanced Research Projects Agency or the Department of Defense .	1<2	joint	joint
P14-1130	1-20	53-59	Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich , high-dimensional feature representations .	In this paper , we use tensors	1-20	53-69	Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich , high-dimensional feature representations .	In this paper , we use tensors to map high-dimensional feature vectors into low dimensional representations .	1>2	bg-goal	bg-goal
P14-1130	1-20	21-31	Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich , high-dimensional feature representations .	A small subset of such features is often selected manually .	1-20	21-31	Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich , high-dimensional feature representations .	A small subset of such features is often selected manually .	1<2	elab-addition	elab-addition
P14-1130	21-31	32-34	A small subset of such features is often selected manually .	This is problematic	21-31	32-52	A small subset of such features is often selected manually .	This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features .	1<2	elab-addition	elab-addition
P14-1130	32-34	35-43	This is problematic	when features lack clear linguistic meaning as in embeddings	32-52	32-52	This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features .	This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features .	1<2	condition	condition
P14-1130	35-43	44-52	when features lack clear linguistic meaning as in embeddings	or when the information is blended across features .	32-52	32-52	This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features .	This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features .	1<2	joint	joint
P14-1130	53-59	60-69	In this paper , we use tensors	to map high-dimensional feature vectors into low dimensional representations .	53-69	53-69	In this paper , we use tensors to map high-dimensional feature vectors into low dimensional representations .	In this paper , we use tensors to map high-dimensional feature vectors into low dimensional representations .	1<2	enablement	enablement
P14-1130	53-59	70-78	In this paper , we use tensors	We explicitly maintain the parameters as a low-rank tensor	53-69	70-104	In this paper , we use tensors to map high-dimensional feature vectors into low dimensional representations .	We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms .	1<2	elab-addition	elab-addition
P14-1130	70-78	79-90	We explicitly maintain the parameters as a low-rank tensor	to obtain low dimensional representations of words in their syntactic roles ,	70-104	70-104	We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms .	We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms .	1<2	enablement	enablement
P14-1130	79-90	91-104	to obtain low dimensional representations of words in their syntactic roles ,	and to leverage modularity in the tensor for easy training with online algorithms .	70-104	70-104	We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms .	We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms .	1<2	joint	joint
P14-1130	53-59	105-118	In this paper , we use tensors	Our parser consistently outperforms the Turbo and MST parsers across 14 different languages .	53-69	105-118	In this paper , we use tensors to map high-dimensional feature vectors into low dimensional representations .	Our parser consistently outperforms the Turbo and MST parsers across 14 different languages .	1<2	evaluation	evaluation
P14-1130	53-59	119-130	In this paper , we use tensors	We also obtain the best published UAS results on 5 languages .	53-69	119-130	In this paper , we use tensors to map high-dimensional feature vectors into low dimensional representations .	We also obtain the best published UAS results on 5 languages .	1<2	evaluation	evaluation
P14-1131	1-8	9-11	We present CoSimRank , a graph-theoretic similarity measure	that is efficient	1-30	1-30	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	1<2	elab-addition	elab-addition
P14-1131	9-11	12-19	that is efficient	because it can compute a single node similarity	1-30	1-30	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	1<2	cause	cause
P14-1131	12-19	20-30	because it can compute a single node similarity	without having to compute the similarities of the entire graph .	1-30	1-30	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	1<2	condition	condition
P14-1131	1-8	31-34	We present CoSimRank , a graph-theoretic similarity measure	We present equivalent formalizations	1-30	31-61	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	1<2	elab-addition	elab-addition
P14-1131	31-34	35-44	We present equivalent formalizations	that show CoSimRank's close relationship to Personalized PageRank and SimRank	31-61	31-61	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	1<2	elab-addition	elab-addition
P14-1131	45-47	48-57	and also show	how we can take advantage of fast matrix multiplication algorithms	31-61	31-61	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	1>2	attribution	attribution
P14-1131	35-44	48-57	that show CoSimRank's close relationship to Personalized PageRank and SimRank	how we can take advantage of fast matrix multiplication algorithms	31-61	31-61	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	1<2	joint	joint
P14-1131	48-57	58-61	how we can take advantage of fast matrix multiplication algorithms	to compute CoSimRank .	31-61	31-61	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	We present equivalent formalizations that show CoSimRank's close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank .	1<2	enablement	enablement
P14-1131	1-8	62-83	We present CoSimRank , a graph-theoretic similarity measure	Another advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures .	1-30	62-83	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	Another advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures .	1<2	elab-addition	elab-addition
P14-1131	1-8	84-108	We present CoSimRank , a graph-theoretic similarity measure	In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction , CoSimRank is faster or more accurate than previous approaches .	1-30	84-108	We present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph .	In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction , CoSimRank is faster or more accurate than previous approaches .	1<2	evaluation	evaluation
P14-1132	1-5	28-44	Following up on recent work	we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning ,	1-63	1-63	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	1>2	bg-general	bg-general
P14-1132	1-5	6-27	Following up on recent work	on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images ,	1-63	1-63	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	1<2	elab-addition	elab-addition
P14-1132	28-44	45-59	we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning ,	in which an image of a previously unseen object is mapped to a linguistic representation	1-63	1-63	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	1<2	elab-addition	elab-addition
P14-1132	45-59	60-63	in which an image of a previously unseen object is mapped to a linguistic representation	denoting its word .	1-63	1-63	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	1<2	elab-addition	elab-addition
P14-1132	28-44	64-81	we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning ,	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task ,	1-63	64-100	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task , in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts .	1<2	elab-addition	elab-addition
P14-1132	64-81	82-100	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task ,	in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts .	64-100	64-100	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task , in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts .	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task , in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts .	1<2	elab-addition	elab-addition
P14-1132	101-107	125-135	By combining prior linguistic and visual knowledge	the learner must learn to associate new objects with words .	101-135	101-135	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	1>2	manner-means	manner-means
P14-1132	101-107	108-114	By combining prior linguistic and visual knowledge	acquired about words and their objects ,	101-135	101-135	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	1<2	elab-addition	elab-addition
P14-1132	101-107	115-124	By combining prior linguistic and visual knowledge	as well as exploiting the limited new evidence available ,	101-135	101-135	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	1<2	joint	joint
P14-1132	64-81	125-135	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task ,	the learner must learn to associate new objects with words .	64-100	101-135	We then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task , in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts .	By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words .	1<2	elab-addition	elab-addition
P14-1132	28-44	136-146	we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning ,	Our results on this task pave the way to realistic simulations	1-63	136-164	Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word .	Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts .	1<2	evaluation	evaluation
P14-1132	136-146	147-155	Our results on this task pave the way to realistic simulations	of how children or robots could use existing knowledge	136-164	136-164	Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts .	Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts .	1<2	elab-addition	elab-addition
P14-1132	147-155	156-164	of how children or robots could use existing knowledge	to bootstrap grounded semantic knowledge about new concepts .	136-164	136-164	Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts .	Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts .	1<2	enablement	enablement
P14-1133	1-11	55-66	A central challenge in semantic parsing is handling the myriad ways	In this paper , we turn semantic parsing on its head .	1-20	55-66	A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed .	In this paper , we turn semantic parsing on its head .	1>2	bg-goal	bg-goal
P14-1133	1-11	12-20	A central challenge in semantic parsing is handling the myriad ways	in which knowledge base predicates can be expressed .	1-20	1-20	A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed .	A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed .	1<2	elab-addition	elab-addition
P14-1133	1-11	21-29	A central challenge in semantic parsing is handling the myriad ways	Traditionally , semantic parsers are trained primarily from text	1-20	21-35	A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed .	Traditionally , semantic parsers are trained primarily from text paired with knowledge base information .	1<2	elab-addition	elab-addition
P14-1133	21-29	30-35	Traditionally , semantic parsers are trained primarily from text	paired with knowledge base information .	21-35	21-35	Traditionally , semantic parsers are trained primarily from text paired with knowledge base information .	Traditionally , semantic parsers are trained primarily from text paired with knowledge base information .	1<2	elab-addition	elab-addition
P14-1133	1-11	36-47	A central challenge in semantic parsing is handling the myriad ways	Our goal is to exploit the much larger amounts of raw text	1-20	36-54	A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed .	Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base .	1<2	elab-addition	elab-addition
P14-1133	36-47	48-54	Our goal is to exploit the much larger amounts of raw text	not tied to any knowledge base .	36-54	36-54	Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base .	Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base .	1<2	elab-addition	elab-addition
P14-1133	67-71	72-96	Given an input utterance ,	we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each .	67-96	67-96	Given an input utterance , we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each .	Given an input utterance , we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each .	1>2	condition	condition
P14-1133	55-66	72-96	In this paper , we turn semantic parsing on its head .	we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each .	55-66	67-96	In this paper , we turn semantic parsing on its head .	Given an input utterance , we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each .	1<2	elab-process_step	elab-process_step
P14-1133	55-66	97-107	In this paper , we turn semantic parsing on its head .	Then , we use a paraphrase model to choose the realization	55-66	97-120	In this paper , we turn semantic parsing on its head .	Then , we use a paraphrase model to choose the realization that best paraphrases the input , and output the corresponding logical form .	1<2	elab-process_step	elab-process_step
P14-1133	97-107	108-113	Then , we use a paraphrase model to choose the realization	that best paraphrases the input ,	97-120	97-120	Then , we use a paraphrase model to choose the realization that best paraphrases the input , and output the corresponding logical form .	Then , we use a paraphrase model to choose the realization that best paraphrases the input , and output the corresponding logical form .	1<2	elab-addition	elab-addition
P14-1133	97-107	114-120	Then , we use a paraphrase model to choose the realization	and output the corresponding logical form .	97-120	97-120	Then , we use a paraphrase model to choose the realization that best paraphrases the input , and output the corresponding logical form .	Then , we use a paraphrase model to choose the realization that best paraphrases the input , and output the corresponding logical form .	1<2	progression	progression
P14-1133	55-66	121-136	In this paper , we turn semantic parsing on its head .	We present two simple paraphrase models , an association model and a vector space model ,	55-66	121-144	In this paper , we turn semantic parsing on its head .	We present two simple paraphrase models , an association model and a vector space model , and train them jointly from question-answer pairs .	1<2	elab-addition	elab-addition
P14-1133	121-136	137-144	We present two simple paraphrase models , an association model and a vector space model ,	and train them jointly from question-answer pairs .	121-144	121-144	We present two simple paraphrase models , an association model and a vector space model , and train them jointly from question-answer pairs .	We present two simple paraphrase models , an association model and a vector space model , and train them jointly from question-answer pairs .	1<2	progression	progression
P14-1133	55-66	145-157	In this paper , we turn semantic parsing on its head .	Our system PARASEMPRE improves state-of-the-art accuracies on two recently released question-answering datasets .	55-66	145-157	In this paper , we turn semantic parsing on its head .	Our system PARASEMPRE improves state-of-the-art accuracies on two recently released question-answering datasets .	1<2	evaluation	evaluation
P14-1134	1-10	22-26	Abstract Meaning Representation ( AMR ) is a semantic formalism	We introduce the first approach	1-21	22-41	Abstract Meaning Representation ( AMR ) is a semantic formalism for which a grow-ing set of annotated examples is available .	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	1>2	bg-goal	bg-goal
P14-1134	1-10	11-21	Abstract Meaning Representation ( AMR ) is a semantic formalism	for which a grow-ing set of annotated examples is available .	1-21	1-21	Abstract Meaning Representation ( AMR ) is a semantic formalism for which a grow-ing set of annotated examples is available .	Abstract Meaning Representation ( AMR ) is a semantic formalism for which a grow-ing set of annotated examples is available .	1<2	elab-addition	elab-addition
P14-1134	22-26	27-33	We introduce the first approach	to parse sentences into this representation ,	22-41	22-41	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	1<2	elab-addition	elab-addition
P14-1134	22-26	34-41	We introduce the first approach	providing a strong baseline for future improvement .	22-41	22-41	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	1<2	elab-addition	elab-addition
P14-1134	22-26	42-49	We introduce the first approach	The method is based on a novel algorithm	22-41	42-73	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	1<2	elab-addition	elab-addition
P14-1134	42-49	50-58	The method is based on a novel algorithm	for finding a maximum spanning , connected subgraph ,	42-73	42-73	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	1<2	elab-addition	elab-addition
P14-1134	50-58	59-67	for finding a maximum spanning , connected subgraph ,	embedded within a Lagrangian relaxation of an optimization problem	42-73	42-73	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	1<2	elab-addition	elab-addition
P14-1134	59-67	68-73	embedded within a Lagrangian relaxation of an optimization problem	that imposes linguistically inspired constraints .	42-73	42-73	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	The method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints .	1<2	elab-addition	elab-addition
P14-1134	22-26	74-85	We introduce the first approach	Our approach is described in the general frame-work of structured prediction ,	22-41	74-103	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	Our approach is described in the general frame-work of structured prediction , allowing future incorporation of additional features and constraints , and may extend to other formalisms as well .	1<2	elab-addition	elab-addition
P14-1134	74-85	86-94	Our approach is described in the general frame-work of structured prediction ,	allowing future incorporation of additional features and constraints ,	74-103	74-103	Our approach is described in the general frame-work of structured prediction , allowing future incorporation of additional features and constraints , and may extend to other formalisms as well .	Our approach is described in the general frame-work of structured prediction , allowing future incorporation of additional features and constraints , and may extend to other formalisms as well .	1<2	elab-addition	elab-addition
P14-1134	74-85	95-103	Our approach is described in the general frame-work of structured prediction ,	and may extend to other formalisms as well .	74-103	74-103	Our approach is described in the general frame-work of structured prediction , allowing future incorporation of additional features and constraints , and may extend to other formalisms as well .	Our approach is described in the general frame-work of structured prediction , allowing future incorporation of additional features and constraints , and may extend to other formalisms as well .	1<2	joint	joint
P14-1134	22-26	104-116	We introduce the first approach	Our open-source system , JAMR , is available at : http : //github.com/jflanigan/jamr	22-41	104-116	We introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement .	Our open-source system , JAMR , is available at : http : //github.com/jflanigan/jamr	1<2	elab-addition	elab-addition
P14-1135	1-4	5-9	We present an approach	for learning context-dependent semantic parsers	1-16	1-16	We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions .	We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions .	1<2	elab-addition	elab-addition
P14-1135	5-9	10-16	for learning context-dependent semantic parsers	to identify and interpret time expressions .	1-16	1-16	We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions .	We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions .	1<2	enablement	enablement
P14-1135	1-4	17-22	We present an approach	We use a Combinatory Categorial Grammar	1-16	17-54	We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions .	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	1<2	elab-addition	elab-addition
P14-1135	17-22	23-28	We use a Combinatory Categorial Grammar	to construct compositional meaning representations ,	17-54	17-54	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	1<2	enablement	enablement
P14-1135	23-28	29-47	to construct compositional meaning representations ,	while considering contextual cues , such as the document creation time and the tense of the governing verb ,	17-54	17-54	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	1<2	manner-means	manner-means
P14-1135	29-47	48-54	while considering contextual cues , such as the document creation time and the tense of the governing verb ,	to compute the final time values .	17-54	17-54	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	We use a Combinatory Categorial Grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values .	1<2	enablement	enablement
P14-1135	55-59	60-80	Experiments on benchmark datasets show	that our approach outperforms previous state-of-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .	55-80	55-80	Experiments on benchmark datasets show that our approach outperforms previous state-of-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .	Experiments on benchmark datasets show that our approach outperforms previous state-of-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .	1>2	attribution	attribution
P14-1135	1-4	60-80	We present an approach	that our approach outperforms previous state-of-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .	1-16	55-80	We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions .	Experiments on benchmark datasets show that our approach outperforms previous state-of-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .	1<2	evaluation	evaluation
P14-1136	1-9	10-19	We present a novel technique for semantic frame identification	using distributed representations of predicates and their syntactic context ;	1-33	1-33	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	1<2	manner-means	manner-means
P14-1136	1-9	20-33	We present a novel technique for semantic frame identification	this technique leverages automatic syntactic parses and a generic set of word embeddings .	1-33	1-33	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	1<2	elab-addition	elab-addition
P14-1136	34-36	42-45	Given labeled data	we learn a model	34-65	34-65	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	1>2	condition	condition
P14-1136	34-36	37-41	Given labeled data	annotated with frame-semantic parses ,	34-65	34-65	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	1<2	elab-addition	elab-addition
P14-1136	1-9	42-45	We present a novel technique for semantic frame identification	we learn a model	1-33	34-65	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	1<2	elab-addition	elab-addition
P14-1136	42-45	46-65	we learn a model	that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	34-65	34-65	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	1<2	elab-addition	elab-addition
P14-1136	42-45	66-74	we learn a model	The latter is used for semantic frame identification ;	34-65	66-94	Given labeled data annotated with frame-semantic parses , we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation .	The latter is used for semantic frame identification ; with a standard argument identification method inspired by prior work , we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	1<2	elab-addition	elab-addition
P14-1136	75-80	86-94	with a standard argument identification method	we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	66-94	66-94	The latter is used for semantic frame identification ; with a standard argument identification method inspired by prior work , we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	The latter is used for semantic frame identification ; with a standard argument identification method inspired by prior work , we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	1>2	manner-means	manner-means
P14-1136	75-80	81-85	with a standard argument identification method	inspired by prior work ,	66-94	66-94	The latter is used for semantic frame identification ; with a standard argument identification method inspired by prior work , we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	The latter is used for semantic frame identification ; with a standard argument identification method inspired by prior work , we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	1<2	elab-addition	elab-addition
P14-1136	1-9	86-94	We present a novel technique for semantic frame identification	we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	1-33	66-94	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	The latter is used for semantic frame identification ; with a standard argument identification method inspired by prior work , we achieve state-of-the-art results on FrameNet-style frame-semantic analysis .	1<2	evaluation	evaluation
P14-1136	1-9	95-105	We present a novel technique for semantic frame identification	Additionally , we report strong results on PropBank-style semantic role labeling	1-33	95-111	We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context ; this technique leverages automatic syntactic parses and a generic set of word embeddings .	Additionally , we report strong results on PropBank-style semantic role labeling in comparison to prior work .	1<2	evaluation	evaluation
P14-1136	95-105	106-111	Additionally , we report strong results on PropBank-style semantic role labeling	in comparison to prior work .	95-111	95-111	Additionally , we report strong results on PropBank-style semantic role labeling in comparison to prior work .	Additionally , we report strong results on PropBank-style semantic role labeling in comparison to prior work .	1<2	comparison	comparison
P14-1137	1-2,9-15	16-25	The sense <*> determines the translation of the word .	In this paper , we propose a sense-based translation model	1-15	16-34	The sense in which a word is used determines the translation of the word .	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	1>2	bg-goal	bg-goal
P14-1137	1-2,9-15	3-8	The sense <*> determines the translation of the word .	in which a word is used	1-15	1-15	The sense in which a word is used determines the translation of the word .	The sense in which a word is used determines the translation of the word .	1<2	elab-addition	elab-addition
P14-1137	16-25	26-34	In this paper , we propose a sense-based translation model	to integrate word senses into statistical machine translation .	16-34	16-34	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	1<2	enablement	enablement
P14-1137	16-25	35-40	In this paper , we propose a sense-based translation model	We build a broad-coverage sense tagger	16-34	35-59	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language .	1<2	elab-addition	elab-addition
P14-1137	35-40	41-47	We build a broad-coverage sense tagger	based on a nonparametric Bayesian topic model	35-59	35-59	We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language .	We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language .	1<2	bg-general	bg-general
P14-1137	41-47	48-59	based on a nonparametric Bayesian topic model	that automatically learns sense clusters for words in the source language .	35-59	35-59	We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language .	We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language .	1<2	elab-addition	elab-addition
P14-1137	16-25	60-74	In this paper , we propose a sense-based translation model	The proposed sense-based translation model enables the decoder to select appropriate translations for source words	16-34	60-87	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers .	1<2	elab-addition	elab-addition
P14-1137	60-74	75-82	The proposed sense-based translation model enables the decoder to select appropriate translations for source words	according to the inferred senses for these words	60-87	60-87	The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers .	The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers .	1<2	bg-general	bg-general
P14-1137	60-74	83-87	The proposed sense-based translation model enables the decoder to select appropriate translations for source words	using maximum entropy classifiers .	60-87	60-87	The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers .	The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers .	1<2	manner-means	manner-means
P14-1137	16-25	88-97	In this paper , we propose a sense-based translation model	Our method is significantly different from previous word sense disambiguation	16-34	88-111	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature .	1<2	comparison	comparison
P14-1137	88-97	98-101	Our method is significantly different from previous word sense disambiguation	reformulated for machine translation	88-111	88-111	Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature .	Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature .	1<2	elab-addition	elab-addition
P14-1137	88-97	102-111	Our method is significantly different from previous word sense disambiguation	in that the latter neglects word senses in nature .	88-111	88-111	Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature .	Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature .	1<2	exp-reason	exp-reason
P14-1137	16-25	112-128	In this paper , we propose a sense-based translation model	We test the effectiveness of the proposed sense-based translation model on a large-scale Chinese-to-English translation task .	16-34	112-128	In this paper , we propose a sense-based translation model to integrate word senses into statistical machine translation .	We test the effectiveness of the proposed sense-based translation model on a large-scale Chinese-to-English translation task .	1<2	evaluation	evaluation
P14-1137	129-130	131-149	Results show	that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation .	129-149	129-149	Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation .	Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation .	1>2	attribution	attribution
P14-1137	112-128	131-149	We test the effectiveness of the proposed sense-based translation model on a large-scale Chinese-to-English translation task .	that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation .	112-128	129-149	We test the effectiveness of the proposed sense-based translation model on a large-scale Chinese-to-English translation task .	Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation .	1<2	elab-addition	elab-addition
P14-1138	1-7	8-17	This study proposes a word alignment model	based on a recurrent neural network ( RNN ) ,	1-31	1-31	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	1<2	bg-general	bg-general
P14-1138	1-7	18-31	This study proposes a word alignment model	in which an unlimited alignment history is represented by recurrently connected hidden layers .	1-31	1-31	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	1<2	elab-addition	elab-addition
P14-1138	1-7	32-35	This study proposes a word alignment model	We perform unsupervised learning	1-31	32-59	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	We perform unsupervised learning using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) , which utilizes artificially generated negative samples .	1<2	elab-addition	elab-addition
P14-1138	32-35	36-52	We perform unsupervised learning	using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) ,	32-59	32-59	We perform unsupervised learning using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) , which utilizes artificially generated negative samples .	We perform unsupervised learning using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) , which utilizes artificially generated negative samples .	1<2	manner-means	manner-means
P14-1138	36-52	53-59	using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) ,	which utilizes artificially generated negative samples .	32-59	32-59	We perform unsupervised learning using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) , which utilizes artificially generated negative samples .	We perform unsupervised learning using noise-contrastive estimation ( Gutmann and Hyvrinen , 2010 ; Mnih and Teh , 2012 ) , which utilizes artificially generated negative samples .	1<2	elab-addition	elab-addition
P14-1138	1-7	60-65	This study proposes a word alignment model	Our alignment model is directional ,	1-31	60-79	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	Our alignment model is directional , similar to the generative IBM models ( Brown et al. , 1993 ) .	1<2	elab-addition	elab-addition
P14-1138	60-65	66-79	Our alignment model is directional ,	similar to the generative IBM models ( Brown et al. , 1993 ) .	60-79	60-79	Our alignment model is directional , similar to the generative IBM models ( Brown et al. , 1993 ) .	Our alignment model is directional , similar to the generative IBM models ( Brown et al. , 1993 ) .	1<2	comparison	comparison
P14-1138	80-84	85-92	To overcome this limitation ,	we encourage agreement between the two directional models	80-109	80-109	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	1>2	enablement	enablement
P14-1138	60-65	85-92	Our alignment model is directional ,	we encourage agreement between the two directional models	60-79	80-109	Our alignment model is directional , similar to the generative IBM models ( Brown et al. , 1993 ) .	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	1<2	elab-addition	elab-addition
P14-1138	85-92	93-97	we encourage agreement between the two directional models	by introducing a penalty function	80-109	80-109	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	1<2	manner-means	manner-means
P14-1138	93-97	98-106	by introducing a penalty function	that ensures word embedding consistency across two directional models	80-109	80-109	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	1<2	elab-addition	elab-addition
P14-1138	93-97	107-109	by introducing a penalty function	during training .	80-109	80-109	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training .	1<2	temporal	temporal
P14-1138	1-7	110-140	This study proposes a word alignment model	The RNN-based model outperforms the feed-forward neural network-based model ( Yang et al. , 2013 ) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks ,	1-31	110-155	This study proposes a word alignment model based on a recurrent neural network ( RNN ) , in which an unlimited alignment history is represented by recurrently connected hidden layers .	The RNN-based model outperforms the feed-forward neural network-based model ( Yang et al. , 2013 ) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks , and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks .	1<2	evaluation	evaluation
P14-1138	110-140	141-155	The RNN-based model outperforms the feed-forward neural network-based model ( Yang et al. , 2013 ) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks ,	and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks .	110-155	110-155	The RNN-based model outperforms the feed-forward neural network-based model ( Yang et al. , 2013 ) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks , and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks .	The RNN-based model outperforms the feed-forward neural network-based model ( Yang et al. , 2013 ) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks , and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks .	1<2	joint	joint
P14-1139	1-17	18-27	Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word aligners .	Unfortunately , most bidirectional formulations are NP-Hard to solve ,	1-17	18-45	Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word aligners .	Unfortunately , most bidirectional formulations are NP-Hard to solve , and a previous attempt to use a relaxation-based decoder yielded few exact solutions ( 6 % ) .	1>2	contrast	contrast
P14-1139	18-27	46-50	Unfortunately , most bidirectional formulations are NP-Hard to solve ,	We present a novel relaxation	18-45	46-63	Unfortunately , most bidirectional formulations are NP-Hard to solve , and a previous attempt to use a relaxation-based decoder yielded few exact solutions ( 6 % ) .	We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	1>2	bg-compare	bg-compare
P14-1139	18-27	28-45	Unfortunately , most bidirectional formulations are NP-Hard to solve ,	and a previous attempt to use a relaxation-based decoder yielded few exact solutions ( 6 % ) .	18-45	18-45	Unfortunately , most bidirectional formulations are NP-Hard to solve , and a previous attempt to use a relaxation-based decoder yielded few exact solutions ( 6 % ) .	Unfortunately , most bidirectional formulations are NP-Hard to solve , and a previous attempt to use a relaxation-based decoder yielded few exact solutions ( 6 % ) .	1<2	joint	joint
P14-1139	46-50	51-63	We present a novel relaxation	for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	46-63	46-63	We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	1<2	elab-addition	elab-addition
P14-1139	46-50	64-77	We present a novel relaxation	The relaxation can be solved with a modified version of the Viterbi algorithm .	46-63	64-77	We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	The relaxation can be solved with a modified version of the Viterbi algorithm .	1<2	elab-addition	elab-addition
P14-1139	78-85	86-97	To find optimal solutions on difficult instances ,	we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning .	78-97	78-97	To find optimal solutions on difficult instances , we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning .	To find optimal solutions on difficult instances , we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning .	1>2	enablement	enablement
P14-1139	46-50	86-97	We present a novel relaxation	we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning .	46-63	78-97	We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	To find optimal solutions on difficult instances , we alternate between incrementally adding constraints and applying optimality-preserving coarse-to-fine pruning .	1<2	elab-addition	elab-addition
P14-1139	46-50	98-109	We present a novel relaxation	The algorithm finds provably exact solutions on 86 % of sentence pairs	46-63	98-116	We present a novel relaxation for decoding the bidirectional model of DeNero and Macherey ( 2011 ) .	The algorithm finds provably exact solutions on 86 % of sentence pairs and shows improvements over directional models .	1<2	evaluation	evaluation
P14-1139	98-109	110-116	The algorithm finds provably exact solutions on 86 % of sentence pairs	and shows improvements over directional models .	98-116	98-116	The algorithm finds provably exact solutions on 86 % of sentence pairs and shows improvements over directional models .	The algorithm finds provably exact solutions on 86 % of sentence pairs and shows improvements over directional models .	1<2	joint	joint
P14-1140	1-15	16-26	In this paper , we propose a novel recursive recurrent neural network ( R2NN )	to model the end-to-end decoding process for statistical machine translation .	1-26	1-26	In this paper , we propose a novel recursive recurrent neural network ( R2NN ) to model the end-to-end decoding process for statistical machine translation .	In this paper , we propose a novel recursive recurrent neural network ( R2NN ) to model the end-to-end decoding process for statistical machine translation .	1<2	enablement	enablement
P14-1140	1-15	27-39	In this paper , we propose a novel recursive recurrent neural network ( R2NN )	R2NN is a combination of recursive neural network and recurrent neural network ,	1-26	27-107	In this paper , we propose a novel recursive recurrent neural network ( R2NN ) to model the end-to-end decoding process for statistical machine translation .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	elab-addition	elab-addition
P14-1140	27-39	40-47	R2NN is a combination of recursive neural network and recurrent neural network ,	and in turn integrates their respective capabilities :	27-107	27-107	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	progression	progression
P14-1140	40-47	48-55	and in turn integrates their respective capabilities :	( 1 ) new information can be used	27-107	27-107	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	elab-aspect	elab-aspect
P14-1140	48-55	56-67	( 1 ) new information can be used	to generate the next hidden state , like recurrent neural networks ,	27-107	27-107	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	enablement	enablement
P14-1140	48-55	68-79	( 1 ) new information can be used	so that language model and translation model can be integrated naturally ;	27-107	27-107	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	result	result
P14-1140	40-47	80-94	and in turn integrates their respective capabilities :	( 2 ) a tree structure can be built , as recursive neural networks ,	27-107	27-107	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	elab-aspect	elab-aspect
P14-1140	80-94	95-107	( 2 ) a tree structure can be built , as recursive neural networks ,	so as to generate the translation candidates in a bottom up manner .	27-107	27-107	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	R2NN is a combination of recursive neural network and recurrent neural network , and in turn integrates their respective capabilities : ( 1 ) new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; ( 2 ) a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner .	1<2	enablement	enablement
P14-1140	1-15	108-113	In this paper , we propose a novel recursive recurrent neural network ( R2NN )	A semi-supervised training approach is proposed	1-26	108-131	In this paper , we propose a novel recursive recurrent neural network ( R2NN ) to model the end-to-end decoding process for statistical machine translation .	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	1<2	elab-addition	elab-addition
P14-1140	108-113	114-118	A semi-supervised training approach is proposed	to train the parameters ,	108-131	108-131	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	1<2	enablement	enablement
P14-1140	108-113	119-125	A semi-supervised training approach is proposed	and the phrase pair embedding is explored	108-131	108-131	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	1<2	joint	joint
P14-1140	119-125	126-131	and the phrase pair embedding is explored	to model translation confidence directly .	108-131	108-131	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	A semi-supervised training approach is proposed to train the parameters , and the phrase pair embedding is explored to model translation confidence directly .	1<2	enablement	enablement
P14-1140	132-140	141-156	Experiments on a Chinese to English translation task show	that our proposed R2NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU .	132-156	132-156	Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU .	Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU .	1>2	attribution	attribution
P14-1140	1-15	141-156	In this paper , we propose a novel recursive recurrent neural network ( R2NN )	that our proposed R2NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU .	1-26	132-156	In this paper , we propose a novel recursive recurrent neural network ( R2NN ) to model the end-to-end decoding process for statistical machine translation .	Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU .	1<2	evaluation	evaluation
P14-1141	1-19	26-30	Instructor intervention in student discussion forums is a vital component in Massive Open Online Courses ( MOOCs ) ,	This paper introduces the problem	1-25	26-38	Instructor intervention in student discussion forums is a vital component in Massive Open Online Courses ( MOOCs ) , where personalized interaction is limited .	This paper introduces the problem of predicting instructor interventions in MOOC forums .	1>2	bg-goal	bg-goal
P14-1141	1-19	20-25	Instructor intervention in student discussion forums is a vital component in Massive Open Online Courses ( MOOCs ) ,	where personalized interaction is limited .	1-25	1-25	Instructor intervention in student discussion forums is a vital component in Massive Open Online Courses ( MOOCs ) , where personalized interaction is limited .	Instructor intervention in student discussion forums is a vital component in Massive Open Online Courses ( MOOCs ) , where personalized interaction is limited .	1<2	elab-addition	elab-addition
P14-1141	26-30	31-38	This paper introduces the problem	of predicting instructor interventions in MOOC forums .	26-38	26-38	This paper introduces the problem of predicting instructor interventions in MOOC forums .	This paper introduces the problem of predicting instructor interventions in MOOC forums .	1<2	elab-addition	elab-addition
P14-1141	26-30	39-43	This paper introduces the problem	We propose several prediction models	26-38	39-61	This paper introduces the problem of predicting instructor interventions in MOOC forums .	We propose several prediction models designed to capture unique aspects of MOOCs , combining course information , forum structure and posts content .	1<2	elab-addition	elab-addition
P14-1141	39-43	44-51	We propose several prediction models	designed to capture unique aspects of MOOCs ,	39-61	39-61	We propose several prediction models designed to capture unique aspects of MOOCs , combining course information , forum structure and posts content .	We propose several prediction models designed to capture unique aspects of MOOCs , combining course information , forum structure and posts content .	1<2	elab-addition	elab-addition
P14-1141	39-43	52-61	We propose several prediction models	combining course information , forum structure and posts content .	39-61	39-61	We propose several prediction models designed to capture unique aspects of MOOCs , combining course information , forum structure and posts content .	We propose several prediction models designed to capture unique aspects of MOOCs , combining course information , forum structure and posts content .	1<2	elab-addition	elab-addition
P14-1141	39-43	62-70	We propose several prediction models	Our models abstract contents of individual posts of threads	39-61	62-83	We propose several prediction models designed to capture unique aspects of MOOCs , combining course information , forum structure and posts content .	Our models abstract contents of individual posts of threads using latent categories , learned jointly with the binary intervention prediction problem .	1<2	elab-addition	elab-addition
P14-1141	62-70	71-74	Our models abstract contents of individual posts of threads	using latent categories ,	62-83	62-83	Our models abstract contents of individual posts of threads using latent categories , learned jointly with the binary intervention prediction problem .	Our models abstract contents of individual posts of threads using latent categories , learned jointly with the binary intervention prediction problem .	1<2	manner-means	manner-means
P14-1141	71-74	75-83	using latent categories ,	learned jointly with the binary intervention prediction problem .	62-83	62-83	Our models abstract contents of individual posts of threads using latent categories , learned jointly with the binary intervention prediction problem .	Our models abstract contents of individual posts of threads using latent categories , learned jointly with the binary intervention prediction problem .	1<2	elab-addition	elab-addition
P14-1141	84-91	92-107	Experiments over data from two Coursera MOOCs demonstrate	that incorporating the structure of threads into the learning problem leads to better predictive performance .	84-107	84-107	Experiments over data from two Coursera MOOCs demonstrate that incorporating the structure of threads into the learning problem leads to better predictive performance .	Experiments over data from two Coursera MOOCs demonstrate that incorporating the structure of threads into the learning problem leads to better predictive performance .	1>2	attribution	attribution
P14-1141	26-30	92-107	This paper introduces the problem	that incorporating the structure of threads into the learning problem leads to better predictive performance .	26-38	84-107	This paper introduces the problem of predicting instructor interventions in MOOC forums .	Experiments over data from two Coursera MOOCs demonstrate that incorporating the structure of threads into the learning problem leads to better predictive performance .	1<2	evaluation	evaluation
P14-1142	1-21	69-74	It is very important for Chinese language processing with the aid of an efficient input method engine ( IME ) ,	we propose a joint graph model	1-33	55-84	It is very important for Chinese language processing with the aid of an efficient input method engine ( IME ) , of which pinyin-to-Chinese ( PTC ) conversion is the core part .	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	1>2	bg-goal	bg-goal
P14-1142	1-21	22-33	It is very important for Chinese language processing with the aid of an efficient input method engine ( IME ) ,	of which pinyin-to-Chinese ( PTC ) conversion is the core part .	1-33	1-33	It is very important for Chinese language processing with the aid of an efficient input method engine ( IME ) , of which pinyin-to-Chinese ( PTC ) conversion is the core part .	It is very important for Chinese language processing with the aid of an efficient input method engine ( IME ) , of which pinyin-to-Chinese ( PTC ) conversion is the core part .	1<2	elab-addition	elab-addition
P14-1142	34-39	45-54	Meanwhile , though typos are inevitable	existing IMEs paid little attention to such big inconvenience .	34-54	34-54	Meanwhile , though typos are inevitable during user pinyin inputting , existing IMEs paid little attention to such big inconvenience .	Meanwhile , though typos are inevitable during user pinyin inputting , existing IMEs paid little attention to such big inconvenience .	1>2	contrast	contrast
P14-1142	34-39	40-44	Meanwhile , though typos are inevitable	during user pinyin inputting ,	34-54	34-54	Meanwhile , though typos are inevitable during user pinyin inputting , existing IMEs paid little attention to such big inconvenience .	Meanwhile , though typos are inevitable during user pinyin inputting , existing IMEs paid little attention to such big inconvenience .	1<2	temporal	temporal
P14-1142	45-54	69-74	existing IMEs paid little attention to such big inconvenience .	we propose a joint graph model	34-54	55-84	Meanwhile , though typos are inevitable during user pinyin inputting , existing IMEs paid little attention to such big inconvenience .	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	1>2	bg-compare	bg-compare
P14-1142	55-68	69-74	In this paper , motivated by a key equivalence of two decoding algorithms ,	we propose a joint graph model	55-84	55-84	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	1>2	elab-addition	elab-addition
P14-1142	69-74	75-84	we propose a joint graph model	to globally optimize PTC and typo correction for IME .	55-84	55-84	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	1<2	enablement	enablement
P14-1142	85-88	89-100	The evaluation results show	that the proposed method outperforms both existing academic and commercial IMEs .	85-100	85-100	The evaluation results show that the proposed method outperforms both existing academic and commercial IMEs .	The evaluation results show that the proposed method outperforms both existing academic and commercial IMEs .	1>2	attribution	attribution
P14-1142	69-74	89-100	we propose a joint graph model	that the proposed method outperforms both existing academic and commercial IMEs .	55-84	85-100	In this paper , motivated by a key equivalence of two decoding algorithms , we propose a joint graph model to globally optimize PTC and typo correction for IME .	The evaluation results show that the proposed method outperforms both existing academic and commercial IMEs .	1<2	evaluation	evaluation
P14-1143	1-23	49-58	Natural touch interfaces , common now in devices such as tablets and smartphones , make it cumbersome for users to select text .	In this paper , we introduce such a paradigm ,	1-23	49-76	Natural touch interfaces , common now in devices such as tablets and smartphones , make it cumbersome for users to select text .	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	1>2	bg-compare	bg-compare
P14-1143	1-23	24-33	Natural touch interfaces , common now in devices such as tablets and smartphones , make it cumbersome for users to select text .	There is a need for a new text selection paradigm	1-23	24-48	Natural touch interfaces , common now in devices such as tablets and smartphones , make it cumbersome for users to select text .	There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades .	1<2	elab-addition	elab-addition
P14-1143	24-33	34-40	There is a need for a new text selection paradigm	that goes beyond the high acuity selection-by-mouse	24-48	24-48	There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades .	There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades .	1<2	elab-addition	elab-addition
P14-1143	34-40	41-48	that goes beyond the high acuity selection-by-mouse	that we have relied on for decades .	24-48	24-48	There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades .	There is a need for a new text selection paradigm that goes beyond the high acuity selection-by-mouse that we have relied on for decades .	1<2	elab-addition	elab-addition
P14-1143	49-58	59-62	In this paper , we introduce such a paradigm ,	called Smart Selection ,	49-76	49-76	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	1<2	elab-addition	elab-addition
P14-1143	49-58	63-76	In this paper , we introduce such a paradigm ,	which aims to recover a user's intended text selection from her touch input .	49-76	49-76	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	1<2	elab-addition	elab-addition
P14-1143	49-58	77-80	In this paper , we introduce such a paradigm ,	We model the problem	49-76	77-104	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	1<2	elab-addition	elab-addition
P14-1143	77-80	81-86	We model the problem	using an ensemble learning approach ,	77-104	77-104	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	1<2	manner-means	manner-means
P14-1143	81-86	87-92	using an ensemble learning approach ,	which leverages multiple linguistic analysis techniques	77-104	77-104	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	1<2	elab-addition	elab-addition
P14-1143	87-92	93-104	which leverages multiple linguistic analysis techniques	combined with information from a knowledge base and a Web graph .	77-104	77-104	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	We model the problem using an ensemble learning approach , which leverages multiple linguistic analysis techniques combined with information from a knowledge base and a Web graph .	1<2	elab-addition	elab-addition
P14-1143	49-58	105-117	In this paper , we introduce such a paradigm ,	We collect a dataset of true intended user selections and simulated user touches	49-76	105-131	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task , which we release to the academic community .	1<2	elab-addition	elab-addition
P14-1143	105-117	118-123	We collect a dataset of true intended user selections and simulated user touches	via a large-scale crowdsourcing task ,	105-131	105-131	We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task , which we release to the academic community .	We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task , which we release to the academic community .	1<2	manner-means	manner-means
P14-1143	118-123	124-131	via a large-scale crowdsourcing task ,	which we release to the academic community .	105-131	105-131	We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task , which we release to the academic community .	We collect a dataset of true intended user selections and simulated user touches via a large-scale crowdsourcing task , which we release to the academic community .	1<2	elab-addition	elab-addition
P14-1143	132-133	134-142	We show	that our model effectively addresses the smart selection task	132-153	132-153	We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques .	We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques .	1>2	attribution	attribution
P14-1143	49-58	134-142	In this paper , we introduce such a paradigm ,	that our model effectively addresses the smart selection task	49-76	132-153	In this paper , we introduce such a paradigm , called Smart Selection , which aims to recover a user's intended text selection from her touch input .	We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques .	1<2	evaluation	evaluation
P14-1143	134-142	143-153	that our model effectively addresses the smart selection task	and significantly outperforms various baselines and standalone linguistic analysis techniques .	132-153	132-153	We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques .	We show that our model effectively addresses the smart selection task and significantly outperforms various baselines and standalone linguistic analysis techniques .	1<2	joint	joint
P14-1144	1-7	52-62	Recently , researchers have begun exploring methods	We present a new annotated corpus of essay-level prompt adherence scores	1-29	52-76	Recently , researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and prompt adherence .	We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension .	1>2	bg-goal	bg-goal
P14-1144	1-7	8-29	Recently , researchers have begun exploring methods	of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and prompt adherence .	1-29	1-29	Recently , researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and prompt adherence .	Recently , researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and prompt adherence .	1<2	elab-addition	elab-addition
P14-1144	1-7	30-51	Recently , researchers have begun exploring methods	The work on modeling prompt adherence , however , has been focused mainly on whether individual sentences adhere to the prompt .	1-29	30-51	Recently , researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and prompt adherence .	The work on modeling prompt adherence , however , has been focused mainly on whether individual sentences adhere to the prompt .	1<2	elab-addition	elab-addition
P14-1144	52-62	63-76	We present a new annotated corpus of essay-level prompt adherence scores	and propose a feature-rich approach to scoring essays along the prompt adherence dimension .	52-76	52-76	We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension .	We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension .	1<2	joint	joint
P14-1144	52-62	77-87	We present a new annotated corpus of essay-level prompt adherence scores	Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system	52-76	77-95	We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension .	Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6 % .	1<2	evaluation	evaluation
P14-1144	77-87	88-95	Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system	yielding improvements of up to 16.6 % .	77-95	77-95	Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6 % .	Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6 % .	1<2	elab-addition	elab-addition
P14-1145	1-17	18-31	We introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses .	We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields ,	1-17	18-41	We introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses .	We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields , and present a loopy belief propagation algorithm for inference .	1<2	elab-addition	elab-addition
P14-1145	18-31	32-41	We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields ,	and present a loopy belief propagation algorithm for inference .	18-41	18-41	We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields , and present a loopy belief propagation algorithm for inference .	We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields , and present a loopy belief propagation algorithm for inference .	1<2	joint	joint
P14-1145	1-17	42-55	We introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses .	The key aspect of our method is that it is the first unified approach	1-17	42-76	We introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses .	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	1<2	elab-addition	elab-addition
P14-1145	42-55	56-66	The key aspect of our method is that it is the first unified approach	that assigns the polarity of both word- and sense-level connotations ,	42-76	42-76	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	1<2	elab-addition	elab-addition
P14-1145	42-55	67-72	The key aspect of our method is that it is the first unified approach	exploiting the innate bipartite graph structure	42-76	42-76	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	1<2	elab-addition	elab-addition
P14-1145	67-72	73-76	exploiting the innate bipartite graph structure	encoded in WordNet .	42-76	42-76	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations , exploiting the innate bipartite graph structure encoded in WordNet .	1<2	elab-addition	elab-addition
P14-1145	1-17	77-80	We introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses .	We present comprehensive evaluation	1-17	77-99	We introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses .	We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons .	1<2	evaluation	evaluation
P14-1145	77-80	81-90	We present comprehensive evaluation	to demonstrate the quality and utility of the resulting lexicon	77-99	77-99	We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons .	We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons .	1<2	enablement	enablement
P14-1145	81-90	91-99	to demonstrate the quality and utility of the resulting lexicon	in comparison to existing connotation and sentiment lexicons .	77-99	77-99	We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons .	We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons .	1<2	comparison	comparison
P14-1146	1-4	5-16	We present a method	that learns word embedding for Twitter sentiment classification in this paper .	1-16	1-16	We present a method that learns word embedding for Twitter sentiment classification in this paper .	We present a method that learns word embedding for Twitter sentiment classification in this paper .	1<2	elab-addition	elab-addition
P14-1146	17-32	33-39	Most existing algorithms for learning continuous word representations typically only model the syntactic context of words	but ignore the sentiment of text .	17-39	17-39	Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text .	Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text .	1>2	contrast	contrast
P14-1146	1-4	33-39	We present a method	but ignore the sentiment of text .	1-16	17-39	We present a method that learns word embedding for Twitter sentiment classification in this paper .	Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text .	1<2	bg-compare	bg-compare
P14-1146	33-39	40-45	but ignore the sentiment of text .	This is problematic for sentiment analysis	17-39	40-70	Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text .	This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity , such as good and bad , to neighboring word vectors .	1<2	elab-addition	elab-addition
P14-1146	40-45	46-70	This is problematic for sentiment analysis	as they usually map words with similar syntactic context but opposite sentiment polarity , such as good and bad , to neighboring word vectors .	40-70	40-70	This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity , such as good and bad , to neighboring word vectors .	This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity , such as good and bad , to neighboring word vectors .	1<2	exp-reason	exp-reason
P14-1146	1-4	71-74	We present a method	We address this issue	1-16	71-94	We present a method that learns word embedding for Twitter sentiment classification in this paper .	We address this issue by learning sentiment-specific word embedding ( SSWE ) , which encodes sentiment information in the continuous representation of words .	1<2	elab-addition	elab-addition
P14-1146	71-74	75-83	We address this issue	by learning sentiment-specific word embedding ( SSWE ) ,	71-94	71-94	We address this issue by learning sentiment-specific word embedding ( SSWE ) , which encodes sentiment information in the continuous representation of words .	We address this issue by learning sentiment-specific word embedding ( SSWE ) , which encodes sentiment information in the continuous representation of words .	1<2	manner-means	manner-means
P14-1146	75-83	84-94	by learning sentiment-specific word embedding ( SSWE ) ,	which encodes sentiment information in the continuous representation of words .	71-94	71-94	We address this issue by learning sentiment-specific word embedding ( SSWE ) , which encodes sentiment information in the continuous representation of words .	We address this issue by learning sentiment-specific word embedding ( SSWE ) , which encodes sentiment information in the continuous representation of words .	1<2	elab-addition	elab-addition
P14-1146	71-74	95-101	We address this issue	Specifically , we develop three neural networks	71-94	95-122	We address this issue by learning sentiment-specific word embedding ( SSWE ) , which encodes sentiment information in the continuous representation of words .	Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g. sentences or tweets ) in their loss functions .	1<2	elab-addition	elab-addition
P14-1146	95-101	102-111,118-122	Specifically , we develop three neural networks	to effectively incorporate the supervision from sentiment polarity of text <*> in their loss functions .	95-122	95-122	Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g. sentences or tweets ) in their loss functions .	Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g. sentences or tweets ) in their loss functions .	1<2	enablement	enablement
P14-1146	102-111,118-122	112-117	to effectively incorporate the supervision from sentiment polarity of text <*> in their loss functions .	( e.g. sentences or tweets )	95-122	95-122	Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g. sentences or tweets ) in their loss functions .	Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g. sentences or tweets ) in their loss functions .	1<2	elab-example	elab-example
P14-1146	123-129	130-139	To obtain large scale training corpora ,	we learn the sentiment-specific word embedding from massive distant-supervised tweets	123-146	123-146	To obtain large scale training corpora , we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons .	To obtain large scale training corpora , we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons .	1>2	enablement	enablement
P14-1146	1-4	130-139	We present a method	we learn the sentiment-specific word embedding from massive distant-supervised tweets	1-16	123-146	We present a method that learns word embedding for Twitter sentiment classification in this paper .	To obtain large scale training corpora , we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons .	1<2	elab-addition	elab-addition
P14-1146	130-139	140-146	we learn the sentiment-specific word embedding from massive distant-supervised tweets	collected by positive and negative emoticons .	123-146	123-146	To obtain large scale training corpora , we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons .	To obtain large scale training corpora , we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons .	1<2	elab-addition	elab-addition
P14-1146	147-161	162-177	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show	that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system	147-194	147-194	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	1>2	attribution	attribution
P14-1146	1-4	162-177	We present a method	that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system	1-16	147-194	We present a method that learns word embedding for Twitter sentiment classification in this paper .	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	1<2	evaluation	evaluation
P14-1146	162-177	178-186	that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system	; ( 2 ) the performance is further improved	147-194	147-194	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	1<2	joint	joint
P14-1146	178-186	187-194	; ( 2 ) the performance is further improved	by concatenating SSWE with existing feature set .	147-194	147-194	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that ( 1 ) the SSWE feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating SSWE with existing feature set .	1<2	manner-means	manner-means
P14-1147	1-11	43-50	Consumers' purchase decisions are increasingly influenced by user-generated online reviews .	In this paper , we explore generalized approaches	1-11	43-110	Consumers' purchase decisions are increasingly influenced by user-generated online reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1>2	bg-goal	bg-goal
P14-1147	1-11	12-21	Consumers' purchase decisions are increasingly influenced by user-generated online reviews .	Accordingly , there has been growing concern about the potential	1-11	12-42	Consumers' purchase decisions are increasingly influenced by user-generated online reviews .	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	1<2	result	result
P14-1147	12-21	22-28	Accordingly , there has been growing concern about the potential	for posting deceptive opinion spam fictitious reviews	12-42	12-42	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	1<2	elab-addition	elab-addition
P14-1147	22-28	29-33	for posting deceptive opinion spam fictitious reviews	that have been deliberately written	12-42	12-42	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	1<2	elab-addition	elab-addition
P14-1147	29-33	34-37	that have been deliberately written	to sound authentic ,	12-42	12-42	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	1<2	enablement	enablement
P14-1147	34-37	38-42	to sound authentic ,	to deceive the reader .	12-42	12-42	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	Accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the reader .	1<2	joint	joint
P14-1147	43-50	51-56	In this paper , we explore generalized approaches	for identifying online deceptive opinion spam	43-110	43-110	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1<2	elab-addition	elab-addition
P14-1147	43-50	57-64	In this paper , we explore generalized approaches	based on a new gold standard dataset ,	43-110	43-110	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1<2	bg-general	bg-general
P14-1147	57-64	65-73	based on a new gold standard dataset ,	which is comprised of data from three different domains	43-110	43-110	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1<2	elab-addition	elab-addition
P14-1147	65-73	74-82	which is comprised of data from three different domains	( i.e. Hotel , Restaurant , Doctor ) ,	43-110	43-110	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1<2	elab-enumember	elab-enumember
P14-1147	65-73	83-91	which is comprised of data from three different domains	each of which contains three types of reviews ,	43-110	43-110	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1<2	elab-addition	elab-addition
P14-1147	83-91	92-110	each of which contains three types of reviews ,	i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	43-110	43-110	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	1<2	elab-enumember	elab-enumember
P14-1147	43-50	111-127	In this paper , we explore generalized approaches	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews ,	43-110	111-156	In this paper , we explore generalized approaches for identifying online deceptive opinion spam based on a new gold standard dataset , which is comprised of data from three different domains ( i.e. Hotel , Restaurant , Doctor ) , each of which contains three types of reviews , i.e. customer generated truthful reviews , Turker generated deceptive reviews and employee ( domain-expert ) generated deceptive reviews .	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews , which we hope will help customers when making purchase decisions and review portal operators , such as TripAdvisor or Yelp , investigate possible fraudulent activity on their sites .	1<2	evaluation	evaluation
P14-1147	111-127	128-133,149-156	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews ,	which we hope will help customers <*> investigate possible fraudulent activity on their sites .	111-156	111-156	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews , which we hope will help customers when making purchase decisions and review portal operators , such as TripAdvisor or Yelp , investigate possible fraudulent activity on their sites .	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews , which we hope will help customers when making purchase decisions and review portal operators , such as TripAdvisor or Yelp , investigate possible fraudulent activity on their sites .	1<2	elab-addition	elab-addition
P14-1147	128-133,149-156	134-148	which we hope will help customers <*> investigate possible fraudulent activity on their sites .	when making purchase decisions and review portal operators , such as TripAdvisor or Yelp ,	111-156	111-156	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews , which we hope will help customers when making purchase decisions and review portal operators , such as TripAdvisor or Yelp , investigate possible fraudulent activity on their sites .	Our approach tries to capture the general difference of language usage between deceptive and truthful reviews , which we hope will help customers when making purchase decisions and review portal operators , such as TripAdvisor or Yelp , investigate possible fraudulent activity on their sites .	1<2	temporal	temporal
P16-1147	1-10	36-40	Traditional syntax models typically leverage part-of-speech ( POS ) information	We propose a simple method	1-17	36-54	Traditional syntax models typically leverage part-of-speech ( POS ) information by constructing features from hand-tuned templates .	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	1>2	bg-compare	bg-compare
P16-1147	1-10	11-17	Traditional syntax models typically leverage part-of-speech ( POS ) information	by constructing features from hand-tuned templates .	1-17	1-17	Traditional syntax models typically leverage part-of-speech ( POS ) information by constructing features from hand-tuned templates .	Traditional syntax models typically leverage part-of-speech ( POS ) information by constructing features from hand-tuned templates .	1<2	manner-means	manner-means
P16-1147	18-19	20-35	We demonstrate	that a better approach is to utilize POS tags as a regularizer of learned representations .	18-35	18-35	We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations .	We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations .	1>2	attribution	attribution
P16-1147	1-10	20-35	Traditional syntax models typically leverage part-of-speech ( POS ) information	that a better approach is to utilize POS tags as a regularizer of learned representations .	1-17	18-35	Traditional syntax models typically leverage part-of-speech ( POS ) information by constructing features from hand-tuned templates .	We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations .	1<2	contrast	contrast
P16-1147	36-40	41-47	We propose a simple method	for learning a stacked pipeline of models	36-54	36-54	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	1<2	elab-addition	elab-addition
P16-1147	41-47	48-54	for learning a stacked pipeline of models	which we call `` stack-propagation '' .	36-54	36-54	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	1<2	elab-addition	elab-addition
P16-1147	36-40	55-63	We propose a simple method	We apply this to dependency parsing and tagging ,	36-54	55-84	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	We apply this to dependency parsing and tagging , where we use the hidden layer of the tagger network as a representation of the input tokens for the parser .	1<2	elab-addition	elab-addition
P16-1147	55-63	64-84	We apply this to dependency parsing and tagging ,	where we use the hidden layer of the tagger network as a representation of the input tokens for the parser .	55-84	55-84	We apply this to dependency parsing and tagging , where we use the hidden layer of the tagger network as a representation of the input tokens for the parser .	We apply this to dependency parsing and tagging , where we use the hidden layer of the tagger network as a representation of the input tokens for the parser .	1<2	elab-addition	elab-addition
P16-1147	36-40	85-97	We propose a simple method	At test time , our parser does not require predicted POS tags .	36-54	85-97	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	At test time , our parser does not require predicted POS tags .	1<2	elab-addition	elab-addition
P16-1147	36-40	98-120	We propose a simple method	On 19 languages from the Universal Dependencies , our method is 1.3 % ( absolute ) more accurate than a state-of-the-art graph-based approach	36-54	98-132	We propose a simple method for learning a stacked pipeline of models which we call `` stack-propagation '' .	On 19 languages from the Universal Dependencies , our method is 1.3 % ( absolute ) more accurate than a state-of-the-art graph-based approach and 2.7 % more accurate than the most comparable greedy model .	1<2	evaluation	evaluation
P16-1147	98-120	121-132	On 19 languages from the Universal Dependencies , our method is 1.3 % ( absolute ) more accurate than a state-of-the-art graph-based approach	and 2.7 % more accurate than the most comparable greedy model .	98-132	98-132	On 19 languages from the Universal Dependencies , our method is 1.3 % ( absolute ) more accurate than a state-of-the-art graph-based approach and 2.7 % more accurate than the most comparable greedy model .	On 19 languages from the Universal Dependencies , our method is 1.3 % ( absolute ) more accurate than a state-of-the-art graph-based approach and 2.7 % more accurate than the most comparable greedy model .	1<2	joint	joint
P16-1148	1-7	8-17	We examine communications in a social network	to study user emotional contrast - the propensity of users	1-28	1-28	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	1<2	enablement	enablement
P16-1148	8-17	18-23	to study user emotional contrast - the propensity of users	to express different emotions than those	1-28	1-28	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	1<2	elab-addition	elab-addition
P16-1148	18-23	24-28	to express different emotions than those	expressed by their neighbors .	1-28	1-28	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	1<2	elab-addition	elab-addition
P16-1148	1-7	29-38	We examine communications in a social network	Our analysis is based on a large Twitter dataset ,	1-28	29-51	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	Our analysis is based on a large Twitter dataset , consisting of the tweets of 123,513 users from the USA and Canada .	1<2	elab-addition	elab-addition
P16-1148	29-38	39-51	Our analysis is based on a large Twitter dataset ,	consisting of the tweets of 123,513 users from the USA and Canada .	29-51	29-51	Our analysis is based on a large Twitter dataset , consisting of the tweets of 123,513 users from the USA and Canada .	Our analysis is based on a large Twitter dataset , consisting of the tweets of 123,513 users from the USA and Canada .	1<2	elab-addition	elab-addition
P16-1148	52-58	59-65	Focusing on Ekman 's basic emotions ,	we analyze differences between the emotional tone	52-85	52-85	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	1>2	elab-addition	elab-addition
P16-1148	1-7	59-65	We examine communications in a social network	we analyze differences between the emotional tone	1-28	52-85	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	1<2	elab-addition	elab-addition
P16-1148	59-65	66-76	we analyze differences between the emotional tone	expressed by these users and their neighbors of different types ,	52-85	52-85	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	1<2	elab-addition	elab-addition
P16-1148	59-65	77-85	we analyze differences between the emotional tone	and correlate these differences with perceived user demographics .	52-85	52-85	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	1<2	joint	joint
P16-1148	86-87	88-103	We demonstrate	that many perceived demographic traits correlate with the emotional contrast between users and their neighbors .	86-103	86-103	We demonstrate that many perceived demographic traits correlate with the emotional contrast between users and their neighbors .	We demonstrate that many perceived demographic traits correlate with the emotional contrast between users and their neighbors .	1>2	attribution	attribution
P16-1148	77-85	88-103	and correlate these differences with perceived user demographics .	that many perceived demographic traits correlate with the emotional contrast between users and their neighbors .	52-85	86-103	Focusing on Ekman 's basic emotions , we analyze differences between the emotional tone expressed by these users and their neighbors of different types , and correlate these differences with perceived user demographics .	We demonstrate that many perceived demographic traits correlate with the emotional contrast between users and their neighbors .	1<2	elab-addition	elab-addition
P16-1148	104-106	125-137	Unlike other approaches	that it is possible to accurately predict a range of perceived demographic traits	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1>2	comparison	comparison
P16-1148	104-106	107-110	Unlike other approaches	on inferring user attributes	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1<2	elab-addition	elab-addition
P16-1148	107-110	111-117	on inferring user attributes	that rely solely on user communications ,	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1<2	elab-addition	elab-addition
P16-1148	118-122	125-137	we explore the network structure	that it is possible to accurately predict a range of perceived demographic traits	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1>2	progression	progression
P16-1148	123-124	125-137	and show	that it is possible to accurately predict a range of perceived demographic traits	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1>2	attribution	attribution
P16-1148	1-7	125-137	We examine communications in a social network	that it is possible to accurately predict a range of perceived demographic traits	1-28	104-149	We examine communications in a social network to study user emotional contrast - the propensity of users to express different emotions than those expressed by their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1<2	elab-addition	elab-addition
P16-1148	125-137	138-142	that it is possible to accurately predict a range of perceived demographic traits	based solely on the emotions	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1<2	bg-general	bg-general
P16-1148	138-142	143-149	based solely on the emotions	emanating from users and their neighbors .	104-149	104-149	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	Unlike other approaches on inferring user attributes that rely solely on user communications , we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors .	1<2	elab-addition	elab-addition
P16-1149	1-7	13-17,25-36	State legislatures often rely on existing text	Resource and expertise constraints , <*> can be taken advantage of by lobbyists and special interest groups .	1-12	13-36	State legislatures often rely on existing text when drafting new bills .	Resource and expertise constraints , which often drive this copying behavior , can be taken advantage of by lobbyists and special interest groups .	1>2	elab-addition	elab-addition
P16-1149	1-7	8-12	State legislatures often rely on existing text	when drafting new bills .	1-12	1-12	State legislatures often rely on existing text when drafting new bills .	State legislatures often rely on existing text when drafting new bills .	1<2	temporal	temporal
P16-1149	13-17,25-36	37-42	Resource and expertise constraints , <*> can be taken advantage of by lobbyists and special interest groups .	These groups provide model bills ,	13-36	37-57	Resource and expertise constraints , which often drive this copying behavior , can be taken advantage of by lobbyists and special interest groups .	These groups provide model bills , which encode policy agendas , with the intent that the models become actual law .	1>2	elab-addition	elab-addition
P16-1149	13-17,25-36	18-24	Resource and expertise constraints , <*> can be taken advantage of by lobbyists and special interest groups .	which often drive this copying behavior ,	13-36	13-36	Resource and expertise constraints , which often drive this copying behavior , can be taken advantage of by lobbyists and special interest groups .	Resource and expertise constraints , which often drive this copying behavior , can be taken advantage of by lobbyists and special interest groups .	1<2	elab-addition	elab-addition
P16-1149	37-42	58-72	These groups provide model bills ,	Unfortunately , model legislation is often opaque to the public-both in source and content .	37-57	58-72	These groups provide model bills , which encode policy agendas , with the intent that the models become actual law .	Unfortunately , model legislation is often opaque to the public-both in source and content .	1>2	contrast	contrast
P16-1149	37-42	43-50	These groups provide model bills ,	which encode policy agendas , with the intent	37-57	37-57	These groups provide model bills , which encode policy agendas , with the intent that the models become actual law .	These groups provide model bills , which encode policy agendas , with the intent that the models become actual law .	1<2	elab-addition	elab-addition
P16-1149	43-50	51-57	which encode policy agendas , with the intent	that the models become actual law .	37-57	37-57	These groups provide model bills , which encode policy agendas , with the intent that the models become actual law .	These groups provide model bills , which encode policy agendas , with the intent that the models become actual law .	1<2	elab-addition	elab-addition
P16-1149	58-72	73-81	Unfortunately , model legislation is often opaque to the public-both in source and content .	In this paper we present LOBBYBACK , a system	58-72	73-90	Unfortunately , model legislation is often opaque to the public-both in source and content .	In this paper we present LOBBYBACK , a system that reverse engineers model legislation from observed text .	1>2	bg-compare	bg-compare
P16-1149	73-81	82-90	In this paper we present LOBBYBACK , a system	that reverse engineers model legislation from observed text .	73-90	73-90	In this paper we present LOBBYBACK , a system that reverse engineers model legislation from observed text .	In this paper we present LOBBYBACK , a system that reverse engineers model legislation from observed text .	1<2	elab-addition	elab-addition
P16-1149	73-81	91-95	In this paper we present LOBBYBACK , a system	LOBBYBACK identifies clusters of bills	73-90	91-117	In this paper we present LOBBYBACK , a system that reverse engineers model legislation from observed text .	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	1<2	elab-addition	elab-addition
P16-1149	91-95	96-99	LOBBYBACK identifies clusters of bills	which have text reuse	91-117	91-117	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	1<2	elab-addition	elab-addition
P16-1149	91-95	100-104	LOBBYBACK identifies clusters of bills	and generates `` prototypes ''	91-117	91-117	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	1<2	joint	joint
P16-1149	100-104	105-112	and generates `` prototypes ''	that represent a canonical version of the text	91-117	91-117	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	1<2	elab-addition	elab-addition
P16-1149	105-112	113-117	that represent a canonical version of the text	shared between the documents .	91-117	91-117	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	LOBBYBACK identifies clusters of bills which have text reuse and generates `` prototypes '' that represent a canonical version of the text shared between the documents .	1<2	elab-addition	elab-addition
P16-1149	118-119	120-125	We demonstrate	that LOBBYBACK accurately reconstructs model legislation	118-136	118-136	We demonstrate that LOBBYBACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills .	We demonstrate that LOBBYBACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills .	1>2	attribution	attribution
P16-1149	73-81	120-125	In this paper we present LOBBYBACK , a system	that LOBBYBACK accurately reconstructs model legislation	73-90	118-136	In this paper we present LOBBYBACK , a system that reverse engineers model legislation from observed text .	We demonstrate that LOBBYBACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills .	1<2	evaluation	evaluation
P16-1149	120-125	126-136	that LOBBYBACK accurately reconstructs model legislation	and apply it to a dataset of over 550k bills .	118-136	118-136	We demonstrate that LOBBYBACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills .	We demonstrate that LOBBYBACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills .	1<2	joint	joint
P16-1150	1-11	12-25	We propose a new task in the field of computational argumentation	in which we investigate qualitative properties of Web arguments , namely their convincingness .	1-25	1-25	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	1<2	elab-addition	elab-addition
P16-1150	1-11	26-33	We propose a new task in the field of computational argumentation	We cast the problem as relation classification ,	1-25	26-49	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	We cast the problem as relation classification , where a pair of arguments having the same stance to the same prompt is judged .	1<2	elab-addition	elab-addition
P16-1150	26-33	34-38,47-49	We cast the problem as relation classification ,	where a pair of arguments <*> is judged .	26-49	26-49	We cast the problem as relation classification , where a pair of arguments having the same stance to the same prompt is judged .	We cast the problem as relation classification , where a pair of arguments having the same stance to the same prompt is judged .	1<2	elab-addition	elab-addition
P16-1150	34-38,47-49	39-46	where a pair of arguments <*> is judged .	having the same stance to the same prompt	26-49	26-49	We cast the problem as relation classification , where a pair of arguments having the same stance to the same prompt is judged .	We cast the problem as relation classification , where a pair of arguments having the same stance to the same prompt is judged .	1<2	elab-addition	elab-addition
P16-1150	50-62	65-81	We annotate a large datasets of 16k pairs of arguments over 32 topics	whether the relation `` A is more convincing than B '' exhibits properties of total ordering ;	50-94	50-94	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	1>2	progression	progression
P16-1150	63-64	65-81	and investigate	whether the relation `` A is more convincing than B '' exhibits properties of total ordering ;	50-94	50-94	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	1>2	attribution	attribution
P16-1150	1-11	65-81	We propose a new task in the field of computational argumentation	whether the relation `` A is more convincing than B '' exhibits properties of total ordering ;	1-25	50-94	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	1<2	elab-addition	elab-addition
P16-1150	65-81	82-88	whether the relation `` A is more convincing than B '' exhibits properties of total ordering ;	these findings are used as global constraints	50-94	50-94	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	1<2	elab-addition	elab-addition
P16-1150	82-88	89-94	these findings are used as global constraints	for cleaning the crowdsourced data .	50-94	50-94	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	We annotate a large datasets of 16k pairs of arguments over 32 topics and investigate whether the relation `` A is more convincing than B '' exhibits properties of total ordering ; these findings are used as global constraints for cleaning the crowdsourced data .	1<2	elab-addition	elab-addition
P16-1150	1-11	95-99	We propose a new task in the field of computational argumentation	We propose two tasks :	1-25	95-127	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	1<2	elab-addition	elab-addition
P16-1150	100-103	104-112	( 1 ) predicting	which argument from an argument pair is more convincing	95-127	95-127	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	1>2	attribution	attribution
P16-1150	95-99	104-112	We propose two tasks :	which argument from an argument pair is more convincing	95-127	95-127	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	1<2	elab-enumember	elab-enumember
P16-1150	95-99	113-122	We propose two tasks :	and ( 2 ) ranking all arguments to the topic	95-127	95-127	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	1<2	elab-enumember	elab-enumember
P16-1150	113-122	123-127	and ( 2 ) ranking all arguments to the topic	based on their convincingness .	95-127	95-127	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	We propose two tasks : ( 1 ) predicting which argument from an argument pair is more convincing and ( 2 ) ranking all arguments to the topic based on their convincingness .	1<2	bg-general	bg-general
P16-1150	128-135	136-149	We experiment with feature-rich SVM and bidirectional LSTM	and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman 's correlation in a cross-topic evaluation .	128-149	128-149	We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman 's correlation in a cross-topic evaluation .	We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman 's correlation in a cross-topic evaluation .	1>2	progression	progression
P16-1150	1-11	136-149	We propose a new task in the field of computational argumentation	and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman 's correlation in a cross-topic evaluation .	1-25	128-149	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spearman 's correlation in a cross-topic evaluation .	1<2	evaluation	evaluation
P16-1150	1-11	150-164	We propose a new task in the field of computational argumentation	We release the newly created corpus UKPConvArg1 and the experimental software under open licenses .	1-25	150-164	We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments , namely their convincingness .	We release the newly created corpus UKPConvArg1 and the experimental software under open licenses .	1<2	elab-addition	elab-addition
P16-1151	1-8	9-23	An extensive literature in computational social science examines	how features of messages , advertisements , and other corpora affect individuals ' decisions ,	1-38	1-38	An extensive literature in computational social science examines how features of messages , advertisements , and other corpora affect individuals ' decisions , but these analyses must specify the relevant features of the text before the experiment .	An extensive literature in computational social science examines how features of messages , advertisements , and other corpora affect individuals ' decisions , but these analyses must specify the relevant features of the text before the experiment .	1>2	attribution	attribution
P16-1151	9-23	24-38	how features of messages , advertisements , and other corpora affect individuals ' decisions ,	but these analyses must specify the relevant features of the text before the experiment .	1-38	1-38	An extensive literature in computational social science examines how features of messages , advertisements , and other corpora affect individuals ' decisions , but these analyses must specify the relevant features of the text before the experiment .	An extensive literature in computational social science examines how features of messages , advertisements , and other corpora affect individuals ' decisions , but these analyses must specify the relevant features of the text before the experiment .	1>2	contrast	contrast
P16-1151	24-38	71-79	but these analyses must specify the relevant features of the text before the experiment .	We introduce a new experimental design and statistical model	1-38	71-95	An extensive literature in computational social science examines how features of messages , advertisements , and other corpora affect individuals ' decisions , but these analyses must specify the relevant features of the text before the experiment .	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	1>2	bg-compare	bg-compare
P16-1151	39-50	51-56	Automated text analysis methods are able to discover features of text ,	but these methods cannot be used	39-70	39-70	Automated text analysis methods are able to discover features of text , but these methods cannot be used to obtain the estimates of causal effects-the quantity of interest for applied researchers .	Automated text analysis methods are able to discover features of text , but these methods cannot be used to obtain the estimates of causal effects-the quantity of interest for applied researchers .	1>2	contrast	contrast
P16-1151	24-38	51-56	but these analyses must specify the relevant features of the text before the experiment .	but these methods cannot be used	1-38	39-70	An extensive literature in computational social science examines how features of messages , advertisements , and other corpora affect individuals ' decisions , but these analyses must specify the relevant features of the text before the experiment .	Automated text analysis methods are able to discover features of text , but these methods cannot be used to obtain the estimates of causal effects-the quantity of interest for applied researchers .	1<2	elab-addition	elab-addition
P16-1151	51-56	57-70	but these methods cannot be used	to obtain the estimates of causal effects-the quantity of interest for applied researchers .	39-70	39-70	Automated text analysis methods are able to discover features of text , but these methods cannot be used to obtain the estimates of causal effects-the quantity of interest for applied researchers .	Automated text analysis methods are able to discover features of text , but these methods cannot be used to obtain the estimates of causal effects-the quantity of interest for applied researchers .	1<2	enablement	enablement
P16-1151	71-79	80-86	We introduce a new experimental design and statistical model	to simultaneously discover treatments in a corpora	71-95	71-95	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	1<2	enablement	enablement
P16-1151	80-86	87-95	to simultaneously discover treatments in a corpora	and estimate causal effects for these discovered treatments .	71-95	71-95	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	1<2	joint	joint
P16-1151	71-79	96-99	We introduce a new experimental design and statistical model	We prove the conditions	71-95	96-118	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	1<2	elab-addition	elab-addition
P16-1151	96-99	100-106	We prove the conditions	to identify the treatment effects of texts	96-118	96-118	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	1<2	enablement	enablement
P16-1151	96-99	107-113	We prove the conditions	and introduce the supervised Indian Buffet process	96-118	96-118	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	1<2	joint	joint
P16-1151	107-113	114-118	and introduce the supervised Indian Buffet process	to discover those treatments .	96-118	96-118	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	We prove the conditions to identify the treatment effects of texts and introduce the supervised Indian Buffet process to discover those treatments .	1<2	enablement	enablement
P16-1151	71-79	119-129	We introduce a new experimental design and statistical model	Our method enables us to discover treatments in a training set	71-95	119-161	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	Our method enables us to discover treatments in a training set using a collection of texts and individuals ' responses to those texts , and then estimate the effects of these interventions in a test set of new texts and survey respondents .	1<2	elab-addition	elab-addition
P16-1151	119-129	130-142	Our method enables us to discover treatments in a training set	using a collection of texts and individuals ' responses to those texts ,	119-161	119-161	Our method enables us to discover treatments in a training set using a collection of texts and individuals ' responses to those texts , and then estimate the effects of these interventions in a test set of new texts and survey respondents .	Our method enables us to discover treatments in a training set using a collection of texts and individuals ' responses to those texts , and then estimate the effects of these interventions in a test set of new texts and survey respondents .	1<2	manner-means	manner-means
P16-1151	119-129	143-161	Our method enables us to discover treatments in a training set	and then estimate the effects of these interventions in a test set of new texts and survey respondents .	119-161	119-161	Our method enables us to discover treatments in a training set using a collection of texts and individuals ' responses to those texts , and then estimate the effects of these interventions in a test set of new texts and survey respondents .	Our method enables us to discover treatments in a training set using a collection of texts and individuals ' responses to those texts , and then estimate the effects of these interventions in a test set of new texts and survey respondents .	1<2	progression	progression
P16-1151	162-172	173-179	We apply the model to an experiment about candidate biographies ,	recovering intuitive features of voters ' decisions	162-192	162-192	We apply the model to an experiment about candidate biographies , recovering intuitive features of voters ' decisions and revealing a penalty for lawyers and a bonus for military service .	We apply the model to an experiment about candidate biographies , recovering intuitive features of voters ' decisions and revealing a penalty for lawyers and a bonus for military service .	1>2	result	result
P16-1151	71-79	173-179	We introduce a new experimental design and statistical model	recovering intuitive features of voters ' decisions	71-95	162-192	We introduce a new experimental design and statistical model to simultaneously discover treatments in a corpora and estimate causal effects for these discovered treatments .	We apply the model to an experiment about candidate biographies , recovering intuitive features of voters ' decisions and revealing a penalty for lawyers and a bonus for military service .	1<2	evaluation	evaluation
P16-1151	173-179	180-192	recovering intuitive features of voters ' decisions	and revealing a penalty for lawyers and a bonus for military service .	162-192	162-192	We apply the model to an experiment about candidate biographies , recovering intuitive features of voters ' decisions and revealing a penalty for lawyers and a bonus for military service .	We apply the model to an experiment about candidate biographies , recovering intuitive features of voters ' decisions and revealing a penalty for lawyers and a bonus for military service .	1<2	joint	joint
P16-1152	1-9	38-49	Structured prediction from bandit feedback describes a learning scenario	We present new learning objectives and algorithms for this interactive scenario ,	1-37	38-60	Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure , a learner only receives partial feedback in form of the loss value of a predicted structure .	We present new learning objectives and algorithms for this interactive scenario , focusing on convergence speed and ease of elicitability of feedback .	1>2	bg-goal	bg-goal
P16-1152	10-20	21-37	where instead of having access to a gold standard structure ,	a learner only receives partial feedback in form of the loss value of a predicted structure .	1-37	1-37	Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure , a learner only receives partial feedback in form of the loss value of a predicted structure .	Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure , a learner only receives partial feedback in form of the loss value of a predicted structure .	1>2	contrast	contrast
P16-1152	1-9	21-37	Structured prediction from bandit feedback describes a learning scenario	a learner only receives partial feedback in form of the loss value of a predicted structure .	1-37	1-37	Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure , a learner only receives partial feedback in form of the loss value of a predicted structure .	Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure , a learner only receives partial feedback in form of the loss value of a predicted structure .	1<2	elab-addition	elab-addition
P16-1152	38-49	50-60	We present new learning objectives and algorithms for this interactive scenario ,	focusing on convergence speed and ease of elicitability of feedback .	38-60	38-60	We present new learning objectives and algorithms for this interactive scenario , focusing on convergence speed and ease of elicitability of feedback .	We present new learning objectives and algorithms for this interactive scenario , focusing on convergence speed and ease of elicitability of feedback .	1<2	elab-addition	elab-addition
P16-1152	61-69	82-83,88-90	We present supervised-to-bandit simulation experiments for several NLP tasks	that bandit <*> eases feedback strength	61-96	61-96	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	1>2	result	result
P16-1152	61-69	70-80	We present supervised-to-bandit simulation experiments for several NLP tasks	( machine translation , sequence labeling , text classification ) ,	61-96	61-96	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	1<2	elab-enumember	elab-enumember
P16-1152	81	82-83,88-90	showing	that bandit <*> eases feedback strength	61-96	61-96	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	1>2	attribution	attribution
P16-1152	38-49	82-83,88-90	We present new learning objectives and algorithms for this interactive scenario ,	that bandit <*> eases feedback strength	38-60	61-96	We present new learning objectives and algorithms for this interactive scenario , focusing on convergence speed and ease of elicitability of feedback .	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	1<2	evaluation	evaluation
P16-1152	82-83,88-90	84-87	that bandit <*> eases feedback strength	learning from relative preferences	61-96	61-96	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	1<2	elab-addition	elab-addition
P16-1152	88-90	91-96	eases feedback strength	and yields improved empirical convergence .	61-96	61-96	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	We present supervised-to-bandit simulation experiments for several NLP tasks ( machine translation , sequence labeling , text classification ) , showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence .	1<2	joint	joint
P16-1153	1-13	14-20	This paper introduces a novel architecture for reinforcement learning with deep neural networks	designed to handle state and action spaces	1-31	1-31	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	1<2	elab-addition	elab-addition
P16-1153	14-20	21-25	designed to handle state and action spaces	characterized by natural language ,	1-31	1-31	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	1<2	elab-addition	elab-addition
P16-1153	21-25	26-31	characterized by natural language ,	as found in text-based games .	1-31	1-31	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	1<2	elab-addition	elab-addition
P16-1153	32-41	42-53	Termed a deep reinforcement relevance network ( DRRN ) ,	the architecture represents action and state spaces with separate embedding vectors ,	32-68	32-68	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	1>2	elab-addition	elab-addition
P16-1153	1-13	42-53	This paper introduces a novel architecture for reinforcement learning with deep neural networks	the architecture represents action and state spaces with separate embedding vectors ,	1-31	32-68	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	1<2	elab-addition	elab-addition
P16-1153	42-53	54-60	the architecture represents action and state spaces with separate embedding vectors ,	which are combined with an interaction function	32-68	32-68	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	1<2	elab-addition	elab-addition
P16-1153	54-60	61-68	which are combined with an interaction function	to approximate the Q-function in reinforcement learning .	32-68	32-68	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	Termed a deep reinforcement relevance network ( DRRN ) , the architecture represents action and state spaces with separate embedding vectors , which are combined with an interaction function to approximate the Q-function in reinforcement learning .	1<2	elab-addition	elab-addition
P16-1153	69-78	79-87	We evaluate the DRRN on two popular text games ,	showing superior performance over other deep Qlearning architectures .	69-87	69-87	We evaluate the DRRN on two popular text games , showing superior performance over other deep Qlearning architectures .	We evaluate the DRRN on two popular text games , showing superior performance over other deep Qlearning architectures .	1>2	result	result
P16-1153	1-13	79-87	This paper introduces a novel architecture for reinforcement learning with deep neural networks	showing superior performance over other deep Qlearning architectures .	1-31	69-87	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	We evaluate the DRRN on two popular text games , showing superior performance over other deep Qlearning architectures .	1<2	evaluation	evaluation
P16-1153	88-93	94-99	Experiments with paraphrased action descriptions show	that the model is extracting meaning	88-107	88-107	Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text .	Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text .	1>2	attribution	attribution
P16-1153	1-13	94-99	This paper introduces a novel architecture for reinforcement learning with deep neural networks	that the model is extracting meaning	1-31	88-107	This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language , as found in text-based games .	Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text .	1<2	evaluation	evaluation
P16-1153	94-99	100-107	that the model is extracting meaning	rather than simply memorizing strings of text .	88-107	88-107	Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text .	Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text .	1<2	contrast	contrast
P16-1154	1-11	93-97	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning	and propose a new model	1-32	81-103	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	1>2	bg-goal	bg-goal
P16-1154	1-11	12-16	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning	referred to as copying ,	1-32	1-32	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	1<2	elab-addition	elab-addition
P16-1154	12-16	17-32	referred to as copying ,	in which certain segments in the input sequence are selectively replicated in the output sequence .	1-32	1-32	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	1<2	elab-addition	elab-addition
P16-1154	12-16	33-42	referred to as copying ,	A similar phenomenon is observable in human language communication .	1-32	33-42	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	A similar phenomenon is observable in human language communication .	1<2	elab-addition	elab-addition
P16-1154	33-42	43-58	A similar phenomenon is observable in human language communication .	For example , humans tend to repeat entity names or even long phrases in conversation .	33-42	43-58	A similar phenomenon is observable in human language communication .	For example , humans tend to repeat entity names or even long phrases in conversation .	1<2	elab-example	elab-example
P16-1154	1-11	59-72	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning	The challenge with regard to copying in Seq2Seq is that new machinery is needed	1-32	59-80	We address an important problem in sequence-to-sequence ( Seq2Seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence .	The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation .	1<2	elab-addition	elab-addition
P16-1154	59-72	73-80	The challenge with regard to copying in Seq2Seq is that new machinery is needed	to decide when to perform the operation .	59-80	59-80	The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation .	The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation .	1<2	enablement	enablement
P16-1154	81-92	93-97	In this paper , we incorporate copying into neural networkbased Seq2Seq learning	and propose a new model	81-103	81-103	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	1>2	progression	progression
P16-1154	93-97	98-103	and propose a new model	called COPYNET with encoderdecoder structure .	81-103	81-103	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	1<2	elab-addition	elab-addition
P16-1154	93-97	104-121	and propose a new model	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism	81-103	104-140	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence .	1<2	elab-addition	elab-addition
P16-1154	104-121	122-129	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism	which can choose subsequences in the input sequence	104-140	104-140	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence .	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence .	1<2	elab-addition	elab-addition
P16-1154	122-129	130-140	which can choose subsequences in the input sequence	and put them at proper places in the output sequence .	104-140	104-140	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence .	COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence .	1<2	joint	joint
P16-1154	93-97	141-159	and propose a new model	Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET .	81-103	141-159	In this paper , we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure .	Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET .	1<2	evaluation	evaluation
P16-1154	141-159	160-176	Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET .	For example , COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks .	141-159	160-176	Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET .	For example , COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks .	1<2	exp-evidence	exp-evidence
P16-1155	1-15	32-37	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms	However , several applications encounter scenarios	1-31	32-62	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain .	However , several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations .	1>2	contrast	contrast
P16-1155	1-15	16-22	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms	for being dependent on annotated training data	1-31	1-31	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain .	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain .	1<2	elab-addition	elab-addition
P16-1155	16-22	23-31	for being dependent on annotated training data	for training new models for every new domain .	1-31	1-31	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain .	Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain .	1<2	elab-addition	elab-addition
P16-1155	32-37	63-81	However , several applications encounter scenarios	This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and disparate label sets .	32-62	63-81	However , several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations .	This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and disparate label sets .	1>2	bg-compare	bg-compare
P16-1155	32-37	38-44	However , several applications encounter scenarios	where models need to transfer/adapt across domains	32-62	32-62	However , several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations .	However , several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations .	1<2	elab-addition	elab-addition
P16-1155	38-44	45-62	where models need to transfer/adapt across domains	when the label sets vary both in terms of count of labels as well as their connotations .	32-62	32-62	However , several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations .	However , several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations .	1<2	condition	condition
P16-1155	63-81	82-91	This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and disparate label sets .	It starts with identifying transferable knowledge from across multiple domains	63-81	82-102	This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and disparate label sets .	It starts with identifying transferable knowledge from across multiple domains that can be useful for learning the target domain task .	1<2	elab-addition	elab-addition
P16-1155	82-91	92-102	It starts with identifying transferable knowledge from across multiple domains	that can be useful for learning the target domain task .	82-102	82-102	It starts with identifying transferable knowledge from across multiple domains that can be useful for learning the target domain task .	It starts with identifying transferable knowledge from across multiple domains that can be useful for learning the target domain task .	1<2	elab-addition	elab-addition
P16-1155	82-91	103-116	It starts with identifying transferable knowledge from across multiple domains	This knowledge in the form of selective labeled instances from different domains is congregated	82-102	103-132	It starts with identifying transferable knowledge from across multiple domains that can be useful for learning the target domain task .	This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task .	1<2	elab-addition	elab-addition
P16-1155	103-116	117-122	This knowledge in the form of selective labeled instances from different domains is congregated	to form an auxiliary training set	103-132	103-132	This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task .	This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task .	1<2	enablement	enablement
P16-1155	117-122	123-132	to form an auxiliary training set	which is used for learning the target domain task .	103-132	103-132	This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task .	This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task .	1<2	elab-addition	elab-addition
P16-1155	63-81	133-156	This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and disparate label sets .	Experimental results validate the efficacy of the proposed algorithm against strong baselines on a real world social media and the 20 Newsgroups datasets .	63-81	133-156	This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and disparate label sets .	Experimental results validate the efficacy of the proposed algorithm against strong baselines on a real world social media and the 20 Newsgroups datasets .	1<2	evaluation	evaluation
P16-1156	1-10	101-108	Languages with rich inflectional morphology exhibit lexical data sparsity ,	We present a latent variable Gaussian graphical model	1-26	101-136	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	1>2	bg-goal	bg-goal
P16-1156	1-10	11-13,20-26	Languages with rich inflectional morphology exhibit lexical data sparsity ,	since the word <*> will vary with the syntactic context .	1-26	1-26	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	1<2	exp-reason	exp-reason
P16-1156	11-13,20-26	14-19	since the word <*> will vary with the syntactic context .	used to express a given concept	1-26	1-26	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	1<2	elab-addition	elab-addition
P16-1156	1-10	27-37	Languages with rich inflectional morphology exhibit lexical data sparsity ,	For instance , each count noun in Czech has 12 forms	1-26	27-47	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	For instance , each count noun in Czech has 12 forms ( where English uses only singular and plural ) .	1<2	elab-example	elab-example
P16-1156	27-37	38-47	For instance , each count noun in Czech has 12 forms	( where English uses only singular and plural ) .	27-47	27-47	For instance , each count noun in Czech has 12 forms ( where English uses only singular and plural ) .	For instance , each count noun in Czech has 12 forms ( where English uses only singular and plural ) .	1<2	elab-addition	elab-addition
P16-1156	27-37	48-64	For instance , each count noun in Czech has 12 forms	Even in large corpora , we are unlikely to observe all inflections of a given lemma .	27-47	48-64	For instance , each count noun in Czech has 12 forms ( where English uses only singular and plural ) .	Even in large corpora , we are unlikely to observe all inflections of a given lemma .	1<2	elab-addition	elab-addition
P16-1156	1-10	65-71	Languages with rich inflectional morphology exhibit lexical data sparsity ,	This reduces the vocabulary coverage of methods	1-26	65-82	Languages with rich inflectional morphology exhibit lexical data sparsity , since the word used to express a given concept will vary with the syntactic context .	This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information .	1<2	elab-addition	elab-addition
P16-1156	65-71	72-82	This reduces the vocabulary coverage of methods	that induce continuous representations for words from distributional corpus information .	65-82	65-82	This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information .	This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information .	1<2	elab-addition	elab-addition
P16-1156	83-86	101-108	We solve this problem	We present a latent variable Gaussian graphical model	83-100	101-136	We solve this problem by exploiting existing morphological resources that can enumerate a word 's component morphemes .	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	1>2	elab-addition	elab-addition
P16-1156	83-86	87-91	We solve this problem	by exploiting existing morphological resources	83-100	83-100	We solve this problem by exploiting existing morphological resources that can enumerate a word 's component morphemes .	We solve this problem by exploiting existing morphological resources that can enumerate a word 's component morphemes .	1<2	manner-means	manner-means
P16-1156	87-91	92-100	by exploiting existing morphological resources	that can enumerate a word 's component morphemes .	83-100	83-100	We solve this problem by exploiting existing morphological resources that can enumerate a word 's component morphemes .	We solve this problem by exploiting existing morphological resources that can enumerate a word 's component morphemes .	1<2	elab-addition	elab-addition
P16-1156	101-108	109-117	We present a latent variable Gaussian graphical model	that allows us to extrapolate continuous representations for words	101-136	101-136	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	1<2	elab-addition	elab-addition
P16-1156	109-117	118-124	that allows us to extrapolate continuous representations for words	not observed in the training corpus ,	101-136	101-136	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	1<2	elab-addition	elab-addition
P16-1156	109-117	125-130	that allows us to extrapolate continuous representations for words	as well as smoothing the representations	101-136	101-136	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	1<2	joint	joint
P16-1156	125-130	131-136	as well as smoothing the representations	provided for the observed words .	101-136	101-136	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	1<2	elab-addition	elab-addition
P16-1156	101-108	137-144	We present a latent variable Gaussian graphical model	The latent variables represent embeddings of morphemes ,	101-136	137-152	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	The latent variables represent embeddings of morphemes , which combine to create embeddings of words .	1<2	elab-addition	elab-addition
P16-1156	137-144	145-152	The latent variables represent embeddings of morphemes ,	which combine to create embeddings of words .	137-152	137-152	The latent variables represent embeddings of morphemes , which combine to create embeddings of words .	The latent variables represent embeddings of morphemes , which combine to create embeddings of words .	1<2	elab-addition	elab-addition
P16-1156	101-108	153-167	We present a latent variable Gaussian graphical model	Over several languages and training sizes , our model improves the embeddings for words ,	101-136	153-182	We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus , as well as smoothing the representations provided for the observed words .	Over several languages and training sizes , our model improves the embeddings for words , when evaluated on an analogy task , skip-gram predictive accuracy , and word similarity .	1<2	evaluation	evaluation
P16-1156	153-167	168-182	Over several languages and training sizes , our model improves the embeddings for words ,	when evaluated on an analogy task , skip-gram predictive accuracy , and word similarity .	153-182	153-182	Over several languages and training sizes , our model improves the embeddings for words , when evaluated on an analogy task , skip-gram predictive accuracy , and word similarity .	Over several languages and training sizes , our model improves the embeddings for words , when evaluated on an analogy task , skip-gram predictive accuracy , and word similarity .	1<2	condition	condition
P16-1157	1-6	15-27	Despite interest in using cross-lingual knowledge	a systematic comparison of the possible approaches is lacking in the literature .	1-27	1-27	Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks , a systematic comparison of the possible approaches is lacking in the literature .	Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks , a systematic comparison of the possible approaches is lacking in the literature .	1>2	contrast	contrast
P16-1157	1-6	7-14	Despite interest in using cross-lingual knowledge	to learn word embeddings for various tasks ,	1-27	1-27	Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks , a systematic comparison of the possible approaches is lacking in the literature .	Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks , a systematic comparison of the possible approaches is lacking in the literature .	1<2	enablement	enablement
P16-1157	15-27	28-36	a systematic comparison of the possible approaches is lacking in the literature .	We perform an extensive evaluation of four popular approaches	1-27	28-56	Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks , a systematic comparison of the possible approaches is lacking in the literature .	We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs .	1>2	bg-goal	bg-goal
P16-1157	28-36	37-41	We perform an extensive evaluation of four popular approaches	of inducing cross-lingual embeddings ,	28-56	28-56	We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs .	We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs .	1<2	elab-addition	elab-addition
P16-1157	28-36	42-56	We perform an extensive evaluation of four popular approaches	each requiring a different form of supervision , on four typologically different language pairs .	28-56	28-56	We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs .	We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs .	1<2	elab-addition	elab-addition
P16-1157	57-64	86-87,93-97	Our evaluation setup spans four different tasks ,	that models <*> almost always perform better ,	57-83	84-108	Our evaluation setup spans four different tasks , including intrinsic evaluation on mono-lingual and cross-lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications .	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	1>2	result	result
P16-1157	57-64	65-83	Our evaluation setup spans four different tasks ,	including intrinsic evaluation on mono-lingual and cross-lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications .	57-83	57-83	Our evaluation setup spans four different tasks , including intrinsic evaluation on mono-lingual and cross-lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications .	Our evaluation setup spans four different tasks , including intrinsic evaluation on mono-lingual and cross-lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications .	1<2	elab-enumember	elab-enumember
P16-1157	84-85	86-87,93-97	We show	that models <*> almost always perform better ,	84-108	84-108	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	1>2	attribution	attribution
P16-1157	28-36	86-87,93-97	We perform an extensive evaluation of four popular approaches	that models <*> almost always perform better ,	28-56	84-108	We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings , each requiring a different form of supervision , on four typologically different language pairs .	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	1<2	evaluation	evaluation
P16-1157	86-87,93-97	88-92	that models <*> almost always perform better ,	which require expensive cross-lingual knowledge	84-108	84-108	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	1<2	elab-enumember	elab-enumember
P16-1157	86-87,93-97	98-108	that models <*> almost always perform better ,	but cheaply supervised models often prove competitive on certain tasks .	84-108	84-108	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	We show that models which require expensive cross-lingual knowledge almost always perform better , but cheaply supervised models often prove competitive on certain tasks .	1<2	contrast	contrast
P16-1158	1-4	21-25	Recent work has shown	despite lacking explicit supervision .	1-25	1-25	Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations , despite lacking explicit supervision .	Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations , despite lacking explicit supervision .	1>2	attribution	attribution
P16-1158	5-20	21-25	that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations ,	despite lacking explicit supervision .	1-25	1-25	Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations , despite lacking explicit supervision .	Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations , despite lacking explicit supervision .	1>2	contrast	contrast
P16-1158	21-25	66-80	despite lacking explicit supervision .	In this paper , we carry out such an evaluation in two learning settings :	1-25	66-104	Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations , despite lacking explicit supervision .	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	1>2	bg-compare	bg-compare
P16-1158	26-32	43-65	Prior work has evaluated this intriguing result	but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	26-65	26-65	Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations , but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations , but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	1>2	contrast	contrast
P16-1158	26-32	33-42	Prior work has evaluated this intriguing result	using a word analogy prediction formulation and hand-selected relations ,	26-65	26-65	Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations , but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations , but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	1<2	manner-means	manner-means
P16-1158	43-65	66-80	but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	In this paper , we carry out such an evaluation in two learning settings :	26-65	66-104	Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations , but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated .	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	1>2	bg-compare	bg-compare
P16-1158	66-80	81-85	In this paper , we carry out such an evaluation in two learning settings :	( 1 ) spectral clustering	66-104	66-104	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	1<2	elab-enumember	elab-enumember
P16-1158	81-85	86-90	( 1 ) spectral clustering	to induce word relations ,	66-104	66-104	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	1<2	enablement	enablement
P16-1158	66-80	91-96	In this paper , we carry out such an evaluation in two learning settings :	and ( 2 ) supervised learning	66-104	66-104	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	1<2	elab-enumember	elab-enumember
P16-1158	91-96	97-104	and ( 2 ) supervised learning	to classify vector differences into relation types .	66-104	66-104	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	1<2	enablement	enablement
P16-1158	105-106	107-116	We find	that word embeddings capture a surprising amount of information ,	105-141	105-141	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	1>2	attribution	attribution
P16-1158	66-80	107-116	In this paper , we carry out such an evaluation in two learning settings :	that word embeddings capture a surprising amount of information ,	66-104	105-141	In this paper , we carry out such an evaluation in two learning settings : ( 1 ) spectral clustering to induce word relations , and ( 2 ) supervised learning to classify vector differences into relation types .	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	1<2	evaluation	evaluation
P16-1158	107-116	117-135	that word embeddings capture a surprising amount of information ,	and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations ,	105-141	105-141	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	1<2	joint	joint
P16-1158	117-135	136-141	and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations ,	including over unseen lexical items .	105-141	105-141	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	We find that word embeddings capture a surprising amount of information , and that , under suitable supervised training , vector subtraction generalises well to a broad range of relations , including over unseen lexical items .	1<2	elab-example	elab-example
P16-1159	12-17	18-34	Unlike conventional maximum likelihood estimation ,	minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics ,	12-40	12-40	Unlike conventional maximum likelihood estimation , minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics , which are not necessarily differentiable .	Unlike conventional maximum likelihood estimation , minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics , which are not necessarily differentiable .	1>2	comparison	comparison
P16-1159	1-11	18-34	We propose minimum risk training for end-to-end neural machine translation .	minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics ,	1-11	12-40	We propose minimum risk training for end-to-end neural machine translation .	Unlike conventional maximum likelihood estimation , minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics , which are not necessarily differentiable .	1<2	elab-addition	elab-addition
P16-1159	18-34	35-40	minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics ,	which are not necessarily differentiable .	12-40	12-40	Unlike conventional maximum likelihood estimation , minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics , which are not necessarily differentiable .	Unlike conventional maximum likelihood estimation , minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics , which are not necessarily differentiable .	1<2	elab-addition	elab-addition
P16-1159	41-42	43-64	Experiments show	that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs .	41-64	41-64	Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs .	Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs .	1>2	attribution	attribution
P16-1159	1-11	43-64	We propose minimum risk training for end-to-end neural machine translation .	that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs .	1-11	41-64	We propose minimum risk training for end-to-end neural machine translation .	Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs .	1<2	evaluation	evaluation
P16-1159	65-77	78-84	Transparent to architectures , our approach can be applied to more neural networks	and potentially benefit more NLP tasks .	65-84	65-84	Transparent to architectures , our approach can be applied to more neural networks and potentially benefit more NLP tasks .	Transparent to architectures , our approach can be applied to more neural networks and potentially benefit more NLP tasks .	1>2	progression	progression
P16-1159	1-11	78-84	We propose minimum risk training for end-to-end neural machine translation .	and potentially benefit more NLP tasks .	1-11	65-84	We propose minimum risk training for end-to-end neural machine translation .	Transparent to architectures , our approach can be applied to more neural networks and potentially benefit more NLP tasks .	1<2	evaluation	evaluation
P16-1160	1-22	23-32	The existing machine translation systems , whether phrase-based or neural , have relied almost exclusively on word-level modelling with explicit segmentation .	In this paper , we ask a fundamental question :	1-22	23-45	The existing machine translation systems , whether phrase-based or neural , have relied almost exclusively on word-level modelling with explicit segmentation .	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	1>2	bg-compare	bg-compare
P16-1160	23-32	33-40	In this paper , we ask a fundamental question :	can neural machine translation generate a character sequence	23-45	23-45	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	1<2	elab-addition	elab-addition
P16-1160	33-40	41-45	can neural machine translation generate a character sequence	without any explicit segmentation ?	23-45	23-45	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	1<2	condition	condition
P16-1160	46-50	51-67	To answer this question ,	we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs	46-82	46-82	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	1>2	enablement	enablement
P16-1160	23-32	51-67	In this paper , we ask a fundamental question :	we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs	23-45	46-82	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	1<2	elab-addition	elab-addition
P16-1160	51-67	68-74	we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs	-En-Cs , En-De , En-Ru and En-Fi-	46-82	46-82	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	1<2	elab-enumember	elab-enumember
P16-1160	51-67	75-82	we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs	using the parallel corpora from WMT '15 .	46-82	46-82	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	To answer this question , we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs -En-Cs , En-De , En-Ru and En-Fi- using the parallel corpora from WMT '15 .	1<2	manner-means	manner-means
P16-1160	83-85	86-107	Our experiments show	that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs .	83-107	83-107	Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs .	Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs .	1>2	attribution	attribution
P16-1160	23-32	86-107	In this paper , we ask a fundamental question :	that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs .	23-45	83-107	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs .	1<2	evaluation	evaluation
P16-1160	23-32	108-131	In this paper , we ask a fundamental question :	Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs , En-De and En-Fi	23-45	108-137	In this paper , we ask a fundamental question : can neural machine translation generate a character sequence without any explicit segmentation ?	Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs , En-De and En-Fi and perform comparably on En-Ru .	1<2	evaluation	evaluation
P16-1160	108-131	132-137	Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs , En-De and En-Fi	and perform comparably on En-Ru .	108-137	108-137	Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs , En-De and En-Fi and perform comparably on En-Ru .	Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs , En-De and En-Fi and perform comparably on En-Ru .	1<2	joint	joint
P16-1161	1-3,7-16	17-24	Discriminative translation models <*> have been shown to help statistical machine translation performance .	We propose a novel extension of this work	1-16	17-29	Discriminative translation models utilizing source context have been shown to help statistical machine translation performance .	We propose a novel extension of this work using target context information .	1>2	bg-goal	bg-goal
P16-1161	1-3,7-16	4-6	Discriminative translation models <*> have been shown to help statistical machine translation performance .	utilizing source context	1-16	1-16	Discriminative translation models utilizing source context have been shown to help statistical machine translation performance .	Discriminative translation models utilizing source context have been shown to help statistical machine translation performance .	1<2	elab-addition	elab-addition
P16-1161	17-24	25-29	We propose a novel extension of this work	using target context information .	17-29	17-29	We propose a novel extension of this work using target context information .	We propose a novel extension of this work using target context information .	1<2	manner-means	manner-means
P16-1161	30-33	34-46	Surprisingly , we show	that this model can be efficiently integrated directly in the decoding process .	30-46	30-46	Surprisingly , we show that this model can be efficiently integrated directly in the decoding process .	Surprisingly , we show that this model can be efficiently integrated directly in the decoding process .	1>2	attribution	attribution
P16-1161	17-24	34-46	We propose a novel extension of this work	that this model can be efficiently integrated directly in the decoding process .	17-29	30-46	We propose a novel extension of this work using target context information .	Surprisingly , we show that this model can be efficiently integrated directly in the decoding process .	1<2	evaluation	evaluation
P16-1161	17-24	47-54	We propose a novel extension of this work	Our approach scales to large training data sizes	17-29	47-67	We propose a novel extension of this work using target context information .	Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs .	1<2	evaluation	evaluation
P16-1161	47-54	55-67	Our approach scales to large training data sizes	and results in consistent improvements in translation quality on four language pairs .	47-67	47-67	Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs .	Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs .	1<2	progression	progression
P16-1161	17-24	68-72	We propose a novel extension of this work	We also provide an analysis	17-29	68-101	We propose a novel extension of this work using target context information .	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	1<2	evaluation	evaluation
P16-1161	68-72	73-87	We also provide an analysis	comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model	68-101	68-101	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	1<2	elab-addition	elab-addition
P16-1161	88-90	91-101	and we show	that our extension allows us to better capture morphological coherence .	68-101	68-101	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	1>2	attribution	attribution
P16-1161	68-72	91-101	We also provide an analysis	that our extension allows us to better capture morphological coherence .	68-101	68-101	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence .	1<2	progression	progression
P16-1161	17-24	102-111	We propose a novel extension of this work	Our work is freely available as part of Moses .	17-29	102-111	We propose a novel extension of this work using target context information .	Our work is freely available as part of Moses .	1<2	elab-addition	elab-addition
P16-1162	1-14	15-21	Neural machine translation ( NMT ) models typically operate with a fixed vocabulary ,	but translation is an open-vocabulary problem .	1-21	1-21	Neural machine translation ( NMT ) models typically operate with a fixed vocabulary , but translation is an open-vocabulary problem .	Neural machine translation ( NMT ) models typically operate with a fixed vocabulary , but translation is an open-vocabulary problem .	1>2	contrast	contrast
P16-1162	15-21	37-49	but translation is an open-vocabulary problem .	In this paper , we introduce a simpler and more effective approach ,	1-21	37-69	Neural machine translation ( NMT ) models typically operate with a fixed vocabulary , but translation is an open-vocabulary problem .	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	1>2	bg-goal	bg-goal
P16-1162	22-29	37-49	Previous work addresses the translation of out-of-vocabulary words	In this paper , we introduce a simpler and more effective approach ,	22-36	37-69	Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary .	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	1>2	bg-compare	bg-compare
P16-1162	22-29	30-36	Previous work addresses the translation of out-of-vocabulary words	by backing off to a dictionary .	22-36	22-36	Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary .	Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary .	1<2	manner-means	manner-means
P16-1162	37-49	50-57	In this paper , we introduce a simpler and more effective approach ,	making the NMT model capable of open-vocabulary translation	37-69	37-69	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	1<2	elab-addition	elab-addition
P16-1162	50-57	58-69	making the NMT model capable of open-vocabulary translation	by encoding rare and unknown words as sequences of subword units .	37-69	37-69	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	1<2	manner-means	manner-means
P16-1162	37-49	70-75	In this paper , we introduce a simpler and more effective approach ,	This is based on the intuition	37-69	70-117	In this paper , we introduce a simpler and more effective approach , making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	elab-addition	elab-addition
P16-1162	70-75	76-81	This is based on the intuition	that various word classes are translatable	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	elab-addition	elab-addition
P16-1162	76-81	82-87	that various word classes are translatable	via smaller units than words ,	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	manner-means	manner-means
P16-1162	82-87	88-90	via smaller units than words ,	for instance names	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	elab-example	elab-example
P16-1162	88-90	91-98	for instance names	( via character copying or transliteration ) ,	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	manner-means	manner-means
P16-1162	82-87	99	via smaller units than words ,	compounds	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	elab-example	elab-example
P16-1162	99	100-105	compounds	( via compositional translation ) ,	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	manner-means	manner-means
P16-1162	82-87	106-109	via smaller units than words ,	and cognates and loanwords	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	elab-example	elab-example
P16-1162	106-109	110-117	and cognates and loanwords	( via phonological and morphological transformations ) .	70-117	70-117	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	1<2	manner-means	manner-means
P16-1162	70-75	118-127	This is based on the intuition	We discuss the suitability of different word segmentation techniques ,	70-117	118-175	This is based on the intuition that various word classes are translatable via smaller units than words , for instance names ( via character copying or transliteration ) , compounds ( via compositional translation ) , and cognates and loanwords ( via phonological and morphological transformations ) .	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	1<2	evaluation	evaluation
P16-1162	118-127	128-135	We discuss the suitability of different word segmentation techniques ,	including simple character ngram models and a segmentation	118-175	118-175	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	1<2	elab-example	elab-example
P16-1162	128-135	136-144	including simple character ngram models and a segmentation	based on the byte pair encoding compression algorithm ,	118-175	118-175	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	1<2	bg-general	bg-general
P16-1162	145-147	148-175	and empirically show	that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	118-175	118-175	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	1>2	attribution	attribution
P16-1162	118-127	148-175	We discuss the suitability of different word segmentation techniques ,	that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	118-175	118-175	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	We discuss the suitability of different word segmentation techniques , including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by up to 1.1 and 1.3 BLEU , respectively .	1<2	progression	progression
P16-1163	1-3,18-24	44-45,52-54	Word pairs , <*> have been proven to be very useful	the performance <*> is limited .	1-34	35-54	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	However , because of the data sparsity problem , the performance achieved by using word pair features is limited .	1>2	contrast	contrast
P16-1163	1-3,18-24	4-17	Word pairs , <*> have been proven to be very useful	which are one of the most easily accessible features between two text segments ,	1-34	1-34	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	1<2	elab-addition	elab-addition
P16-1163	18-24	25-29	have been proven to be very useful	for detecting the discourse relations	1-34	1-34	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	1<2	elab-addition	elab-addition
P16-1163	25-29	30-34	for detecting the discourse relations	held between text segments .	1-34	1-34	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	Word pairs , which are one of the most easily accessible features between two text segments , have been proven to be very useful for detecting the discourse relations held between text segments .	1<2	elab-addition	elab-addition
P16-1163	35-43	44-45,52-54	However , because of the data sparsity problem ,	the performance <*> is limited .	35-54	35-54	However , because of the data sparsity problem , the performance achieved by using word pair features is limited .	However , because of the data sparsity problem , the performance achieved by using word pair features is limited .	1>2	exp-reason	exp-reason
P16-1163	44-45,52-54	55-67	the performance <*> is limited .	In this paper , in order to overcome the data sparsity problem ,	35-54	55-80	However , because of the data sparsity problem , the performance achieved by using word pair features is limited .	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	1>2	bg-goal	bg-goal
P16-1163	44-45,52-54	46-51	the performance <*> is limited .	achieved by using word pair features	35-54	35-54	However , because of the data sparsity problem , the performance achieved by using word pair features is limited .	However , because of the data sparsity problem , the performance achieved by using word pair features is limited .	1<2	elab-addition	elab-addition
P16-1163	55-67	68-74	In this paper , in order to overcome the data sparsity problem ,	we propose the use of word embeddings	55-80	55-80	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	1>2	enablement	enablement
P16-1163	68-74	75-80	we propose the use of word embeddings	to replace the original words .	55-80	55-80	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	1<2	enablement	enablement
P16-1163	55-67	81-88	In this paper , in order to overcome the data sparsity problem ,	Moreover , we adopt a gated relevance network	55-80	81-114	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	1<2	elab-addition	elab-addition
P16-1163	81-88	89-97	Moreover , we adopt a gated relevance network	to capture the semantic interaction between word pairs ,	81-114	81-114	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	1<2	enablement	enablement
P16-1163	81-88	98-103	Moreover , we adopt a gated relevance network	and then aggregate those semantic interactions	81-114	81-114	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	1<2	progression	progression
P16-1163	98-103	104-107	and then aggregate those semantic interactions	using a pooling layer	81-114	81-114	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	1<2	manner-means	manner-means
P16-1163	104-107	108-114	using a pooling layer	to select the most informative interactions .	81-114	81-114	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	Moreover , we adopt a gated relevance network to capture the semantic interaction between word pairs , and then aggregate those semantic interactions using a pooling layer to select the most informative interactions .	1<2	enablement	enablement
P16-1163	115-122	123-126,132-135	Experimental results on Penn Discourse Tree Bank show	that the proposed method <*> can achieve better performance	115-147	115-147	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	1>2	attribution	attribution
P16-1163	55-67	123-126,132-135	In this paper , in order to overcome the data sparsity problem ,	that the proposed method <*> can achieve better performance	55-80	115-147	In this paper , in order to overcome the data sparsity problem , we propose the use of word embeddings to replace the original words .	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	1<2	evaluation	evaluation
P16-1163	123-126,132-135	127-131	that the proposed method <*> can achieve better performance	without using manually designed features	115-147	115-147	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	1<2	condition	condition
P16-1163	132-135	136-147	can achieve better performance	on recognizing the discourse level relations in all of the relations .	115-147	115-147	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations .	1<2	elab-addition	elab-addition
P16-1164	1-5	15-27	Quotation detection is the task	The state of the art treats this problem as a sequence labeling task	1-14	15-34	Quotation detection is the task of locating spans of quoted speech in text .	The state of the art treats this problem as a sequence labeling task and employs linear-chain conditional random fields .	1>2	elab-addition	elab-addition
P16-1164	1-5	6-14	Quotation detection is the task	of locating spans of quoted speech in text .	1-14	1-14	Quotation detection is the task of locating spans of quoted speech in text .	Quotation detection is the task of locating spans of quoted speech in text .	1<2	elab-definition	elab-definition
P16-1164	15-27	68-78	The state of the art treats this problem as a sequence labeling task	We perform an extensive analysis with two new model architectures .	15-34	68-78	The state of the art treats this problem as a sequence labeling task and employs linear-chain conditional random fields .	We perform an extensive analysis with two new model architectures .	1>2	bg-compare	bg-compare
P16-1164	15-27	28-34	The state of the art treats this problem as a sequence labeling task	and employs linear-chain conditional random fields .	15-34	15-34	The state of the art treats this problem as a sequence labeling task and employs linear-chain conditional random fields .	The state of the art treats this problem as a sequence labeling task and employs linear-chain conditional random fields .	1<2	joint	joint
P16-1164	35-42	68-78	We question the efficacy of this choice :	We perform an extensive analysis with two new model architectures .	35-67	68-78	We question the efficacy of this choice : The Markov assumption in the model prohibits it from making joint decisions about the begin , end , and internal context of a quotation .	We perform an extensive analysis with two new model architectures .	1>2	elab-addition	elab-addition
P16-1164	35-42	43-67	We question the efficacy of this choice :	The Markov assumption in the model prohibits it from making joint decisions about the begin , end , and internal context of a quotation .	35-67	35-67	We question the efficacy of this choice : The Markov assumption in the model prohibits it from making joint decisions about the begin , end , and internal context of a quotation .	We question the efficacy of this choice : The Markov assumption in the model prohibits it from making joint decisions about the begin , end , and internal context of a quotation .	1<2	elab-addition	elab-addition
P16-1164	79-80	81-88,95-103	We find	that ( a ) , simple boundary classification <*> is competitive with the state of the art ;	79-121	79-121	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	1>2	attribution	attribution
P16-1164	68-78	81-88,95-103	We perform an extensive analysis with two new model architectures .	that ( a ) , simple boundary classification <*> is competitive with the state of the art ;	68-78	79-121	We perform an extensive analysis with two new model architectures .	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	1<2	evaluation	evaluation
P16-1164	81-88,95-103	89-94	that ( a ) , simple boundary classification <*> is competitive with the state of the art ;	combined with a greedy prediction strategy	79-121	79-121	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	1<2	elab-addition	elab-addition
P16-1164	81-88,95-103	104-115	that ( a ) , simple boundary classification <*> is competitive with the state of the art ;	( b ) , a semi-Markov model significantly outperforms all others ,	79-121	79-121	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	1<2	joint	joint
P16-1164	104-115	116-121	( b ) , a semi-Markov model significantly outperforms all others ,	by relaxing the Markov assumption .	79-121	79-121	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	We find that ( a ) , simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art ; ( b ) , a semi-Markov model significantly outperforms all others , by relaxing the Markov assumption .	1<2	manner-means	manner-means
P16-1165	1-13	22-29	This paper addresses the problem of speech act recognition in written asynchronous conversations	We propose a class of conditional structured models	1-21	22-42	This paper addresses the problem of speech act recognition in written asynchronous conversations ( e.g. , fora , emails ) .	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	1>2	bg-goal	bg-goal
P16-1165	1-13	14-21	This paper addresses the problem of speech act recognition in written asynchronous conversations	( e.g. , fora , emails ) .	1-21	1-21	This paper addresses the problem of speech act recognition in written asynchronous conversations ( e.g. , fora , emails ) .	This paper addresses the problem of speech act recognition in written asynchronous conversations ( e.g. , fora , emails ) .	1<2	elab-example	elab-example
P16-1165	22-29	30-34	We propose a class of conditional structured models	defined over arbitrary graph structures	22-42	22-42	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	1<2	elab-addition	elab-addition
P16-1165	22-29	35-42	We propose a class of conditional structured models	to capture the conversational dependencies between sentences .	22-42	22-42	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	1<2	enablement	enablement
P16-1165	22-29	43-47	We propose a class of conditional structured models	Our models use sentence representations	22-42	43-61	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	Our models use sentence representations encoded by a long short term memory ( LSTM ) recurrent neural model .	1<2	elab-addition	elab-addition
P16-1165	43-47	48-61	Our models use sentence representations	encoded by a long short term memory ( LSTM ) recurrent neural model .	43-61	43-61	Our models use sentence representations encoded by a long short term memory ( LSTM ) recurrent neural model .	Our models use sentence representations encoded by a long short term memory ( LSTM ) recurrent neural model .	1<2	elab-addition	elab-addition
P16-1165	62-73	74-82	Empirical evaluation shows the effectiveness of our approach over existing ones :	( i ) LSTMs provide better task-specific representations ,	62-95	62-95	Empirical evaluation shows the effectiveness of our approach over existing ones : ( i ) LSTMs provide better task-specific representations , and ( ii ) the global joint model improves over local models .	Empirical evaluation shows the effectiveness of our approach over existing ones : ( i ) LSTMs provide better task-specific representations , and ( ii ) the global joint model improves over local models .	1>2	attribution	attribution
P16-1165	22-29	74-82	We propose a class of conditional structured models	( i ) LSTMs provide better task-specific representations ,	22-42	62-95	We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences .	Empirical evaluation shows the effectiveness of our approach over existing ones : ( i ) LSTMs provide better task-specific representations , and ( ii ) the global joint model improves over local models .	1<2	evaluation	evaluation
P16-1165	74-82	83-95	( i ) LSTMs provide better task-specific representations ,	and ( ii ) the global joint model improves over local models .	62-95	62-95	Empirical evaluation shows the effectiveness of our approach over existing ones : ( i ) LSTMs provide better task-specific representations , and ( ii ) the global joint model improves over local models .	Empirical evaluation shows the effectiveness of our approach over existing ones : ( i ) LSTMs provide better task-specific representations , and ( ii ) the global joint model improves over local models .	1<2	joint	joint
P16-1166	1-7	8-22	This paper describes the first robust approach	to automatically labeling clauses with their situation entity type ( Smith , 2003 ) ,	1-44	1-44	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	1<2	elab-addition	elab-addition
P16-1166	8-22	23-29	to automatically labeling clauses with their situation entity type ( Smith , 2003 ) ,	capturing aspectual phenomena at the clause level	1-44	1-44	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	1<2	elab-addition	elab-addition
P16-1166	23-29	30-44	capturing aspectual phenomena at the clause level	which are relevant for interpreting both semantics at the clause level and discourse structure .	1-44	1-44	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	1<2	elab-addition	elab-addition
P16-1166	1-7	45-59	This paper describes the first robust approach	Previous work on this task used a small data set from a limited domain ,	1-44	45-76	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	Previous work on this task used a small data set from a limited domain , and relied mainly on words as features , an approach which is impractical in larger settings .	1<2	bg-compare	bg-compare
P16-1166	45-59	60-69	Previous work on this task used a small data set from a limited domain ,	and relied mainly on words as features , an approach	45-76	45-76	Previous work on this task used a small data set from a limited domain , and relied mainly on words as features , an approach which is impractical in larger settings .	Previous work on this task used a small data set from a limited domain , and relied mainly on words as features , an approach which is impractical in larger settings .	1<2	joint	joint
P16-1166	60-69	70-76	and relied mainly on words as features , an approach	which is impractical in larger settings .	45-76	45-76	Previous work on this task used a small data set from a limited domain , and relied mainly on words as features , an approach which is impractical in larger settings .	Previous work on this task used a small data set from a limited domain , and relied mainly on words as features , an approach which is impractical in larger settings .	1<2	elab-addition	elab-addition
P16-1166	1-7	77-90	This paper describes the first robust approach	We provide a new corpus of texts from 13 genres ( 40,000 clauses )	1-44	77-96	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	We provide a new corpus of texts from 13 genres ( 40,000 clauses ) annotated with situation entity types .	1<2	elab-addition	elab-addition
P16-1166	77-90	91-96	We provide a new corpus of texts from 13 genres ( 40,000 clauses )	annotated with situation entity types .	77-96	77-96	We provide a new corpus of texts from 13 genres ( 40,000 clauses ) annotated with situation entity types .	We provide a new corpus of texts from 13 genres ( 40,000 clauses ) annotated with situation entity types .	1<2	elab-addition	elab-addition
P16-1166	97-98	99-103,124-128	We show	that our sequence labeling approach <*> is robust across genres ,	97-136	97-136	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	1>2	attribution	attribution
P16-1166	1-7	99-103,124-128	This paper describes the first robust approach	that our sequence labeling approach <*> is robust across genres ,	1-44	97-136	This paper describes the first robust approach to automatically labeling clauses with their situation entity type ( Smith , 2003 ) , capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure .	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	1<2	evaluation	evaluation
P16-1166	99-103,124-128	104-118	that our sequence labeling approach <*> is robust across genres ,	using distributional information in the form of Brown clusters , as well as syntactic-semantic features	97-136	97-136	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	1<2	manner-means	manner-means
P16-1166	104-118	119-123	using distributional information in the form of Brown clusters , as well as syntactic-semantic features	targeted to the task ,	97-136	97-136	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	1<2	elab-addition	elab-addition
P16-1166	99-103,124-128	129-136	that our sequence labeling approach <*> is robust across genres ,	reaching accuracies of up to 76 % .	97-136	97-136	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	We show that our sequence labeling approach using distributional information in the form of Brown clusters , as well as syntactic-semantic features targeted to the task , is robust across genres , reaching accuracies of up to 76 % .	1<2	exp-evidence	exp-evidence
P16-1167	1-9	66-74	Activities and events in our lives are structural ,	In this paper , we present a data-driven approach	1-22	66-86	Activities and events in our lives are structural , be it a vacation , a camping trip , or a wedding .	In this paper , we present a data-driven approach to learning event knowledge from a large collection of photo albums .	1>2	bg-goal	bg-goal
P16-1167	1-9	10-22	Activities and events in our lives are structural ,	be it a vacation , a camping trip , or a wedding .	1-22	1-22	Activities and events in our lives are structural , be it a vacation , a camping trip , or a wedding .	Activities and events in our lives are structural , be it a vacation , a camping trip , or a wedding .	1<2	elab-example	elab-example
P16-1167	23-27	28-31	While individual details vary ,	there are characteristic patterns	23-40	23-40	While individual details vary , there are characteristic patterns that are specific to each of these scenarios .	While individual details vary , there are characteristic patterns that are specific to each of these scenarios .	1>2	contrast	contrast
P16-1167	1-9	28-31	Activities and events in our lives are structural ,	there are characteristic patterns	1-22	23-40	Activities and events in our lives are structural , be it a vacation , a camping trip , or a wedding .	While individual details vary , there are characteristic patterns that are specific to each of these scenarios .	1<2	elab-addition	elab-addition
P16-1167	28-31	32-40	there are characteristic patterns	that are specific to each of these scenarios .	23-40	23-40	While individual details vary , there are characteristic patterns that are specific to each of these scenarios .	While individual details vary , there are characteristic patterns that are specific to each of these scenarios .	1<2	elab-addition	elab-addition
P16-1167	28-31	41-52	there are characteristic patterns	For example , a wedding typically consists of a sequence of events	23-40	41-65	While individual details vary , there are characteristic patterns that are specific to each of these scenarios .	For example , a wedding typically consists of a sequence of events such as walking down the aisle , exchanging vows , and dancing .	1<2	elab-example	elab-example
P16-1167	41-52	53-65	For example , a wedding typically consists of a sequence of events	such as walking down the aisle , exchanging vows , and dancing .	41-65	41-65	For example , a wedding typically consists of a sequence of events such as walking down the aisle , exchanging vows , and dancing .	For example , a wedding typically consists of a sequence of events such as walking down the aisle , exchanging vows , and dancing .	1<2	elab-example	elab-example
P16-1167	66-74	75-86	In this paper , we present a data-driven approach	to learning event knowledge from a large collection of photo albums .	66-86	66-86	In this paper , we present a data-driven approach to learning event knowledge from a large collection of photo albums .	In this paper , we present a data-driven approach to learning event knowledge from a large collection of photo albums .	1<2	elab-addition	elab-addition
P16-1167	66-74	87-93	In this paper , we present a data-driven approach	We formulate the task as constrained optimization	66-86	87-110	In this paper , we present a data-driven approach to learning event knowledge from a large collection of photo albums .	We formulate the task as constrained optimization to induce the prototypical temporal structure of an event , integrating both visual and textual cues .	1<2	elab-addition	elab-addition
P16-1167	87-93	94-103	We formulate the task as constrained optimization	to induce the prototypical temporal structure of an event ,	87-110	87-110	We formulate the task as constrained optimization to induce the prototypical temporal structure of an event , integrating both visual and textual cues .	We formulate the task as constrained optimization to induce the prototypical temporal structure of an event , integrating both visual and textual cues .	1<2	enablement	enablement
P16-1167	94-103	104-110	to induce the prototypical temporal structure of an event ,	integrating both visual and textual cues .	87-110	87-110	We formulate the task as constrained optimization to induce the prototypical temporal structure of an event , integrating both visual and textual cues .	We formulate the task as constrained optimization to induce the prototypical temporal structure of an event , integrating both visual and textual cues .	1<2	elab-addition	elab-addition
P16-1167	111-113	114-129	Comprehensive evaluation demonstrates	that it is possible to learn multimodal knowledge of event structure from noisy web content .	111-129	111-129	Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content .	Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content .	1>2	attribution	attribution
P16-1167	66-74	114-129	In this paper , we present a data-driven approach	that it is possible to learn multimodal knowledge of event structure from noisy web content .	66-86	111-129	In this paper , we present a data-driven approach to learning event knowledge from a large collection of photo albums .	Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content .	1<2	evaluation	evaluation
P16-1168	1-17	104-119	Automatically generating a natural language description of an image is a fundamental problem in artificial intelligence .	We have developed a Japanese version of the MS COCO caption dataset and a generative model	1-17	104-145	Automatically generating a natural language description of an image is a fundamental problem in artificial intelligence .	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	1>2	bg-compare	bg-compare
P16-1168	1-17	18-27	Automatically generating a natural language description of an image is a fundamental problem in artificial intelligence .	This task involves both computer vision and natural language processing	1-17	18-36	Automatically generating a natural language description of an image is a fundamental problem in artificial intelligence .	This task involves both computer vision and natural language processing and is called `` image caption generation . ''	1<2	elab-addition	elab-addition
P16-1168	18-27	28-36	This task involves both computer vision and natural language processing	and is called `` image caption generation . ''	18-36	18-36	This task involves both computer vision and natural language processing and is called `` image caption generation . ''	This task involves both computer vision and natural language processing and is called `` image caption generation . ''	1<2	joint	joint
P16-1168	28-36	37-49	and is called `` image caption generation . ''	Research on image caption generation has typically focused on taking in an image	18-36	37-65	This task involves both computer vision and natural language processing and is called `` image caption generation . ''	Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English .	1<2	elab-addition	elab-addition
P16-1168	37-49	50-55	Research on image caption generation has typically focused on taking in an image	and generating a caption in English	37-65	37-65	Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English .	Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English .	1<2	joint	joint
P16-1168	50-55	56-65	and generating a caption in English	as existing image caption corpora are mostly in English .	37-65	37-65	Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English .	Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English .	1<2	exp-evidence	exp-evidence
P16-1168	56-65	66-87	as existing image caption corpora are mostly in English .	The lack of corpora in languages other than English is an issue , especially for morphologically rich languages such as Japanese .	37-65	66-87	Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English .	The lack of corpora in languages other than English is an issue , especially for morphologically rich languages such as Japanese .	1<2	elab-addition	elab-addition
P16-1168	66-87	88-103	The lack of corpora in languages other than English is an issue , especially for morphologically rich languages such as Japanese .	There is thus a need for corpora sufficiently large for image captioning in other languages .	66-87	88-103	The lack of corpora in languages other than English is an issue , especially for morphologically rich languages such as Japanese .	There is thus a need for corpora sufficiently large for image captioning in other languages .	1<2	cause	cause
P16-1168	104-119	120-125	We have developed a Japanese version of the MS COCO caption dataset and a generative model	based on a deep recurrent architecture	104-145	104-145	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	1<2	bg-general	bg-general
P16-1168	120-125	126-130	based on a deep recurrent architecture	that takes in an image	104-145	104-145	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	1<2	elab-addition	elab-addition
P16-1168	126-130	131-138	that takes in an image	and uses this Japanese version of the dataset	104-145	104-145	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	1<2	joint	joint
P16-1168	131-138	139-145	and uses this Japanese version of the dataset	to generate a caption in Japanese .	104-145	104-145	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	1<2	enablement	enablement
P16-1168	146-155	156-159	As the Japanese portion of the corpus is small ,	our model was designed	146-174	146-174	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	1>2	exp-reason	exp-reason
P16-1168	104-119	156-159	We have developed a Japanese version of the MS COCO caption dataset and a generative model	our model was designed	104-145	146-174	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	1<2	elab-addition	elab-addition
P16-1168	156-159	160-164,170-174	our model was designed	to transfer the knowledge representation <*> into the Japanese portion .	146-174	146-174	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	1<2	enablement	enablement
P16-1168	160-164,170-174	165-169	to transfer the knowledge representation <*> into the Japanese portion .	obtained from the English portion	146-174	146-174	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	As the Japanese portion of the corpus is small , our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion .	1<2	elab-addition	elab-addition
P16-1168	175-176	177-190	Experiments showed	that the resulting bilingual comparable corpus has better performance than a monolingual corpus ,	175-203	175-203	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	1>2	attribution	attribution
P16-1168	104-119	177-190	We have developed a Japanese version of the MS COCO caption dataset and a generative model	that the resulting bilingual comparable corpus has better performance than a monolingual corpus ,	104-145	175-203	We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese .	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	1<2	evaluation	evaluation
P16-1168	191	192-194,199-203	indicating	that image understanding <*> benefits a resource-poor language .	175-203	175-203	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	1>2	attribution	attribution
P16-1168	177-190	192-194,199-203	that the resulting bilingual comparable corpus has better performance than a monolingual corpus ,	that image understanding <*> benefits a resource-poor language .	175-203	175-203	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	1<2	elab-addition	elab-addition
P16-1168	192-194,199-203	195-198	that image understanding <*> benefits a resource-poor language .	using a resource-rich language	175-203	175-203	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus , indicating that image understanding using a resource-rich language benefits a resource-poor language .	1<2	elab-addition	elab-addition
P16-1169	1-4	35-44	We study the problem	Instead , we propose a probabilistic model for taxonomy induction	1-15	35-51	We study the problem of automatically building hypernym taxonomies from textual and visual data .	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	1>2	bg-goal	bg-goal
P16-1169	1-4	5-15	We study the problem	of automatically building hypernym taxonomies from textual and visual data .	1-15	1-15	We study the problem of automatically building hypernym taxonomies from textual and visual data .	We study the problem of automatically building hypernym taxonomies from textual and visual data .	1<2	elab-addition	elab-addition
P16-1169	16-28	35-44	Previous works in taxonomy induction generally ignore the increasingly prominent visual data ,	Instead , we propose a probabilistic model for taxonomy induction	16-34	35-51	Previous works in taxonomy induction generally ignore the increasingly prominent visual data , which encode important perceptual semantics .	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	1>2	bg-compare	bg-compare
P16-1169	16-28	29-34	Previous works in taxonomy induction generally ignore the increasingly prominent visual data ,	which encode important perceptual semantics .	16-34	16-34	Previous works in taxonomy induction generally ignore the increasingly prominent visual data , which encode important perceptual semantics .	Previous works in taxonomy induction generally ignore the increasingly prominent visual data , which encode important perceptual semantics .	1<2	elab-addition	elab-addition
P16-1169	35-44	45-51	Instead , we propose a probabilistic model for taxonomy induction	by jointly leveraging text and images .	35-51	35-51	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	1<2	manner-means	manner-means
P16-1169	52-57	58-61	To avoid hand-crafted feature engineering ,	we design end-to-end features	52-70	52-70	To avoid hand-crafted feature engineering , we design end-to-end features based on distributed representations of images and words .	To avoid hand-crafted feature engineering , we design end-to-end features based on distributed representations of images and words .	1>2	enablement	enablement
P16-1169	35-44	58-61	Instead , we propose a probabilistic model for taxonomy induction	we design end-to-end features	35-51	52-70	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	To avoid hand-crafted feature engineering , we design end-to-end features based on distributed representations of images and words .	1<2	elab-addition	elab-addition
P16-1169	58-61	62-70	we design end-to-end features	based on distributed representations of images and words .	52-70	52-70	To avoid hand-crafted feature engineering , we design end-to-end features based on distributed representations of images and words .	To avoid hand-crafted feature engineering , we design end-to-end features based on distributed representations of images and words .	1<2	bg-general	bg-general
P16-1169	35-44	71-75	Instead , we propose a probabilistic model for taxonomy induction	The model is discriminatively trained	35-51	71-103	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images .	1<2	elab-addition	elab-addition
P16-1169	71-75	76-82	The model is discriminatively trained	given a small set of existing ontologies	71-103	71-103	The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images .	The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images .	1<2	condition	condition
P16-1169	71-75	83-103	The model is discriminatively trained	and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images .	71-103	71-103	The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images .	The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images .	1<2	joint	joint
P16-1169	35-44	104-114	Instead , we propose a probabilistic model for taxonomy induction	We evaluate our model and features on the WordNet hierarchies ,	35-51	104-125	Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images .	We evaluate our model and features on the WordNet hierarchies , where our system outperforms previous approaches by a large gap .	1<2	evaluation	evaluation
P16-1169	104-114	115-125	We evaluate our model and features on the WordNet hierarchies ,	where our system outperforms previous approaches by a large gap .	104-125	104-125	We evaluate our model and features on the WordNet hierarchies , where our system outperforms previous approaches by a large gap .	We evaluate our model and features on the WordNet hierarchies , where our system outperforms previous approaches by a large gap .	1<2	elab-addition	elab-addition
P16-1170	1-31	43-48	There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images .	To move beyond the literal ,	1-31	43-74	There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images .	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	1>2	contrast	contrast
P16-1170	1-31	32-42	There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images .	These tasks have focused on literal descriptions of the image .	1-31	32-42	There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images .	These tasks have focused on literal descriptions of the image .	1<2	elab-addition	elab-addition
P16-1170	43-48	75-91	To move beyond the literal ,	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) ,	43-74	75-108	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	1>2	bg-goal	bg-goal
P16-1170	49-52	53-67	we choose to explore	how questions about an image are often directed at commonsense inference and the abstract events	43-74	43-74	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	1>2	attribution	attribution
P16-1170	43-48	53-67	To move beyond the literal ,	how questions about an image are often directed at commonsense inference and the abstract events	43-74	43-74	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	1<2	enablement	enablement
P16-1170	53-67	68-74	how questions about an image are often directed at commonsense inference and the abstract events	evoked by objects in the image .	43-74	43-74	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	To move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image .	1<2	elab-addition	elab-addition
P16-1170	75-91	92-96	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) ,	where the system is tasked	75-108	75-108	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	1<2	elab-addition	elab-addition
P16-1170	92-96	97-103	where the system is tasked	with asking a natural and engaging question	75-108	75-108	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	1<2	manner-means	manner-means
P16-1170	97-103	104-108	with asking a natural and engaging question	when shown an image .	75-108	75-108	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	1<2	temporal	temporal
P16-1170	75-91	109-112	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) ,	We provide three datasets	75-108	109-138	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	We provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far .	1<2	elab-addition	elab-addition
P16-1170	109-112	113-129	We provide three datasets	which cover a variety of images from object-centric to event-centric , with considerably more abstract training data	109-138	109-138	We provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far .	We provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far .	1<2	elab-addition	elab-addition
P16-1170	113-129	130-138	which cover a variety of images from object-centric to event-centric , with considerably more abstract training data	than provided to state-of-the-art captioning systems thus far .	109-138	109-138	We provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far .	We provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far .	1<2	comparison	comparison
P16-1170	75-91	139-147	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) ,	We train and test several generative and retrieval models	75-108	139-154	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	We train and test several generative and retrieval models to tackle the task of VQG .	1<2	elab-addition	elab-addition
P16-1170	139-147	148-154	We train and test several generative and retrieval models	to tackle the task of VQG .	139-154	139-154	We train and test several generative and retrieval models to tackle the task of VQG .	We train and test several generative and retrieval models to tackle the task of VQG .	1<2	enablement	enablement
P16-1170	155-157	171-179	Evaluation results show	there is still a wide gap with human performance	155-192	155-192	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	1>2	attribution	attribution
P16-1170	158-170	171-179	that while such models ask reasonable questions for a variety of images ,	there is still a wide gap with human performance	155-192	155-192	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	1>2	temporal	temporal
P16-1170	75-91	171-179	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) ,	there is still a wide gap with human performance	75-108	155-192	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	1<2	evaluation	evaluation
P16-1170	171-179	180-183	there is still a wide gap with human performance	which motivates further work	155-192	155-192	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	1<2	elab-addition	elab-addition
P16-1170	180-183	184-192	which motivates further work	on connecting images with commonsense knowledge and pragmatics .	155-192	155-192	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	Evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics .	1<2	elab-addition	elab-addition
P16-1170	75-91	193-202	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) ,	Our proposed task offers a new challenge to the community	75-108	193-216	In this paper , we introduce the novel task of Visual Question Generation ( VQG ) , where the system is tasked with asking a natural and engaging question when shown an image .	Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .	1<2	evaluation	evaluation
P16-1170	193-202	203-207	Our proposed task offers a new challenge to the community	which we hope furthers interest	193-216	193-216	Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .	Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .	1<2	elab-addition	elab-addition
P16-1170	203-207	208-216	which we hope furthers interest	in exploring deeper connections between vision & language .	193-216	193-216	Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .	Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .	1<2	elab-addition	elab-addition
P16-1171	1-4	5-23	Linguistics studies have shown	that action verbs often denote some Change of State ( CoS ) as the result of an action .	1-23	1-23	Linguistics studies have shown that action verbs often denote some Change of State ( CoS ) as the result of an action .	Linguistics studies have shown that action verbs often denote some Change of State ( CoS ) as the result of an action .	1>2	attribution	attribution
P16-1171	5-23	24-44	that action verbs often denote some Change of State ( CoS ) as the result of an action .	However , the causality of action verbs and its potential connection with the physical world has not been systematically explored .	1-23	24-44	Linguistics studies have shown that action verbs often denote some Change of State ( CoS ) as the result of an action .	However , the causality of action verbs and its potential connection with the physical world has not been systematically explored .	1>2	contrast	contrast
P16-1171	24-44	50-69	However , the causality of action verbs and its potential connection with the physical world has not been systematically explored .	this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	24-44	45-69	However , the causality of action verbs and its potential connection with the physical world has not been systematically explored .	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	1>2	bg-goal	bg-goal
P16-1171	45-49	50-69	To address this limitation ,	this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	45-69	45-69	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	1>2	enablement	enablement
P16-1171	50-69	70-75	this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	We first conducted a crowdsourcing experiment	45-69	70-86	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	We first conducted a crowdsourcing experiment and identified eighteen categories of physical causality for action verbs .	1<2	elab-process_step	elab-process_step
P16-1171	70-75	76-86	We first conducted a crowdsourcing experiment	and identified eighteen categories of physical causality for action verbs .	70-86	70-86	We first conducted a crowdsourcing experiment and identified eighteen categories of physical causality for action verbs .	We first conducted a crowdsourcing experiment and identified eighteen categories of physical causality for action verbs .	1<2	joint	joint
P16-1171	50-69	87-100	this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	For a subset of these categories , we then defined a set of detectors	45-69	87-113	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	For a subset of these categories , we then defined a set of detectors that detect the corresponding change from visual perception of the physical environment .	1<2	elab-process_step	elab-process_step
P16-1171	87-100	101-113	For a subset of these categories , we then defined a set of detectors	that detect the corresponding change from visual perception of the physical environment .	87-113	87-113	For a subset of these categories , we then defined a set of detectors that detect the corresponding change from visual perception of the physical environment .	For a subset of these categories , we then defined a set of detectors that detect the corresponding change from visual perception of the physical environment .	1<2	elab-addition	elab-addition
P16-1171	50-69	114-127	this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	We further incorporated physical causality modeling and state detection in grounded language understanding .	45-69	114-127	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	We further incorporated physical causality modeling and state detection in grounded language understanding .	1<2	elab-process_step	elab-process_step
P16-1171	50-69	128-143	this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	Our empirical studies have demonstrated the effectiveness of causality modeling in grounding language to perception .	45-69	128-143	To address this limitation , this paper presents a study on physical causality of action verbs and their implied changes in the physical world .	Our empirical studies have demonstrated the effectiveness of causality modeling in grounding language to perception .	1<2	evaluation	evaluation
P16-1172	1-11	12-18	This paper presents a problem-reduction approach to extractive multi-document summarization :	we propose a reduction to the problem	1-31	1-31	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	1<2	elab-addition	elab-addition
P16-1172	12-18	19-26	we propose a reduction to the problem	of scoring individual sentences with their ROUGE scores	1-31	1-31	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	1<2	elab-addition	elab-addition
P16-1172	12-18	27-31	we propose a reduction to the problem	based on supervised learning .	1-31	1-31	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	1<2	bg-general	bg-general
P16-1172	1-11	32-40	This paper presents a problem-reduction approach to extractive multi-document summarization :	For the summarization , we solve an optimization problem	1-31	32-52	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	For the summarization , we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized .	1<2	elab-addition	elab-addition
P16-1172	32-40	41-52	For the summarization , we solve an optimization problem	where the ROUGE score of the selected summary sentences is maximized .	32-52	32-52	For the summarization , we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized .	For the summarization , we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized .	1<2	elab-addition	elab-addition
P16-1172	32-40	53-70	For the summarization , we solve an optimization problem	To this end , we derive an approximation of the ROUGE-N score of a set of sentences ,	32-52	53-81	For the summarization , we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized .	To this end , we derive an approximation of the ROUGE-N score of a set of sentences , and define a principled discrete optimization problem for sentence selection .	1<2	elab-addition	elab-addition
P16-1172	53-70	71-81	To this end , we derive an approximation of the ROUGE-N score of a set of sentences ,	and define a principled discrete optimization problem for sentence selection .	53-81	53-81	To this end , we derive an approximation of the ROUGE-N score of a set of sentences , and define a principled discrete optimization problem for sentence selection .	To this end , we derive an approximation of the ROUGE-N score of a set of sentences , and define a principled discrete optimization problem for sentence selection .	1<2	joint	joint
P16-1172	82-86	97-106	Mathematical and empirical evidence suggests	thus reducing the problem to the sentence scoring task .	82-106	82-106	Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly , thus reducing the problem to the sentence scoring task .	Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly , thus reducing the problem to the sentence scoring task .	1>2	attribution	attribution
P16-1172	87-96	97-106	that the sentence selection step is solved almost exactly ,	thus reducing the problem to the sentence scoring task .	82-106	82-106	Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly , thus reducing the problem to the sentence scoring task .	Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly , thus reducing the problem to the sentence scoring task .	1>2	result	result
P16-1172	1-11	97-106	This paper presents a problem-reduction approach to extractive multi-document summarization :	thus reducing the problem to the sentence scoring task .	1-31	82-106	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly , thus reducing the problem to the sentence scoring task .	1<2	exp-evidence	exp-evidence
P16-1172	1-11	107-116	This paper presents a problem-reduction approach to extractive multi-document summarization :	We perform a detailed experimental evaluation on two DUC datasets	1-31	107-124	This paper presents a problem-reduction approach to extractive multi-document summarization : we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning .	We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach .	1<2	evaluation	evaluation
P16-1172	107-116	117-124	We perform a detailed experimental evaluation on two DUC datasets	to demonstrate the validity of our approach .	107-124	107-124	We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach .	We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach .	1<2	enablement	enablement
P16-1173	1-12	18-20	There has been almost no work on phrase structure annotation and parsing	despite the fact	1-33	1-33	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	1>2	contrast	contrast
P16-1173	1-12	13-17	There has been almost no work on phrase structure annotation and parsing	specially designed for learner English	1-33	1-33	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	1<2	elab-addition	elab-addition
P16-1173	18-20	39-53	despite the fact	in this paper , we first propose a phrase structure annotation scheme for learner English	1-33	34-62	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	1>2	bg-goal	bg-goal
P16-1173	18-20	21-33	despite the fact	that they are useful for representing the structural characteristics of learner English .	1-33	1-33	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner English .	1<2	elab-addition	elab-addition
P16-1173	34-38	39-53	To address this problem ,	in this paper , we first propose a phrase structure annotation scheme for learner English	34-62	34-62	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	1>2	enablement	enablement
P16-1173	39-53	54-59	in this paper , we first propose a phrase structure annotation scheme for learner English	and annotate two different learner corpora	34-62	34-62	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	1<2	progression	progression
P16-1173	54-59	60-62	and annotate two different learner corpora	using it .	34-62	34-62	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	1<2	manner-means	manner-means
P16-1173	39-53	63-69	in this paper , we first propose a phrase structure annotation scheme for learner English	Second , we show their usefulness ,	34-62	63-97	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	Second , we show their usefulness , reporting on ( a ) inter-annotator agreement rate , ( b ) characteristic CFG rules in the corpora , and ( c ) parsing performance on them .	1<2	elab-process_step	elab-process_step
P16-1173	63-69	70-97	Second , we show their usefulness ,	reporting on ( a ) inter-annotator agreement rate , ( b ) characteristic CFG rules in the corpora , and ( c ) parsing performance on them .	63-97	63-97	Second , we show their usefulness , reporting on ( a ) inter-annotator agreement rate , ( b ) characteristic CFG rules in the corpora , and ( c ) parsing performance on them .	Second , we show their usefulness , reporting on ( a ) inter-annotator agreement rate , ( b ) characteristic CFG rules in the corpora , and ( c ) parsing performance on them .	1<2	elab-addition	elab-addition
P16-1173	39-53	98-103	in this paper , we first propose a phrase structure annotation scheme for learner English	In addition , we explore methods	34-62	98-119	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	In addition , we explore methods to improve phrase structure parsing for learner English ( achieving an F-measure of 0.878 ) .	1<2	elab-process_step	elab-process_step
P16-1173	98-103	104-111	In addition , we explore methods	to improve phrase structure parsing for learner English	98-119	98-119	In addition , we explore methods to improve phrase structure parsing for learner English ( achieving an F-measure of 0.878 ) .	In addition , we explore methods to improve phrase structure parsing for learner English ( achieving an F-measure of 0.878 ) .	1<2	enablement	enablement
P16-1173	104-111	112-119	to improve phrase structure parsing for learner English	( achieving an F-measure of 0.878 ) .	98-119	98-119	In addition , we explore methods to improve phrase structure parsing for learner English ( achieving an F-measure of 0.878 ) .	In addition , we explore methods to improve phrase structure parsing for learner English ( achieving an F-measure of 0.878 ) .	1<2	elab-addition	elab-addition
P16-1173	39-53	120-144	in this paper , we first propose a phrase structure annotation scheme for learner English	Finally , we release the full annotation guidelines , the annotated data , and the improved parser model for learner English to the public .	34-62	120-144	To address this problem , in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it .	Finally , we release the full annotation guidelines , the annotated data , and the improved parser model for learner English to the public .	1<2	elab-process_step	elab-process_step
P16-1174	1-22	23-32	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	HLR combines psycholinguistic theory with modern machine learning techniques ,	1-22	23-50	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	HLR combines psycholinguistic theory with modern machine learning techniques , indirectly estimating the `` halflife '' of a word or concept in a student 's long-term memory .	1<2	elab-addition	elab-addition
P16-1174	23-32	33-50	HLR combines psycholinguistic theory with modern machine learning techniques ,	indirectly estimating the `` halflife '' of a word or concept in a student 's long-term memory .	23-50	23-50	HLR combines psycholinguistic theory with modern machine learning techniques , indirectly estimating the `` halflife '' of a word or concept in a student 's long-term memory .	HLR combines psycholinguistic theory with modern machine learning techniques , indirectly estimating the `` halflife '' of a word or concept in a student 's long-term memory .	1<2	elab-addition	elab-addition
P16-1174	1-22	51-63	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	We use data from Duolingo - a popular online language learning application -	1-22	51-84	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	1<2	elab-addition	elab-addition
P16-1174	51-63	64-68	We use data from Duolingo - a popular online language learning application -	to fit HLR models ,	51-84	51-84	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	1<2	enablement	enablement
P16-1174	51-63	69-74	We use data from Duolingo - a popular online language learning application -	reducing error by 45 % +	51-84	51-84	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	1<2	elab-addition	elab-addition
P16-1174	69-74	75-78	reducing error by 45 % +	compared to several baselines	51-84	51-84	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	1<2	comparison	comparison
P16-1174	75-78	79-84	compared to several baselines	at predicting student recall rates .	51-84	51-84	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	We use data from Duolingo - a popular online language learning application - to fit HLR models , reducing error by 45 % + compared to several baselines at predicting student recall rates .	1<2	elab-addition	elab-addition
P16-1174	85-90	91-102	HLR model weights also shed light	on which linguistic concepts are systematically challenging for second language learners .	85-102	85-102	HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners .	HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners .	1>2	attribution	attribution
P16-1174	1-22	91-102	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	on which linguistic concepts are systematically challenging for second language learners .	1-22	85-102	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners .	1<2	elab-addition	elab-addition
P16-1174	1-22	103-122	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	Finally , HLR was able to improve Duolingo daily student engagement by 12 % in an operational user study .	1-22	103-122	We present half-life regression ( HLR ) , a novel model for spaced repetition practice with applications to second language acquisition .	Finally , HLR was able to improve Duolingo daily student engagement by 12 % in an operational user study .	1<2	evaluation	evaluation
P16-1175	1-7	23-27	Foreign language learners can acquire new vocabulary	we devise an experimental framework	1-16	17-36	Foreign language learners can acquire new vocabulary by using cognate and context clues when reading .	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	1>2	bg-goal	bg-goal
P16-1175	1-7	8-13	Foreign language learners can acquire new vocabulary	by using cognate and context clues	1-16	1-16	Foreign language learners can acquire new vocabulary by using cognate and context clues when reading .	Foreign language learners can acquire new vocabulary by using cognate and context clues when reading .	1<2	manner-means	manner-means
P16-1175	8-13	14-16	by using cognate and context clues	when reading .	1-16	1-16	Foreign language learners can acquire new vocabulary by using cognate and context clues when reading .	Foreign language learners can acquire new vocabulary by using cognate and context clues when reading .	1<2	temporal	temporal
P16-1175	17-22	23-27	To measure such incidental comprehension ,	we devise an experimental framework	17-36	17-36	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	1>2	enablement	enablement
P16-1175	23-27	28-36	we devise an experimental framework	that involves reading mixed-language `` macaronic '' sentences .	17-36	17-36	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	1<2	elab-addition	elab-addition
P16-1175	37-38	45-49	Using data	we train a graphical model	37-85	37-85	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	1>2	manner-means	manner-means
P16-1175	37-38	39-44	Using data	collected via Amazon Mechanical Turk ,	37-85	37-85	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	1<2	elab-addition	elab-addition
P16-1175	23-27	45-49	we devise an experimental framework	we train a graphical model	17-36	37-85	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	1<2	elab-addition	elab-addition
P16-1175	45-49	50-60	we train a graphical model	to simulate a human subject 's comprehension of foreign words ,	37-85	37-85	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	1<2	enablement	enablement
P16-1175	50-60	61-85	to simulate a human subject 's comprehension of foreign words ,	based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	37-85	37-85	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	Using data collected via Amazon Mechanical Turk , we train a graphical model to simulate a human subject 's comprehension of foreign words , based on cognate clues ( edit distance to an English word ) , context clues ( pointwise mutual information ) , and prior exposure .	1<2	bg-general	bg-general
P16-1175	23-27	86-91	we devise an experimental framework	Our model does a reasonable job	17-36	86-118	To measure such incidental comprehension , we devise an experimental framework that involves reading mixed-language `` macaronic '' sentences .	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	1<2	evaluation	evaluation
P16-1175	92-93	94-103	at predicting	which words a user will be able to understand ,	86-118	86-118	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	1>2	attribution	attribution
P16-1175	86-91	94-103	Our model does a reasonable job	which words a user will be able to understand ,	86-118	86-118	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	1<2	elab-addition	elab-addition
P16-1175	86-91	104-118	Our model does a reasonable job	which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	86-118	86-118	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	Our model does a reasonable job at predicting which words a user will be able to understand , which should facilitate the automatic construction of comprehensible text for personalized foreign language education .	1<2	elab-addition	elab-addition
P16-1176	1-10	11-18	We present a computational analysis of three language varieties :	native , advanced non-native , and translation .	1-18	1-18	We present a computational analysis of three language varieties : native , advanced non-native , and translation .	We present a computational analysis of three language varieties : native , advanced non-native , and translation .	1<2	elab-enumember	elab-enumember
P16-1176	1-10	19-34	We present a computational analysis of three language varieties :	Our goal is to investigate the similarities and differences between non-native language productions and translations ,	1-18	19-40	We present a computational analysis of three language varieties : native , advanced non-native , and translation .	Our goal is to investigate the similarities and differences between non-native language productions and translations , contrasting both with native language .	1<2	bg-goal	bg-goal
P16-1176	19-34	35-40	Our goal is to investigate the similarities and differences between non-native language productions and translations ,	contrasting both with native language .	19-40	19-40	Our goal is to investigate the similarities and differences between non-native language productions and translations , contrasting both with native language .	Our goal is to investigate the similarities and differences between non-native language productions and translations , contrasting both with native language .	1<2	comparison	comparison
P16-1176	41-46	47-51	Using a collection of computational methods	we establish three main results	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1>2	manner-means	manner-means
P16-1176	1-10	47-51	We present a computational analysis of three language varieties :	we establish three main results	1-18	41-120	We present a computational analysis of three language varieties : native , advanced non-native , and translation .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	elab-addition	elab-addition
P16-1176	47-51	52-64	we establish three main results	: ( 1 ) the three types of texts are easily distinguishable ;	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	elab-enumember	elab-enumember
P16-1176	52-64	65-76	: ( 1 ) the three types of texts are easily distinguishable ;	( 2 ) nonnative language and translations are closer to each other	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	elab-enumember	elab-enumember
P16-1176	65-76	77-85	( 2 ) nonnative language and translations are closer to each other	than each of them is to native language ;	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	comparison	comparison
P16-1176	52-64	86-101	: ( 1 ) the three types of texts are easily distinguishable ;	and ( 3 ) some of these characteristics depend on the source or native language ,	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	elab-enumember	elab-enumember
P16-1176	86-101	102-106	and ( 3 ) some of these characteristics depend on the source or native language ,	while others do not ,	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	comparison	comparison
P16-1176	86-101	107-112	and ( 3 ) some of these characteristics depend on the source or native language ,	reflecting , perhaps , unified principles	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	elab-addition	elab-addition
P16-1176	107-112	113-120	reflecting , perhaps , unified principles	that similarly affect translations and non-native language .	41-120	41-120	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	Using a collection of computational methods we establish three main results : ( 1 ) the three types of texts are easily distinguishable ; ( 2 ) nonnative language and translations are closer to each other than each of them is to native language ; and ( 3 ) some of these characteristics depend on the source or native language , while others do not , reflecting , perhaps , unified principles that similarly affect translations and non-native language .	1<2	elab-addition	elab-addition
P16-1177	1-6	7-12	We present a pairwise context-sensitive Autoencoder	for computing text pair similarity .	1-12	1-12	We present a pairwise context-sensitive Autoencoder for computing text pair similarity .	We present a pairwise context-sensitive Autoencoder for computing text pair similarity .	1<2	elab-addition	elab-addition
P16-1177	13-20	21-23	Our model encodes input text into context-sensitive representations	and uses them	13-30	13-30	Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs .	Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs .	1>2	progression	progression
P16-1177	1-6	21-23	We present a pairwise context-sensitive Autoencoder	and uses them	1-12	13-30	We present a pairwise context-sensitive Autoencoder for computing text pair similarity .	Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs .	1<2	elab-addition	elab-addition
P16-1177	21-23	24-30	and uses them	to compute similarity between text pairs .	13-30	13-30	Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs .	Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs .	1<2	enablement	enablement
P16-1177	1-6	31-48	We present a pairwise context-sensitive Autoencoder	Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task .	1-12	31-48	We present a pairwise context-sensitive Autoencoder for computing text pair similarity .	Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task .	1<2	evaluation	evaluation
P16-1177	1-6	49-54,69-76	We present a pairwise context-sensitive Autoencoder	For retrieval , our unsupervised approach <*> shows comparable performance with the state-of-the-art supervised models	1-12	49-83	We present a pairwise context-sensitive Autoencoder for computing text pair similarity .	For retrieval , our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them .	1<2	evaluation	evaluation
P16-1177	49-54,69-76	55-68	For retrieval , our unsupervised approach <*> shows comparable performance with the state-of-the-art supervised models	that merely ranks inputs with respect to the cosine similarity between their hidden representations	49-83	49-83	For retrieval , our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them .	For retrieval , our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them .	1<2	elab-addition	elab-addition
P16-1177	69-76	77-83	shows comparable performance with the state-of-the-art supervised models	and in some cases outperforms them .	49-83	49-83	For retrieval , our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them .	For retrieval , our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them .	1<2	progression	progression
P16-1178	1-11	37-45	Online news editors ask themselves the same question many times :	In this work , we address this important question	1-22	37-55	Online news editors ask themselves the same question many times : what is missing in this news article to go online ?	In this work , we address this important question and characterise the constituents of news article editorial quality .	1>2	bg-goal	bg-goal
P16-1178	1-11	12-18	Online news editors ask themselves the same question many times :	what is missing in this news article	1-22	1-22	Online news editors ask themselves the same question many times : what is missing in this news article to go online ?	Online news editors ask themselves the same question many times : what is missing in this news article to go online ?	1<2	elab-addition	elab-addition
P16-1178	12-18	19-22	what is missing in this news article	to go online ?	1-22	1-22	Online news editors ask themselves the same question many times : what is missing in this news article to go online ?	Online news editors ask themselves the same question many times : what is missing in this news article to go online ?	1<2	elab-addition	elab-addition
P16-1178	1-11	23-28	Online news editors ask themselves the same question many times :	This is not an easy question	1-22	23-36	Online news editors ask themselves the same question many times : what is missing in this news article to go online ?	This is not an easy question to be answered by computational linguistic methods .	1<2	elab-addition	elab-addition
P16-1178	23-28	29-36	This is not an easy question	to be answered by computational linguistic methods .	23-36	23-36	This is not an easy question to be answered by computational linguistic methods .	This is not an easy question to be answered by computational linguistic methods .	1<2	elab-addition	elab-addition
P16-1178	37-45	46-55	In this work , we address this important question	and characterise the constituents of news article editorial quality .	37-55	37-55	In this work , we address this important question and characterise the constituents of news article editorial quality .	In this work , we address this important question and characterise the constituents of news article editorial quality .	1<2	joint	joint
P16-1178	37-45	56-62	In this work , we address this important question	More specifically , we identify 14 aspects	37-55	56-70	In this work , we address this important question and characterise the constituents of news article editorial quality .	More specifically , we identify 14 aspects related to the content of news articles .	1<2	elab-addition	elab-addition
P16-1178	56-62	63-70	More specifically , we identify 14 aspects	related to the content of news articles .	56-70	56-70	More specifically , we identify 14 aspects related to the content of news articles .	More specifically , we identify 14 aspects related to the content of news articles .	1<2	elab-addition	elab-addition
P16-1178	37-45	71-81	In this work , we address this important question	Through a correlation analysis , we quantify their independence and relation	37-55	71-89	In this work , we address this important question and characterise the constituents of news article editorial quality .	Through a correlation analysis , we quantify their independence and relation to assessing an article 's editorial quality .	1<2	elab-addition	elab-addition
P16-1178	71-81	82-89	Through a correlation analysis , we quantify their independence and relation	to assessing an article 's editorial quality .	71-89	71-89	Through a correlation analysis , we quantify their independence and relation to assessing an article 's editorial quality .	Through a correlation analysis , we quantify their independence and relation to assessing an article 's editorial quality .	1<2	enablement	enablement
P16-1178	90-92	93-97,102-113	We also demonstrate	that the identified aspects , <*> can be used effectively in quality control methods for online news .	90-113	90-113	We also demonstrate that the identified aspects , when combined together , can be used effectively in quality control methods for online news .	We also demonstrate that the identified aspects , when combined together , can be used effectively in quality control methods for online news .	1>2	attribution	attribution
P16-1178	37-45	93-97,102-113	In this work , we address this important question	that the identified aspects , <*> can be used effectively in quality control methods for online news .	37-55	90-113	In this work , we address this important question and characterise the constituents of news article editorial quality .	We also demonstrate that the identified aspects , when combined together , can be used effectively in quality control methods for online news .	1<2	evaluation	evaluation
P16-1178	93-97,102-113	98-101	that the identified aspects , <*> can be used effectively in quality control methods for online news .	when combined together ,	90-113	90-113	We also demonstrate that the identified aspects , when combined together , can be used effectively in quality control methods for online news .	We also demonstrate that the identified aspects , when combined together , can be used effectively in quality control methods for online news .	1<2	condition	condition
P16-1179	1-18	19-28	Named Entity Disambiguation ( NED ) algorithms disambiguate mentions of named entities with respect to a knowledge-base ,	but sometimes the context might be poor or misleading .	1-28	1-28	Named Entity Disambiguation ( NED ) algorithms disambiguate mentions of named entities with respect to a knowledge-base , but sometimes the context might be poor or misleading .	Named Entity Disambiguation ( NED ) algorithms disambiguate mentions of named entities with respect to a knowledge-base , but sometimes the context might be poor or misleading .	1>2	contrast	contrast
P16-1179	19-28	29-41	but sometimes the context might be poor or misleading .	In this paper we introduce the acquisition of two kinds of background information	1-28	29-55	Named Entity Disambiguation ( NED ) algorithms disambiguate mentions of named entities with respect to a knowledge-base , but sometimes the context might be poor or misleading .	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	1>2	bg-goal	bg-goal
P16-1179	29-41	42-46	In this paper we introduce the acquisition of two kinds of background information	to alleviate that problem :	29-55	29-55	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	1<2	enablement	enablement
P16-1179	29-41	47-55	In this paper we introduce the acquisition of two kinds of background information	entity similarity and selectional preferences for syntactic positions .	29-55	29-55	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	1<2	elab-enumember	elab-enumember
P16-1179	56-58	68-76	We show ,	that the additional sources of context are complementary ,	56-101	56-101	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	1>2	elab-addition	elab-addition
P16-1179	56-58	59-67	We show ,	using a generative Naive Bayes model for NED ,	56-101	56-101	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	1<2	manner-means	manner-means
P16-1179	29-41	68-76	In this paper we introduce the acquisition of two kinds of background information	that the additional sources of context are complementary ,	29-55	56-101	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	1<2	evaluation	evaluation
P16-1179	68-76	77-90	that the additional sources of context are complementary ,	and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets ,	56-101	56-101	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	1<2	joint	joint
P16-1179	77-90	91-101	and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets ,	yielding the third best and the best results , respectively .	56-101	56-101	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	We show , using a generative Naive Bayes model for NED , that the additional sources of context are complementary , and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets , yielding the third best and the best results , respectively .	1<2	elab-addition	elab-addition
P16-1179	29-41	102-106	In this paper we introduce the acquisition of two kinds of background information	We provide examples and analysis	29-55	102-116	In this paper we introduce the acquisition of two kinds of background information to alleviate that problem : entity similarity and selectional preferences for syntactic positions .	We provide examples and analysis which show the value of the acquired background information .	1<2	elab-addition	elab-addition
P16-1179	102-106	107-116	We provide examples and analysis	which show the value of the acquired background information .	102-116	102-116	We provide examples and analysis which show the value of the acquired background information .	We provide examples and analysis which show the value of the acquired background information .	1<2	elab-addition	elab-addition
P16-1180	1-22	23-35	Finding paraphrases in text is an important task with implications for generation , summarization and question answering , among other applications .	Of particular interest to those applications is the specific formulation of the task	1-22	23-61	Finding paraphrases in text is an important task with implications for generation , summarization and question answering , among other applications .	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	1>2	elab-addition	elab-addition
P16-1180	23-35	84-90	Of particular interest to those applications is the specific formulation of the task	In this paper we present an approach	23-61	84-120	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	1>2	bg-goal	bg-goal
P16-1180	23-35	36-41	Of particular interest to those applications is the specific formulation of the task	where the paraphrases are templated ,	23-61	23-61	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	1<2	elab-addition	elab-addition
P16-1180	23-35	42-46	Of particular interest to those applications is the specific formulation of the task	which provides an easy way	23-61	23-61	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	1<2	elab-addition	elab-addition
P16-1180	42-46	47-53	which provides an easy way	to lexicalize one message in multiple ways	23-61	23-61	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	1<2	enablement	enablement
P16-1180	47-53	54-61	to lexicalize one message in multiple ways	by simply plugging in the relevant entities .	23-61	23-61	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated , which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities .	1<2	manner-means	manner-means
P16-1180	62-74	84-90	Previous work has focused on mining paraphrases from parallel and comparable corpora ,	In this paper we present an approach	62-83	84-120	Previous work has focused on mining paraphrases from parallel and comparable corpora , or mining very short sub-sentence synonyms and paraphrases .	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	1>2	bg-compare	bg-compare
P16-1180	62-74	75-83	Previous work has focused on mining paraphrases from parallel and comparable corpora ,	or mining very short sub-sentence synonyms and paraphrases .	62-83	62-83	Previous work has focused on mining paraphrases from parallel and comparable corpora , or mining very short sub-sentence synonyms and paraphrases .	Previous work has focused on mining paraphrases from parallel and comparable corpora , or mining very short sub-sentence synonyms and paraphrases .	1<2	joint	joint
P16-1180	84-90	91-96	In this paper we present an approach	which combines distributional and KB-driven methods	84-120	84-120	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	1<2	elab-addition	elab-addition
P16-1180	91-96	97-105	which combines distributional and KB-driven methods	to allow robust mining of sentence-level paraphrasal templates ,	84-120	84-120	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	1<2	enablement	enablement
P16-1180	91-96	106-120	which combines distributional and KB-driven methods	utilizing a rich type system for the slots , from a plain text corpus .	84-120	84-120	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates , utilizing a rich type system for the slots , from a plain text corpus .	1<2	manner-means	manner-means
P16-1181	1-13	14-27	We cast sentence boundary detection and syntactic parsing as a joint problem ,	so an entire text document forms a training instance for transition-based dependency parsing .	1-27	1-27	We cast sentence boundary detection and syntactic parsing as a joint problem , so an entire text document forms a training instance for transition-based dependency parsing .	We cast sentence boundary detection and syntactic parsing as a joint problem , so an entire text document forms a training instance for transition-based dependency parsing .	1<2	cause	cause
P16-1181	28-40	43-57	When trained with an early update or max-violation strategy for inexact search ,	that only a tiny part of these very long training instances is ever exploited .	28-57	28-57	When trained with an early update or max-violation strategy for inexact search , we observe that only a tiny part of these very long training instances is ever exploited .	When trained with an early update or max-violation strategy for inexact search , we observe that only a tiny part of these very long training instances is ever exploited .	1>2	condition	condition
P16-1181	41-42	43-57	we observe	that only a tiny part of these very long training instances is ever exploited .	28-57	28-57	When trained with an early update or max-violation strategy for inexact search , we observe that only a tiny part of these very long training instances is ever exploited .	When trained with an early update or max-violation strategy for inexact search , we observe that only a tiny part of these very long training instances is ever exploited .	1>2	attribution	attribution
P16-1181	1-13	43-57	We cast sentence boundary detection and syntactic parsing as a joint problem ,	that only a tiny part of these very long training instances is ever exploited .	1-27	28-57	We cast sentence boundary detection and syntactic parsing as a joint problem , so an entire text document forms a training instance for transition-based dependency parsing .	When trained with an early update or max-violation strategy for inexact search , we observe that only a tiny part of these very long training instances is ever exploited .	1<2	elab-addition	elab-addition
P16-1181	43-57	58-61	that only a tiny part of these very long training instances is ever exploited .	We demonstrate this effect	28-57	58-75	When trained with an early update or max-violation strategy for inexact search , we observe that only a tiny part of these very long training instances is ever exploited .	We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task .	1<2	elab-addition	elab-addition
P16-1181	58-61	62-75	We demonstrate this effect	by extending the ArcStandard transition system with swap for the joint prediction task .	58-75	58-75	We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task .	We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task .	1<2	manner-means	manner-means
P16-1181	76-83	84-91	When we use an alternative update strategy ,	our models are considerably better on both tasks	76-105	76-105	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	1>2	condition	condition
P16-1181	1-13	84-91	We cast sentence boundary detection and syntactic parsing as a joint problem ,	our models are considerably better on both tasks	1-27	76-105	We cast sentence boundary detection and syntactic parsing as a joint problem , so an entire text document forms a training instance for transition-based dependency parsing .	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	1<2	evaluation	evaluation
P16-1181	84-91	92-97	our models are considerably better on both tasks	and train in substantially less time	76-105	76-105	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	1<2	joint	joint
P16-1181	92-97	98-100	and train in substantially less time	compared to models	76-105	76-105	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	1<2	comparison	comparison
P16-1181	98-100	101-105	compared to models	trained with early update/max-violation .	76-105	76-105	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation .	1<2	elab-addition	elab-addition
P16-1181	1-13	106-131	We cast sentence boundary detection and syntactic parsing as a joint problem ,	A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection .	1-27	106-131	We cast sentence boundary detection and syntactic parsing as a joint problem , so an entire text document forms a training instance for transition-based dependency parsing .	A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection .	1<2	evaluation	evaluation
P16-1182	1-13	57-68	Precise evaluation metrics are important for assessing progress in high-level language generation tasks	In this paper , we perform a case study for metric evaluation	1-21	57-90	Precise evaluation metrics are important for assessing progress in high-level language generation tasks such as machine translation or image captioning .	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	1>2	bg-goal	bg-goal
P16-1182	1-13	14-21	Precise evaluation metrics are important for assessing progress in high-level language generation tasks	such as machine translation or image captioning .	1-21	1-21	Precise evaluation metrics are important for assessing progress in high-level language generation tasks such as machine translation or image captioning .	Precise evaluation metrics are important for assessing progress in high-level language generation tasks such as machine translation or image captioning .	1<2	elab-example	elab-example
P16-1182	22-28	35-42	Historically , these metrics have been evaluated	However , human-derived scores are often alarmingly inconsistent	22-34	35-56	Historically , these metrics have been evaluated using correlation with human judgment .	However , human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness .	1>2	contrast	contrast
P16-1182	22-28	29-34	Historically , these metrics have been evaluated	using correlation with human judgment .	22-34	22-34	Historically , these metrics have been evaluated using correlation with human judgment .	Historically , these metrics have been evaluated using correlation with human judgment .	1<2	manner-means	manner-means
P16-1182	35-42	57-68	However , human-derived scores are often alarmingly inconsistent	In this paper , we perform a case study for metric evaluation	35-56	57-90	However , human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness .	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	1>2	bg-compare	bg-compare
P16-1182	35-42	43-49	However , human-derived scores are often alarmingly inconsistent	and are also limited in their ability	35-56	35-56	However , human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness .	However , human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness .	1<2	joint	joint
P16-1182	43-49	50-56	and are also limited in their ability	to identify precise areas of weakness .	35-56	35-56	However , human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness .	However , human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness .	1<2	elab-addition	elab-addition
P16-1182	57-68	69-72	In this paper , we perform a case study for metric evaluation	by measuring the effect	57-90	57-90	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	1<2	manner-means	manner-means
P16-1182	69-72	73-90	by measuring the effect	that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	57-90	57-90	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	1<2	elab-addition	elab-addition
P16-1182	57-68	91-99	In this paper , we perform a case study for metric evaluation	These sentence `` corruptions '' serve as unit tests	57-90	91-111	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	These sentence `` corruptions '' serve as unit tests for precisely measuring the strengths and weaknesses of a given metric .	1<2	elab-addition	elab-addition
P16-1182	91-99	100-111	These sentence `` corruptions '' serve as unit tests	for precisely measuring the strengths and weaknesses of a given metric .	91-111	91-111	These sentence `` corruptions '' serve as unit tests for precisely measuring the strengths and weaknesses of a given metric .	These sentence `` corruptions '' serve as unit tests for precisely measuring the strengths and weaknesses of a given metric .	1<2	elab-addition	elab-addition
P16-1182	112-113	126-141	We find	but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics	112-159	112-159	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	1>2	attribution	attribution
P16-1182	114-125	126-141	that not only are human annotations heavily inconsistent in this study ,	but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics	112-159	112-159	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	1>2	contrast	contrast
P16-1182	57-68	126-141	In this paper , we perform a case study for metric evaluation	but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics	57-90	112-159	In this paper , we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations ( e.g. active to passive voice ) have on the automatic metric scores .	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	1<2	evaluation	evaluation
P16-1182	126-141	142-149	but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics	( e.g. comparing passive and active sentences )	112-159	112-159	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	1<2	elab-example	elab-example
P16-1182	126-141	150-159	but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics	better than a simple correlation with human judgment can .	112-159	112-159	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	We find that not only are human annotations heavily inconsistent in this study , but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics ( e.g. comparing passive and active sentences ) better than a simple correlation with human judgment can .	1<2	comparison	comparison
P16-1183	1-16	45-50	For many applications , the query speed of N-gram language models is a computational bottleneck .	We present the first language model	1-16	45-68	For many applications , the query speed of N-gram language models is a computational bottleneck .	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	1>2	bg-goal	bg-goal
P16-1183	17-30	31-44	Although massively parallel hardware like GPUs offer a potential solution to this bottleneck ,	exploiting this hardware requires a careful rethinking of basic algorithms and data structures .	17-44	17-44	Although massively parallel hardware like GPUs offer a potential solution to this bottleneck , exploiting this hardware requires a careful rethinking of basic algorithms and data structures .	Although massively parallel hardware like GPUs offer a potential solution to this bottleneck , exploiting this hardware requires a careful rethinking of basic algorithms and data structures .	1>2	contrast	contrast
P16-1183	1-16	31-44	For many applications , the query speed of N-gram language models is a computational bottleneck .	exploiting this hardware requires a careful rethinking of basic algorithms and data structures .	1-16	17-44	For many applications , the query speed of N-gram language models is a computational bottleneck .	Although massively parallel hardware like GPUs offer a potential solution to this bottleneck , exploiting this hardware requires a careful rethinking of basic algorithms and data structures .	1<2	elab-addition	elab-addition
P16-1183	45-50	51-55	We present the first language model	designed for such hardware ,	45-68	45-68	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	1<2	elab-addition	elab-addition
P16-1183	51-55	56-57	designed for such hardware ,	using B-trees	45-68	45-68	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	1<2	manner-means	manner-means
P16-1183	56-57	58-61	using B-trees	to maximize data parallelism	45-68	45-68	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	1<2	enablement	enablement
P16-1183	58-61	62-68	to maximize data parallelism	and minimize memory footprint and latency .	45-68	45-68	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	1<2	joint	joint
P16-1183	69-88	89-111	Compared with a single-threaded instance of KenLM ( Heafield , 2011 ) , a highly optimized CPUbased language model ,	our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task .	69-111	69-111	Compared with a single-threaded instance of KenLM ( Heafield , 2011 ) , a highly optimized CPUbased language model , our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task .	Compared with a single-threaded instance of KenLM ( Heafield , 2011 ) , a highly optimized CPUbased language model , our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task .	1>2	comparison	comparison
P16-1183	45-50	89-111	We present the first language model	our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task .	45-68	69-111	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	Compared with a single-threaded instance of KenLM ( Heafield , 2011 ) , a highly optimized CPUbased language model , our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task .	1<2	evaluation	evaluation
P16-1183	112-117	118-127	When we saturate both devices ,	the GPU delivers nearly twice the throughput per hardware dollar	112-137	112-137	When we saturate both devices , the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures .	When we saturate both devices , the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures .	1>2	condition	condition
P16-1183	45-50	118-127	We present the first language model	the GPU delivers nearly twice the throughput per hardware dollar	45-68	112-137	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	When we saturate both devices , the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures .	1<2	evaluation	evaluation
P16-1183	118-127	128-137	the GPU delivers nearly twice the throughput per hardware dollar	even when the CPU implementation uses faster data structures .	112-137	112-137	When we saturate both devices , the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures .	When we saturate both devices , the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures .	1<2	contrast	contrast
P16-1183	45-50	138-146	We present the first language model	Our implementation is freely available at https : //github.com/XapaJIaMnu/gLM	45-68	138-146	We present the first language model designed for such hardware , using B-trees to maximize data parallelism and minimize memory footprint and latency .	Our implementation is freely available at https : //github.com/XapaJIaMnu/gLM	1<2	elab-addition	elab-addition
P16-1184	1-9	19-21	Morphologically rich languages often lack the annotated linguistic resources	We propose models	1-18	19-37	Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools .	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	1>2	bg-goal	bg-goal
P16-1184	1-9	10-18	Morphologically rich languages often lack the annotated linguistic resources	required to develop accurate natural language processing tools .	1-18	1-18	Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools .	Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools .	1<2	elab-addition	elab-addition
P16-1184	19-21	22-32	We propose models	suitable for training morphological taggers with rich tagsets for low-resource languages	19-37	19-37	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	1<2	elab-addition	elab-addition
P16-1184	22-32	33-37	suitable for training morphological taggers with rich tagsets for low-resource languages	without using direct supervision .	19-37	19-37	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	1<2	condition	condition
P16-1184	19-21	38-42	We propose models	Our approach extends existing approaches	19-37	38-66	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	Our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token .	1<2	elab-addition	elab-addition
P16-1184	38-42	43-49	Our approach extends existing approaches	of projecting part-of-speech tags across languages ,	38-66	38-66	Our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token .	Our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token .	1<2	elab-addition	elab-addition
P16-1184	43-49	50-66	of projecting part-of-speech tags across languages ,	using bitext to infer constraints on the possible tags for a given word type or token .	38-66	38-66	Our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token .	Our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags for a given word type or token .	1<2	manner-means	manner-means
P16-1184	19-21	67-71	We propose models	We propose a tagging model	19-37	67-82	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	We propose a tagging model using Wsabie , a discriminative embeddingbased model with rank-based learning .	1<2	elab-addition	elab-addition
P16-1184	67-71	72-82	We propose a tagging model	using Wsabie , a discriminative embeddingbased model with rank-based learning .	67-82	67-82	We propose a tagging model using Wsabie , a discriminative embeddingbased model with rank-based learning .	We propose a tagging model using Wsabie , a discriminative embeddingbased model with rank-based learning .	1<2	manner-means	manner-means
P16-1184	67-71	83-102	We propose a tagging model	In our evaluation on 11 languages , on average this model performs on par with a baseline weakly-supervised HMM ,	67-82	83-107	We propose a tagging model using Wsabie , a discriminative embeddingbased model with rank-based learning .	In our evaluation on 11 languages , on average this model performs on par with a baseline weakly-supervised HMM , while being more scalable .	1<2	evaluation	evaluation
P16-1184	83-102	103-107	In our evaluation on 11 languages , on average this model performs on par with a baseline weakly-supervised HMM ,	while being more scalable .	83-107	83-107	In our evaluation on 11 languages , on average this model performs on par with a baseline weakly-supervised HMM , while being more scalable .	In our evaluation on 11 languages , on average this model performs on par with a baseline weakly-supervised HMM , while being more scalable .	1<2	joint	joint
P16-1184	108-110	111-115	Multilingual experiments show	that the method performs best	108-122	108-122	Multilingual experiments show that the method performs best when projecting between related language pairs .	Multilingual experiments show that the method performs best when projecting between related language pairs .	1>2	attribution	attribution
P16-1184	67-71	111-115	We propose a tagging model	that the method performs best	67-82	108-122	We propose a tagging model using Wsabie , a discriminative embeddingbased model with rank-based learning .	Multilingual experiments show that the method performs best when projecting between related language pairs .	1<2	evaluation	evaluation
P16-1184	111-115	116-122	that the method performs best	when projecting between related language pairs .	108-122	108-122	Multilingual experiments show that the method performs best when projecting between related language pairs .	Multilingual experiments show that the method performs best when projecting between related language pairs .	1<2	condition	condition
P16-1184	123-128	131-134,139-151	Despite the inherently lossy projection ,	that the morphological tags <*> improve the downstream performance of a parser by +0.6 LAS on average .	123-151	123-151	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	1>2	contrast	contrast
P16-1184	129-130	131-134,139-151	we show	that the morphological tags <*> improve the downstream performance of a parser by +0.6 LAS on average .	123-151	123-151	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	1>2	attribution	attribution
P16-1184	19-21	131-134,139-151	We propose models	that the morphological tags <*> improve the downstream performance of a parser by +0.6 LAS on average .	19-37	123-151	We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision .	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	1<2	evaluation	evaluation
P16-1184	131-134,139-151	135-138	that the morphological tags <*> improve the downstream performance of a parser by +0.6 LAS on average .	predicted by our models	123-151	123-151	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average .	1<2	elab-addition	elab-addition
P16-1185	1-14	15-25	While end-to-end neural machine translation ( NMT ) has made remarkable progress recently ,	NMT systems only rely on parallel corpora for parameter estimation .	1-25	1-25	While end-to-end neural machine translation ( NMT ) has made remarkable progress recently , NMT systems only rely on parallel corpora for parameter estimation .	While end-to-end neural machine translation ( NMT ) has made remarkable progress recently , NMT systems only rely on parallel corpora for parameter estimation .	1>2	comparison	comparison
P16-1185	15-25	56-81	NMT systems only rely on parallel corpora for parameter estimation .	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	1-25	56-81	While end-to-end neural machine translation ( NMT ) has made remarkable progress recently , NMT systems only rely on parallel corpora for parameter estimation .	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	1>2	bg-compare	bg-compare
P16-1185	26-44	45-51	Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages ,	it is appealing to exploit monolingual corpora	26-55	26-55	Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages , it is appealing to exploit monolingual corpora to improve NMT .	Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages , it is appealing to exploit monolingual corpora to improve NMT .	1>2	exp-reason	exp-reason
P16-1185	15-25	45-51	NMT systems only rely on parallel corpora for parameter estimation .	it is appealing to exploit monolingual corpora	1-25	26-55	While end-to-end neural machine translation ( NMT ) has made remarkable progress recently , NMT systems only rely on parallel corpora for parameter estimation .	Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages , it is appealing to exploit monolingual corpora to improve NMT .	1<2	elab-addition	elab-addition
P16-1185	45-51	52-55	it is appealing to exploit monolingual corpora	to improve NMT .	26-55	26-55	Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages , it is appealing to exploit monolingual corpora to improve NMT .	Since parallel corpora are usually limited in quantity , quality , and coverage , especially for low-resource languages , it is appealing to exploit monolingual corpora to improve NMT .	1<2	enablement	enablement
P16-1185	56-81	82-90	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	The central idea is to reconstruct the monolingual corpora	56-81	82-111	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	The central idea is to reconstruct the monolingual corpora using an autoencoder , in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively .	1<2	elab-addition	elab-addition
P16-1185	82-90	91-94	The central idea is to reconstruct the monolingual corpora	using an autoencoder ,	82-111	82-111	The central idea is to reconstruct the monolingual corpora using an autoencoder , in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively .	The central idea is to reconstruct the monolingual corpora using an autoencoder , in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively .	1<2	manner-means	manner-means
P16-1185	91-94	95-111	using an autoencoder ,	in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively .	82-111	82-111	The central idea is to reconstruct the monolingual corpora using an autoencoder , in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively .	The central idea is to reconstruct the monolingual corpora using an autoencoder , in which the sourceto-target and target-to-source translation models serve as the encoder and decoder , respectively .	1<2	elab-addition	elab-addition
P16-1185	112-125	126-132	Our approach can not only exploit the monolingual corpora of the target language ,	but also of the source language .	112-132	112-132	Our approach can not only exploit the monolingual corpora of the target language , but also of the source language .	Our approach can not only exploit the monolingual corpora of the target language , but also of the source language .	1>2	progression	progression
P16-1185	56-81	126-132	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	but also of the source language .	56-81	112-132	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	Our approach can not only exploit the monolingual corpora of the target language , but also of the source language .	1<2	elab-addition	elab-addition
P16-1185	133-138	139-151	Experiments on the ChineseEnglish dataset show	that our approach achieves significant improvements over state-of-the-art SMT and NMT systems .	133-151	133-151	Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems .	Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems .	1>2	attribution	attribution
P16-1185	56-81	139-151	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	that our approach achieves significant improvements over state-of-the-art SMT and NMT systems .	56-81	133-151	We propose a semisupervised approach for training NMT models on the concatenation of labeled ( parallel corpora ) and unlabeled ( monolingual corpora ) data .	Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems .	1<2	evaluation	evaluation
P16-1186	1-11	20-27	Training neural network language models over large vocabularies is computationally costly	We present a systematic comparison of neural strategies	1-19	20-50	Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney .	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	1>2	bg-goal	bg-goal
P16-1186	1-11	12-15	Training neural network language models over large vocabularies is computationally costly	compared to count-based models	1-19	1-19	Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney .	Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney .	1<2	comparison	comparison
P16-1186	12-15	16-19	compared to count-based models	such as Kneser-Ney .	1-19	1-19	Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney .	Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney .	1<2	elab-example	elab-example
P16-1186	20-27	28-34	We present a systematic comparison of neural strategies	to represent and train large vocabularies ,	20-50	20-50	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	1<2	enablement	enablement
P16-1186	28-34	35-50	to represent and train large vocabularies ,	including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	20-50	20-50	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	1<2	elab-enumember	elab-enumember
P16-1186	20-27	51-54	We present a systematic comparison of neural strategies	We extend self normalization	20-50	51-69	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax .	1<2	evaluation	evaluation
P16-1186	51-54	55-61	We extend self normalization	to be a proper estimator of likelihood	51-69	51-69	We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax .	We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax .	1<2	enablement	enablement
P16-1186	51-54	62-69	We extend self normalization	and introduce an efficient variant of softmax .	51-69	51-69	We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax .	We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax .	1<2	joint	joint
P16-1186	20-27	70-78	We present a systematic comparison of neural strategies	We evaluate each method on three popular benchmarks ,	20-50	70-92	We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax , hierarchical softmax , target sampling , noise contrastive estimation and self normalization .	We evaluate each method on three popular benchmarks , examining performance on rare words , the speed/accuracy trade-off and complementarity to Kneser-Ney .	1<2	evaluation	evaluation
P16-1186	70-78	79-92	We evaluate each method on three popular benchmarks ,	examining performance on rare words , the speed/accuracy trade-off and complementarity to Kneser-Ney .	70-92	70-92	We evaluate each method on three popular benchmarks , examining performance on rare words , the speed/accuracy trade-off and complementarity to Kneser-Ney .	We evaluate each method on three popular benchmarks , examining performance on rare words , the speed/accuracy trade-off and complementarity to Kneser-Ney .	1<2	elab-addition	elab-addition
P16-1187	1-13	22-29	Distributional semantic models ( DSMs ) are often evaluated on artificial similarity datasets	We present a large-scale multilingual evaluation of DSMs	1-21	22-47	Distributional semantic models ( DSMs ) are often evaluated on artificial similarity datasets containing single words or fully compositional phrases .	We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	1>2	bg-goal	bg-goal
P16-1187	1-13	14-21	Distributional semantic models ( DSMs ) are often evaluated on artificial similarity datasets	containing single words or fully compositional phrases .	1-21	1-21	Distributional semantic models ( DSMs ) are often evaluated on artificial similarity datasets containing single words or fully compositional phrases .	Distributional semantic models ( DSMs ) are often evaluated on artificial similarity datasets containing single words or fully compositional phrases .	1<2	elab-addition	elab-addition
P16-1187	22-29	30-47	We present a large-scale multilingual evaluation of DSMs	for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	22-47	22-47	We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	1<2	enablement	enablement
P16-1187	22-29	48-54	We present a large-scale multilingual evaluation of DSMs	We build a total of 816 DSMs	22-47	48-67	We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	We build a total of 816 DSMs and perform 2,856 evaluations using word2vec , GloVe , and PPMI-based models .	1<2	elab-addition	elab-addition
P16-1187	48-54	55-58	We build a total of 816 DSMs	and perform 2,856 evaluations	48-67	48-67	We build a total of 816 DSMs and perform 2,856 evaluations using word2vec , GloVe , and PPMI-based models .	We build a total of 816 DSMs and perform 2,856 evaluations using word2vec , GloVe , and PPMI-based models .	1<2	joint	joint
P16-1187	55-58	59-67	and perform 2,856 evaluations	using word2vec , GloVe , and PPMI-based models .	48-67	48-67	We build a total of 816 DSMs and perform 2,856 evaluations using word2vec , GloVe , and PPMI-based models .	We build a total of 816 DSMs and perform 2,856 evaluations using word2vec , GloVe , and PPMI-based models .	1<2	manner-means	manner-means
P16-1187	22-29	68-81	We present a large-scale multilingual evaluation of DSMs	In addition to the DSMs , we compare the impact of different parameters ,	22-47	68-96	We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	In addition to the DSMs , we compare the impact of different parameters , such as level of corpus preprocessing , context window size and number of dimensions .	1<2	elab-addition	elab-addition
P16-1187	68-81	82-96	In addition to the DSMs , we compare the impact of different parameters ,	such as level of corpus preprocessing , context window size and number of dimensions .	68-96	68-96	In addition to the DSMs , we compare the impact of different parameters , such as level of corpus preprocessing , context window size and number of dimensions .	In addition to the DSMs , we compare the impact of different parameters , such as level of corpus preprocessing , context window size and number of dimensions .	1<2	elab-example	elab-example
P16-1187	22-29	97-107	We present a large-scale multilingual evaluation of DSMs	The results obtained have a high correlation with human judgments ,	22-47	97-130	We present a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French .	The results obtained have a high correlation with human judgments , being comparable to or outperforming the state of the art for some datasets ( Spearman 's p=.82 for the Reddy dataset ) .	1<2	evaluation	evaluation
P16-1187	97-107	108-130	The results obtained have a high correlation with human judgments ,	being comparable to or outperforming the state of the art for some datasets ( Spearman 's p=.82 for the Reddy dataset ) .	97-130	97-130	The results obtained have a high correlation with human judgments , being comparable to or outperforming the state of the art for some datasets ( Spearman 's p=.82 for the Reddy dataset ) .	The results obtained have a high correlation with human judgments , being comparable to or outperforming the state of the art for some datasets ( Spearman 's p=.82 for the Reddy dataset ) .	1<2	comparison	comparison
P16-1188	1-8	9-16	We present a discriminative model for single-document summarization	that integrally combines compression and anaphoricity constraints .	1-16	1-16	We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints .	We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints .	1<2	elab-addition	elab-addition
P16-1188	1-8	17-21	We present a discriminative model for single-document summarization	Our model selects textual units	1-16	17-43	We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints .	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	1<2	elab-addition	elab-addition
P16-1188	17-21	22-26	Our model selects textual units	to include in the summary	17-43	17-43	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	1<2	enablement	enablement
P16-1188	22-26	27-34	to include in the summary	based on a rich set of sparse features	17-43	17-43	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	1<2	bg-general	bg-general
P16-1188	27-34	35-43	based on a rich set of sparse features	whose weights are learned on a large corpus .	17-43	17-43	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus .	1<2	elab-addition	elab-addition
P16-1188	1-8	44-53	We present a discriminative model for single-document summarization	We allow for the deletion of content within a sentence	1-16	44-77	We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints .	We allow for the deletion of content within a sentence when that deletion is licensed by compression rules ; in our framework , these are implemented as dependencies between subsentential units of text .	1<2	elab-addition	elab-addition
P16-1188	44-53	54-62	We allow for the deletion of content within a sentence	when that deletion is licensed by compression rules ;	44-77	44-77	We allow for the deletion of content within a sentence when that deletion is licensed by compression rules ; in our framework , these are implemented as dependencies between subsentential units of text .	We allow for the deletion of content within a sentence when that deletion is licensed by compression rules ; in our framework , these are implemented as dependencies between subsentential units of text .	1<2	condition	condition
P16-1188	54-62	63-77	when that deletion is licensed by compression rules ;	in our framework , these are implemented as dependencies between subsentential units of text .	44-77	44-77	We allow for the deletion of content within a sentence when that deletion is licensed by compression rules ; in our framework , these are implemented as dependencies between subsentential units of text .	We allow for the deletion of content within a sentence when that deletion is licensed by compression rules ; in our framework , these are implemented as dependencies between subsentential units of text .	1<2	elab-addition	elab-addition
P16-1188	1-8	78-83	We present a discriminative model for single-document summarization	Anaphoricity constraints then improve cross-sentence coherence	1-16	78-113	We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints .	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	1<2	elab-addition	elab-addition
P16-1188	84-85	96-103	by guaranteeing	the pronoun 's antecedent is included as well	78-113	78-113	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	1>2	attribution	attribution
P16-1188	86-90	96-103	that , for each pronoun	the pronoun 's antecedent is included as well	78-113	78-113	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	1>2	elab-addition	elab-addition
P16-1188	86-90	91-95	that , for each pronoun	included in the summary ,	78-113	78-113	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	1<2	elab-addition	elab-addition
P16-1188	78-83	96-103	Anaphoricity constraints then improve cross-sentence coherence	the pronoun 's antecedent is included as well	78-113	78-113	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	1<2	manner-means	manner-means
P16-1188	96-103	104-113	the pronoun 's antecedent is included as well	or the pronoun is rewritten as a full mention .	78-113	78-113	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that , for each pronoun included in the summary , the pronoun 's antecedent is included as well or the pronoun is rewritten as a full mention .	1<2	joint	joint
P16-1188	114-117	118-136	When trained end-to-end ,	our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality .	114-136	114-136	When trained end-to-end , our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality .	When trained end-to-end , our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality .	1>2	temporal	temporal
P16-1188	1-8	118-136	We present a discriminative model for single-document summarization	our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality .	1-16	114-136	We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints .	When trained end-to-end , our final system outperforms prior work on both ROUGE as well as on human judgments of linguistic quality .	1<2	evaluation	evaluation
P16-1189	1-7	8-15	We describe and evaluate a simple method	to extract parallel sentences from comparable corpora .	1-15	1-15	We describe and evaluate a simple method to extract parallel sentences from comparable corpora .	We describe and evaluate a simple method to extract parallel sentences from comparable corpora .	1<2	elab-addition	elab-addition
P16-1189	1-7	16-18,22-33	We describe and evaluate a simple method	The approach , <*> is based on expanded lexical sets and the Jaccard similarity coefficient .	1-15	16-33	We describe and evaluate a simple method to extract parallel sentences from comparable corpora .	The approach , termed STACC , is based on expanded lexical sets and the Jaccard similarity coefficient .	1<2	elab-addition	elab-addition
P16-1189	16-18,22-33	19-21	The approach , <*> is based on expanded lexical sets and the Jaccard similarity coefficient .	termed STACC ,	16-33	16-33	The approach , termed STACC , is based on expanded lexical sets and the Jaccard similarity coefficient .	The approach , termed STACC , is based on expanded lexical sets and the Jaccard similarity coefficient .	1<2	elab-addition	elab-addition
P16-1189	34-55	57-67	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs ,	that it either matches or outperforms current methods across the board	34-77	34-77	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	1>2	result	result
P16-1189	56	57-67	showing	that it either matches or outperforms current methods across the board	34-77	34-77	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	1>2	attribution	attribution
P16-1189	1-7	57-67	We describe and evaluate a simple method	that it either matches or outperforms current methods across the board	1-15	34-77	We describe and evaluate a simple method to extract parallel sentences from comparable corpora .	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	1<2	evaluation	evaluation
P16-1189	57-67	68-77	that it either matches or outperforms current methods across the board	and gives significantly better results on the noisiest datasets .	34-77	34-77	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	We evaluate our system against state-of-theart methods on a large range of datasets in different domains , for ten language pairs , showing that it either matches or outperforms current methods across the board and gives significantly better results on the noisiest datasets .	1<2	joint	joint
P16-1189	1-7	78-83	We describe and evaluate a simple method	STACC is a portable method ,	1-15	78-106	We describe and evaluate a simple method to extract parallel sentences from comparable corpora .	STACC is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora .	1<2	elab-addition	elab-addition
P16-1189	78-83	84-94	STACC is a portable method ,	requiring no particular adaptation for new domains or language pairs ,	78-106	78-106	STACC is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora .	STACC is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora .	1<2	elab-addition	elab-addition
P16-1189	84-94	95-106	requiring no particular adaptation for new domains or language pairs ,	thus enabling the efficient mining of parallel sentences in comparable corpora .	78-106	78-106	STACC is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora .	STACC is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora .	1<2	cause	cause
P16-1190	1-5	6-19	We propose a joint formulation	for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	1-19	1-19	We propose a joint formulation for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	We propose a joint formulation for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	1<2	enablement	enablement
P16-1190	20-23	42-46	Unlike prior work ,	our approach is oneshot :	20-62	20-62	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	1>2	comparison	comparison
P16-1190	20-23	24-31	Unlike prior work ,	which first learns the embeddings from parallel data	20-62	20-62	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	1<2	elab-addition	elab-addition
P16-1190	24-31	32-41	which first learns the embeddings from parallel data	and then plugs them in a supervised learning problem ,	20-62	20-62	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	1<2	progression	progression
P16-1190	1-5	42-46	We propose a joint formulation	our approach is oneshot :	1-19	20-62	We propose a joint formulation for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	1<2	elab-addition	elab-addition
P16-1190	42-46	47-62	our approach is oneshot :	a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	20-62	20-62	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	Unlike prior work , which first learns the embeddings from parallel data and then plugs them in a supervised learning problem , our approach is oneshot : a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss .	1<2	elab-definition	elab-definition
P16-1190	1-5	63-66	We propose a joint formulation	We present theoretical results	1-19	63-96	We propose a joint formulation for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	1<2	elab-addition	elab-addition
P16-1190	63-66	67-72	We present theoretical results	showing the limitation of Euclidean co-regularizers	63-96	63-96	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	1<2	elab-addition	elab-addition
P16-1190	67-72	73-78	showing the limitation of Euclidean co-regularizers	to increase the embedding dimension ,	63-96	63-96	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	1<2	enablement	enablement
P16-1190	67-72	79-80	showing the limitation of Euclidean co-regularizers	a limitation	63-96	63-96	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	1<2	elab-addition	elab-addition
P16-1190	79-80	81-87	a limitation	which does not exist for other co-regularizers	63-96	63-96	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	1<2	elab-addition	elab-addition
P16-1190	81-87	88-96	which does not exist for other co-regularizers	( such as the ` 1- distance ) .	63-96	63-96	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension , a limitation which does not exist for other co-regularizers ( such as the ` 1- distance ) .	1<2	elab-example	elab-example
P16-1190	97-100	101-109	Despite its simplicity ,	our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset	97-123	97-123	Despite its simplicity , our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German , with training times below 1 minute .	Despite its simplicity , our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German , with training times below 1 minute .	1>2	contrast	contrast
P16-1190	1-5	101-109	We propose a joint formulation	our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset	1-19	97-123	We propose a joint formulation for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	Despite its simplicity , our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German , with training times below 1 minute .	1<2	evaluation	evaluation
P16-1190	101-109	110-123	our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset	when transferring from English to German , with training times below 1 minute .	97-123	97-123	Despite its simplicity , our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German , with training times below 1 minute .	Despite its simplicity , our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German , with training times below 1 minute .	1<2	temporal	temporal
P16-1190	1-5	124-141	We propose a joint formulation	On the TED Corpus , we obtain the highest reported scores on 10 out of 11 languages .	1-19	124-141	We propose a joint formulation for learning task-specific cross-lingual word embeddings , along with classifiers for that task .	On the TED Corpus , we obtain the highest reported scores on 10 out of 11 languages .	1<2	evaluation	evaluation
P16-1191	1-3,7-15	41-52	Coarse-grained semantic categories <*> have proven useful for a range of downstream tasks	We present a novel joint embedding model of words and supersenses ,	1-23	41-67	Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation .	We present a novel joint embedding model of words and supersenses , providing insights into the relationship between words and supersenses in the same vector space .	1>2	bg-goal	bg-goal
P16-1191	1-3,7-15	4-6	Coarse-grained semantic categories <*> have proven useful for a range of downstream tasks	such as supersenses	1-23	1-23	Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation .	Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation .	1<2	elab-example	elab-example
P16-1191	7-15	16-23	have proven useful for a range of downstream tasks	such as question answering or machine translation .	1-23	1-23	Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation .	Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation .	1<2	elab-example	elab-example
P16-1191	1-3,7-15	24-40	Coarse-grained semantic categories <*> have proven useful for a range of downstream tasks	To date , no effort has been put into integrating the supersenses into distributional word representations .	1-23	24-40	Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation .	To date , no effort has been put into integrating the supersenses into distributional word representations .	1<2	elab-addition	elab-addition
P16-1191	41-52	53-67	We present a novel joint embedding model of words and supersenses ,	providing insights into the relationship between words and supersenses in the same vector space .	41-67	41-67	We present a novel joint embedding model of words and supersenses , providing insights into the relationship between words and supersenses in the same vector space .	We present a novel joint embedding model of words and supersenses , providing insights into the relationship between words and supersenses in the same vector space .	1<2	elab-addition	elab-addition
P16-1191	68-77	80-96	Using these embeddings in a deep neural network model ,	that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	68-96	68-96	Using these embeddings in a deep neural network model , we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	Using these embeddings in a deep neural network model , we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	1>2	manner-means	manner-means
P16-1191	78-79	80-96	we demonstrate	that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	68-96	68-96	Using these embeddings in a deep neural network model , we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	Using these embeddings in a deep neural network model , we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	1>2	attribution	attribution
P16-1191	41-52	80-96	We present a novel joint embedding model of words and supersenses ,	that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	41-67	68-96	We present a novel joint embedding model of words and supersenses , providing insights into the relationship between words and supersenses in the same vector space .	Using these embeddings in a deep neural network model , we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks .	1<2	evaluation	evaluation
P16-1192	1-11	18-29	Parsing for a wide variety of grammar formalisms can be performed	However , naive implementations of parsing by intersection are very inefficient .	1-17	18-29	Parsing for a wide variety of grammar formalisms can be performed by intersecting finite tree automata .	However , naive implementations of parsing by intersection are very inefficient .	1>2	contrast	contrast
P16-1192	1-11	12-17	Parsing for a wide variety of grammar formalisms can be performed	by intersecting finite tree automata .	1-17	1-17	Parsing for a wide variety of grammar formalisms can be performed by intersecting finite tree automata .	Parsing for a wide variety of grammar formalisms can be performed by intersecting finite tree automata .	1<2	manner-means	manner-means
P16-1192	18-29	30-32	However , naive implementations of parsing by intersection are very inefficient .	We present techniques	18-29	30-60	However , naive implementations of parsing by intersection are very inefficient .	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	1>2	bg-compare	bg-compare
P16-1192	30-32	33-41	We present techniques	that speed up tree-automata-based parsing , to the point	30-60	30-60	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	1<2	elab-addition	elab-addition
P16-1192	33-41	42-49	that speed up tree-automata-based parsing , to the point	that it becomes practically feasible on realistic data	30-60	30-60	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	1<2	elab-addition	elab-addition
P16-1192	42-49	50-60	that it becomes practically feasible on realistic data	when applied to context-free , TAG , and graph parsing .	30-60	30-60	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	1<2	condition	condition
P16-1192	30-32	61-73	We present techniques	For graph parsing , we obtain the best runtimes in the literature .	30-60	61-73	We present techniques that speed up tree-automata-based parsing , to the point that it becomes practically feasible on realistic data when applied to context-free , TAG , and graph parsing .	For graph parsing , we obtain the best runtimes in the literature .	1<2	evaluation	evaluation
P16-1193	1-5	14-24	Distributional semantics creates vectorspace representations	but their relation to semantic entailment has been less clear .	1-24	1-24	Distributional semantics creates vectorspace representations that capture many forms of semantic similarity , but their relation to semantic entailment has been less clear .	Distributional semantics creates vectorspace representations that capture many forms of semantic similarity , but their relation to semantic entailment has been less clear .	1>2	contrast	contrast
P16-1193	1-5	6-13	Distributional semantics creates vectorspace representations	that capture many forms of semantic similarity ,	1-24	1-24	Distributional semantics creates vectorspace representations that capture many forms of semantic similarity , but their relation to semantic entailment has been less clear .	Distributional semantics creates vectorspace representations that capture many forms of semantic similarity , but their relation to semantic entailment has been less clear .	1<2	elab-addition	elab-addition
P16-1193	14-24	25-29	but their relation to semantic entailment has been less clear .	We propose a vector-space model	1-24	25-41	Distributional semantics creates vectorspace representations that capture many forms of semantic similarity , but their relation to semantic entailment has been less clear .	We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment .	1>2	bg-goal	bg-goal
P16-1193	25-29	30-41	We propose a vector-space model	which provides a formal foundation for a distributional semantics of entailment .	25-41	25-41	We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment .	We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment .	1<2	elab-addition	elab-addition
P16-1193	42-46	47-60	Using a mean-field approximation ,	we develop approximate inference procedures and entailment operators over vectors of probabilities of features	42-67	42-67	Using a mean-field approximation , we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known ( versus unknown ) .	Using a mean-field approximation , we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known ( versus unknown ) .	1>2	manner-means	manner-means
P16-1193	25-29	47-60	We propose a vector-space model	we develop approximate inference procedures and entailment operators over vectors of probabilities of features	25-41	42-67	We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment .	Using a mean-field approximation , we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known ( versus unknown ) .	1<2	elab-addition	elab-addition
P16-1193	47-60	61-67	we develop approximate inference procedures and entailment operators over vectors of probabilities of features	being known ( versus unknown ) .	42-67	42-67	Using a mean-field approximation , we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known ( versus unknown ) .	Using a mean-field approximation , we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known ( versus unknown ) .	1<2	elab-addition	elab-addition
P16-1193	25-29	68-71	We propose a vector-space model	We use this framework	25-41	68-99	We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment .	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	1<2	elab-addition	elab-addition
P16-1193	68-71	72-80	We use this framework	to reinterpret an existing distributionalsemantic model ( Word2Vec )	68-99	68-99	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	1<2	enablement	enablement
P16-1193	72-80	81-93	to reinterpret an existing distributionalsemantic model ( Word2Vec )	as approximating an entailment-based model of the distributions of words in contexts ,	68-99	68-99	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	1<2	elab-addition	elab-addition
P16-1193	72-80	94-99	to reinterpret an existing distributionalsemantic model ( Word2Vec )	thereby predicting lexical entailment relations .	68-99	68-99	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	We use this framework to reinterpret an existing distributionalsemantic model ( Word2Vec ) as approximating an entailment-based model of the distributions of words in contexts , thereby predicting lexical entailment relations .	1<2	cause	cause
P16-1193	25-29	100-117	We propose a vector-space model	In both unsupervised and semi-supervised experiments on hyponymy detection , we get substantial improvements over previous results .	25-41	100-117	We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment .	In both unsupervised and semi-supervised experiments on hyponymy detection , we get substantial improvements over previous results .	1<2	evaluation	evaluation
