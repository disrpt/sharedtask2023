doc	unit1_toks	unit2_toks	unit1_txt	unit2_txt	s1_toks	s2_toks	unit1_sent	unit2_sent	dir	orig_label	label
D14-1001	1-4,11-27	28-35,48-60	Electronically available multi-modal data <*> is unprecedented in terms of its volume , variety , velocity , ( and veracity ) .	The increased interest and investment in cognitive computing <*> presents a unique opportunity for novel statistical models for natural language processing .	1-27	28-60	Electronically available multi-modal data ( primarily text and meta-data ) is unprecedented in terms of its volume , variety , velocity , ( and veracity ) .	The increased interest and investment in cognitive computing for building systems and solutions that enable and support richer human-machine interactions presents a unique opportunity for novel statistical models for natural language processing .	1>2	result	result
D14-1001	1-4,11-27	5-10	Electronically available multi-modal data <*> is unprecedented in terms of its volume , variety , velocity , ( and veracity ) .	( primarily text and meta-data )	1-27	1-27	Electronically available multi-modal data ( primarily text and meta-data ) is unprecedented in terms of its volume , variety , velocity , ( and veracity ) .	Electronically available multi-modal data ( primarily text and meta-data ) is unprecedented in terms of its volume , variety , velocity , ( and veracity ) .	1<2	elab-example	elab-example
D14-1001	28-35,48-60	61-71	The increased interest and investment in cognitive computing <*> presents a unique opportunity for novel statistical models for natural language processing .	In this talk , I will describe a journey at IBM	28-60	61-94	The increased interest and investment in cognitive computing for building systems and solutions that enable and support richer human-machine interactions presents a unique opportunity for novel statistical models for natural language processing .	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	1>2	progression	progression
D14-1001	28-35,48-60	36-40	The increased interest and investment in cognitive computing <*> presents a unique opportunity for novel statistical models for natural language processing .	for building systems and solutions	28-60	28-60	The increased interest and investment in cognitive computing for building systems and solutions that enable and support richer human-machine interactions presents a unique opportunity for novel statistical models for natural language processing .	The increased interest and investment in cognitive computing for building systems and solutions that enable and support richer human-machine interactions presents a unique opportunity for novel statistical models for natural language processing .	1<2	enablement	enablement
D14-1001	36-40	41-47	for building systems and solutions	that enable and support richer human-machine interactions	28-60	28-60	The increased interest and investment in cognitive computing for building systems and solutions that enable and support richer human-machine interactions presents a unique opportunity for novel statistical models for natural language processing .	The increased interest and investment in cognitive computing for building systems and solutions that enable and support richer human-machine interactions presents a unique opportunity for novel statistical models for natural language processing .	1<2	elab-addition	elab-addition
D14-1001	61-71	72-76	In this talk , I will describe a journey at IBM	during the past three decades	61-94	61-94	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	1<2	temporal	temporal
D14-1001	61-71	77-83	In this talk , I will describe a journey at IBM	in developing novel statistical models for NLP	61-94	61-94	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	1<2	elab-addition	elab-addition
D14-1001	77-83	84-94	in developing novel statistical models for NLP	covering statistical parsing , machine translation , and question-answering systems .	61-94	61-94	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	1<2	elab-addition	elab-addition
D14-1001	61-71	95-111	In this talk , I will describe a journey at IBM	Along with a discussion of some of the recent successes , I will discuss some difficult challenges	61-94	95-125	In this talk , I will describe a journey at IBM during the past three decades in developing novel statistical models for NLP covering statistical parsing , machine translation , and question-answering systems .	Along with a discussion of some of the recent successes , I will discuss some difficult challenges that need to be addressed to achieve more effective cognitive systems and applications .	1<2	progression	progression
D14-1001	95-111	112-116	Along with a discussion of some of the recent successes , I will discuss some difficult challenges	that need to be addressed	95-125	95-125	Along with a discussion of some of the recent successes , I will discuss some difficult challenges that need to be addressed to achieve more effective cognitive systems and applications .	Along with a discussion of some of the recent successes , I will discuss some difficult challenges that need to be addressed to achieve more effective cognitive systems and applications .	1<2	elab-addition	elab-addition
D14-1001	112-116	117-125	that need to be addressed	to achieve more effective cognitive systems and applications .	95-125	95-125	Along with a discussion of some of the recent successes , I will discuss some difficult challenges that need to be addressed to achieve more effective cognitive systems and applications .	Along with a discussion of some of the recent successes , I will discuss some difficult challenges that need to be addressed to achieve more effective cognitive systems and applications .	1<2	enablement	enablement
D14-1002	1-19	20-24	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks	designed for text analysis ,	1-45	1-45	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	1<2	elab-addition	elab-addition
D14-1002	1-19	25-28	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks	for recommending target documents	1-45	1-45	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	1<2	enablement	enablement
D14-1002	25-28	29-35	for recommending target documents	to be of interest to a user	1-45	1-45	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	1<2	enablement	enablement
D14-1002	29-35	36-40	to be of interest to a user	based on a source document	1-45	1-45	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	1<2	bg-general	bg-general
D14-1002	36-40	41-45	based on a source document	that she is reading .	1-45	1-45	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	1<2	elab-addition	elab-addition
D14-1002	1-19	46-69	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents ,	1-45	46-78	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	1<2	elab-process_step	elab-process_step
D14-1002	46-69	70-78	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents ,	which we collect from commercial Web browser logs .	46-78	46-78	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	We observe , identify , and detect naturally occurring signals of interestingness in click transitions on the Web between source and target documents , which we collect from commercial Web browser logs .	1<2	elab-addition	elab-addition
D14-1002	1-19	79-88	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks	The DSSM is trained on millions of Web transitions ,	1-45	79-121	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	1<2	elab-process_step	elab-process_step
D14-1002	79-88	89-104	The DSSM is trained on millions of Web transitions ,	and maps source-target document pairs to feature vectors in a latent space in such a way	79-121	79-121	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	1<2	progression	progression
D14-1002	89-104	105-121	and maps source-target document pairs to feature vectors in a latent space in such a way	that the distance between source documents and their corresponding interesting targets in that space is minimized .	79-121	79-121	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	The DSSM is trained on millions of Web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized .	1<2	elab-addition	elab-addition
D14-1002	122-128	141-148	The effectiveness of the DSSM is demonstrated	The results on large-scale , real-world datasets show	122-140	141-189	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1>2	cause	cause
D14-1002	122-128	129-133	The effectiveness of the DSSM is demonstrated	using two interestingness tasks :	122-140	122-140	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	1<2	manner-means	manner-means
D14-1002	129-133	134-140	using two interestingness tasks :	automatic highlighting and contextual entity search .	122-140	122-140	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	The effectiveness of the DSSM is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search .	1<2	elab-enumember	elab-enumember
D14-1002	141-148	149-158	The results on large-scale , real-world datasets show	that the semantics of documents are important for modeling interestingness	141-189	141-189	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1>2	attribution	attribution
D14-1002	1-19	149-158	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks	that the semantics of documents are important for modeling interestingness	1-45	141-189	This paper presents a deep semantic similarity model ( DSSM ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1<2	evaluation	evaluation
D14-1002	149-158	159-171	that the semantics of documents are important for modeling interestingness	and that the DSSM leads to significant quality improvement on both tasks ,	141-189	141-189	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1<2	joint	joint
D14-1002	159-171	172-178	and that the DSSM leads to significant quality improvement on both tasks ,	outperforming not only the classic document models	141-189	141-189	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1<2	elab-addition	elab-addition
D14-1002	172-178	179-183	outperforming not only the classic document models	that do not use semantics	141-189	141-189	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1<2	elab-addition	elab-addition
D14-1002	172-178	184-189	outperforming not only the classic document models	but also state-of-the-art topic models .	141-189	141-189	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	The results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the DSSM leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .	1<2	progression	progression
D14-1003	1-7	8-12	This work presents two different translation models	using recurrent neural networks .	1-12	1-12	This work presents two different translation models using recurrent neural networks .	This work presents two different translation models using recurrent neural networks .	1<2	manner-means	manner-means
D14-1003	1-7	13-19	This work presents two different translation models	The first one is a word-based approach	1-12	13-23	This work presents two different translation models using recurrent neural networks .	The first one is a word-based approach using word alignments .	1<2	elab-aspect	elab-aspect
D14-1003	13-19	20-23	The first one is a word-based approach	using word alignments .	13-23	13-23	The first one is a word-based approach using word alignments .	The first one is a word-based approach using word alignments .	1<2	manner-means	manner-means
D14-1003	1-7	24-30	This work presents two different translation models	Second , we present phrase-based translation models	1-12	24-38	This work presents two different translation models using recurrent neural networks .	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	1<2	elab-aspect	elab-aspect
D14-1003	24-30	31-38	Second , we present phrase-based translation models	that are more consistent with phrase-based decoding .	24-38	24-38	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	Second , we present phrase-based translation models that are more consistent with phrase-based decoding .	1<2	elab-addition	elab-addition
D14-1003	1-7	39-53	This work presents two different translation models	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation ,	1-12	39-72	This work presents two different translation models using recurrent neural networks .	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	1<2	progression	progression
D14-1003	39-53	54-65	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation ,	allowing us to use the full source sentence in our models ,	39-72	39-72	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	1<2	elab-addition	elab-addition
D14-1003	54-65	66-72	allowing us to use the full source sentence in our models ,	which is also of theoretical interest .	39-72	39-72	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest .	1<2	elab-addition	elab-addition
D14-1003	73-74	75-84	We demonstrate	that our translation models are capable of improving strong baselines	73-109	73-109	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	1>2	attribution	attribution
D14-1003	1-7	75-84	This work presents two different translation models	that our translation models are capable of improving strong baselines	1-12	73-109	This work presents two different translation models using recurrent neural networks .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	1<2	evaluation	evaluation
D14-1003	75-84	85-94	that our translation models are capable of improving strong baselines	already including recurrent neural language models on three tasks :	73-109	73-109	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	1<2	elab-addition	elab-addition
D14-1003	85-94	95-109	already including recurrent neural language models on three tasks :	IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	73-109	73-109	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	1<2	elab-enumember	elab-enumember
D14-1003	75-84	110-121	that our translation models are capable of improving strong baselines	We obtain gains up to 1.6 % BLEU and 1.7 % TER	73-109	110-126	We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	1<2	exp-evidence	exp-evidence
D14-1003	110-121	122-126	We obtain gains up to 1.6 % BLEU and 1.7 % TER	by rescoring 1000-best lists .	110-126	110-126	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists .	1<2	manner-means	manner-means
D14-1004	1-15	28-33	This paper investigates the use of neural networks for the acquisition of selectional preferences .	we propose a neural network model	1-15	16-47	This paper investigates the use of neural networks for the acquisition of selectional preferences .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	1>2	bg-goal	bg-goal
D14-1004	16-27	28-33	Inspired by recent advances of neural network models for NLP applications ,	we propose a neural network model	16-47	16-47	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	1>2	bg-general	bg-general
D14-1004	28-33	34-47	we propose a neural network model	that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	16-47	16-47	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	1<2	elab-addition	elab-addition
D14-1004	28-33	48-52	we propose a neural network model	The model is entirely unsupervised	16-47	48-61	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	1<2	elab-aspect	elab-aspect
D14-1004	48-52	53-61	The model is entirely unsupervised	- preferences are learned from unannotated corpus data .	48-61	48-61	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	The model is entirely unsupervised - preferences are learned from unannotated corpus data .	1<2	exp-reason	exp-reason
D14-1004	28-33	62-68	we propose a neural network model	We propose two neural network architectures :	16-47	62-87	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	1<2	elab-aspect	elab-aspect
D14-1004	62-68	69-75	We propose two neural network architectures :	one that handles standard two-way selectional preferences	62-87	62-87	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	1<2	elab-enumember	elab-enumember
D14-1004	69-75	76-87	one that handles standard two-way selectional preferences	and one that is able to deal with multi-way selectional preferences .	62-87	62-87	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	We propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences .	1<2	joint	joint
D14-1004	88-97	98-110	The model's performance is evaluated on a pseudo-disambiguation task ,	on which it is shown to achieve state of the art performance .	88-110	88-110	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	1>2	manner-means	manner-means
D14-1004	28-33	98-110	we propose a neural network model	on which it is shown to achieve state of the art performance .	16-47	88-110	Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate .	The model's performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .	1<2	evaluation	evaluation
D14-1005	1-5	6-19	We construct multi-modal concept representations	by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed	1-42	1-42	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	1<2	manner-means	manner-means
D14-1005	6-19	20-33	by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed	using the feature extraction layers of a deep convolutional neural network ( CNN )	1-42	1-42	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	1<2	manner-means	manner-means
D14-1005	20-33	34-42	using the feature extraction layers of a deep convolutional neural network ( CNN )	trained on a large labeled object recognition dataset .	1-42	1-42	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	1<2	elab-addition	elab-addition
D14-1005	1-5	43-53	We construct multi-modal concept representations	This transfer learning approach brings a clear performance gain over features	1-42	43-60	We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset .	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	1<2	evaluation	evaluation
D14-1005	43-53	54-60	This transfer learning approach brings a clear performance gain over features	based on the traditional bag-of-visual-word approach .	43-60	43-60	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	1<2	bg-general	bg-general
D14-1005	43-53	61-74	This transfer learning approach brings a clear performance gain over features	Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks .	43-60	61-74	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks .	1<2	manner-means	manner-means
D14-1005	43-53	75-78	This transfer learning approach brings a clear performance gain over features	We use visual features	43-60	75-87	This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach .	We use visual features computed using either ImageNet or ESP Game images .	1<2	elab-addition	elab-addition
D14-1005	75-78	79-87	We use visual features	computed using either ImageNet or ESP Game images .	75-87	75-87	We use visual features computed using either ImageNet or ESP Game images .	We use visual features computed using either ImageNet or ESP Game images .	1<2	elab-addition	elab-addition
D14-1006	1-9	10-18	In this paper , we present a novel approach	for identifying argumentative discourse structures in persuasive essays .	1-18	1-18	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	1<2	enablement	enablement
D14-1006	10-18	19-26	for identifying argumentative discourse structures in persuasive essays .	The structure of argumentation consists of several components	1-18	19-39	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	1<2	elab-addition	elab-addition
D14-1006	19-26	27-32	The structure of argumentation consists of several components	( i.e. claims and premises )	19-39	19-39	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	1<2	elab-example	elab-example
D14-1006	19-26	33-39	The structure of argumentation consists of several components	that are connected with argumentative relations .	19-39	19-39	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	The structure of argumentation consists of several components ( i.e. claims and premises ) that are connected with argumentative relations .	1<2	elab-addition	elab-addition
D14-1006	1-9	40-48	In this paper , we present a novel approach	We consider this task in two consecutive steps .	1-18	40-48	In this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays .	We consider this task in two consecutive steps .	1<2	elab-addition	elab-addition
D14-1006	40-48	49-56	We consider this task in two consecutive steps .	First , we identify the components of arguments	40-48	49-60	We consider this task in two consecutive steps .	First , we identify the components of arguments using multiclass classification .	1<2	elab-process_step	elab-process_step
D14-1006	49-56	57-60	First , we identify the components of arguments	using multiclass classification .	49-60	49-60	First , we identify the components of arguments using multiclass classification .	First , we identify the components of arguments using multiclass classification .	1<2	manner-means	manner-means
D14-1006	40-48	61-74	We consider this task in two consecutive steps .	Second , we classify a pair of argument components as either support or non-support	40-48	61-82	We consider this task in two consecutive steps .	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	1<2	elab-process_step	elab-process_step
D14-1006	61-74	75-82	Second , we classify a pair of argument components as either support or non-support	for identifying the structure of argumentative discourse .	61-82	61-82	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	Second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse .	1<2	enablement	enablement
D14-1006	40-48	83-90	We consider this task in two consecutive steps .	For both tasks , we evaluate several classifiers	40-48	83-105	We consider this task in two consecutive steps .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	1<2	evaluation	evaluation
D14-1006	83-90	91-95	For both tasks , we evaluate several classifiers	and propose novel feature sets	83-105	83-105	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	1<2	progression	progression
D14-1006	91-95	96-105	and propose novel feature sets	including structural , lexical , syntactic and contextual features .	83-105	83-105	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	1<2	elab-enumember	elab-enumember
D14-1006	83-90	106-116	For both tasks , we evaluate several classifiers	In our experiments , we obtain a macro F1-score of 0.726	83-105	106-126	For both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	1<2	result	result
D14-1006	106-116	117-126	In our experiments , we obtain a macro F1-score of 0.726	for identifying argument components and 0.722 for argumentative relations .	106-126	106-126	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	In our experiments , we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .	1<2	elab-addition	elab-addition
D14-1007	1-21	22-27	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System	built on a distributed architecture .	1-27	1-27	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	1<2	elab-addition	elab-addition
D14-1007	1-21	28-45	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification ,	1-27	28-55	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	1<2	elab-addition	elab-addition
D14-1007	28-45	46-55	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification ,	such that confirmation and clarification interaction mechanisms are supported .	28-55	28-55	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	1<2	cause	cause
D14-1007	56-61	71-79	In addition , it is shown	the extensibility of the system can be preserved ,	56-98	56-98	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	1>2	attribution	attribution
D14-1007	62-70	71-79	that by using a model parameter tying trick ,	the extensibility of the system can be preserved ,	56-98	56-98	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	1>2	manner-means	manner-means
D14-1007	28-45	71-79	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification ,	the extensibility of the system can be preserved ,	28-55	56-98	In the proposed framework , the domain selection problem is treated as sequential planning instead of classification , such that confirmation and clarification interaction mechanisms are supported .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	1<2	progression	progression
D14-1007	71-79	80-91	the extensibility of the system can be preserved ,	where dialogue components in new domains can be easily plugged in ,	56-98	56-98	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	1<2	elab-addition	elab-addition
D14-1007	80-91	92-98	where dialogue components in new domains can be easily plugged in ,	without re-training the domain selection policy .	56-98	56-98	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	In addition , it is shown that by using a model parameter tying trick , the extensibility of the system can be preserved , where dialogue components in new domains can be easily plugged in , without re-training the domain selection policy .	1<2	condition	condition
D14-1007	99-101	102-105	The experimental results	based on human subjects	99-116	99-116	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	1<2	bg-general	bg-general
D14-1007	99-101,106	107-116	<*> The experimental results <*> suggest	that the proposed model marginally outperforms a non-trivial baseline .	99-116	99-116	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	1>2	attribution	attribution
D14-1007	1-21	107-116	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System	that the proposed model marginally outperforms a non-trivial baseline .	1-27	99-116	This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multi-domain Spoken Dialogue System built on a distributed architecture .	The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline .	1<2	evaluation	evaluation
D14-1008	1-6	16-38	Discourse parsing is a challenging task	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	1-15	16-38	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	1>2	result	result
D14-1008	1-6	7-15	Discourse parsing is a challenging task	and plays a critical role in discourse analysis .	1-15	1-15	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	Discourse parsing is a challenging task and plays a critical role in discourse analysis .	1<2	progression	progression
D14-1008	16-38	53-66	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	In this paper , we propose a novel constituent-based approach to argument labeling ,	16-38	53-78	In this paper , we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank ( PDTB ) .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	1>2	bg-goal	bg-goal
D14-1008	39-52	53-66	Previous studies cast this task as a linear tagging or subtree extraction problem .	In this paper , we propose a novel constituent-based approach to argument labeling ,	39-52	53-78	Previous studies cast this task as a linear tagging or subtree extraction problem .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	1>2	bg-compare	bg-compare
D14-1008	53-66	67-78	In this paper , we propose a novel constituent-based approach to argument labeling ,	which integrates the advantages of both linear tagging and subtree extraction .	53-78	53-78	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	1<2	elab-addition	elab-addition
D14-1008	53-66	79-89	In this paper , we propose a novel constituent-based approach to argument labeling ,	In particular , the proposed approach unifies intra- and inter-sentence cases	53-78	79-100	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	1<2	elab-aspect	elab-aspect
D14-1008	79-89	90-100	In particular , the proposed approach unifies intra- and inter-sentence cases	by treating the immediately preceding sentence as a special constituent .	79-100	79-100	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	In particular , the proposed approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent .	1<2	manner-means	manner-means
D14-1008	53-66	101-108	In this paper , we propose a novel constituent-based approach to argument labeling ,	Besides , a joint inference mechanism is introduced	53-78	101-123	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	1<2	elab-aspect	elab-aspect
D14-1008	101-108	109-118	Besides , a joint inference mechanism is introduced	to incorporate global information across arguments into our constituent-based approach	101-123	101-123	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	1<2	enablement	enablement
D14-1008	109-118	119-123	to incorporate global information across arguments into our constituent-based approach	via integer linear programming .	101-123	101-123	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	Besides , a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming .	1<2	manner-means	manner-means
D14-1008	53-66	124-140	In this paper , we propose a novel constituent-based approach to argument labeling ,	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	53-78	124-140	In this paper , we propose a novel constituent-based approach to argument labeling , which integrates the advantages of both linear tagging and subtree extraction .	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	1<2	evaluation	evaluation
D14-1008	124-140	141-150	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	It also shows the effectiveness of our joint inference mechanism	124-140	141-157	Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	1<2	joint	joint
D14-1008	141-150	151-157	It also shows the effectiveness of our joint inference mechanism	in modeling global information across arguments .	141-157	141-157	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments .	1<2	elab-addition	elab-addition
D14-1009	1-12	13-16	We present STIR ( STrongly Incremental Repair detection ) , a system	that detects speech repairs	1-26	1-26	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	1<2	elab-addition	elab-addition
D14-1009	13-16	17-26	that detects speech repairs	and edit terms on transcripts incrementally with minimal latency .	1-26	1-26	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	1<2	joint	joint
D14-1009	1-12	27-43	We present STIR ( STrongly Incremental Repair detection ) , a system	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers	1-26	27-50	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	1<2	manner-means	manner-means
D14-1009	27-43	44-50	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers	detecting the different stages of repairs .	27-50	27-50	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs .	1<2	elab-addition	elab-addition
D14-1009	1-12	51-83	We present STIR ( STrongly Incremental Repair detection ) , a system	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	1-26	51-83	We present STIR ( STrongly Incremental Repair detection ) , a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency .	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	1<2	evaluation	evaluation
D14-1009	51-83	84-87	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	We evaluate its performance	51-83	84-98	Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods , but with better incremental accuracy , faster time-to-detection and less computational overhead .	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	1<2	manner-means	manner-means
D14-1009	84-87	88-90	We evaluate its performance	using incremental metrics	84-98	84-98	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	1<2	manner-means	manner-means
D14-1009	84-87	91-98	We evaluate its performance	and propose new repair processing evaluation standards .	84-98	84-98	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	We evaluate its performance using incremental metrics and propose new repair processing evaluation standards .	1<2	progression	progression
D14-1010	1-4	29-39	There is rich knowledge	In this paper we adopt partial-label learning with conditional random fields	1-10	29-52	There is rich knowledge encoded in online web data .	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	1>2	bg-goal	bg-goal
D14-1010	1-4	5-10	There is rich knowledge	encoded in online web data .	1-10	1-10	There is rich knowledge encoded in online web data .	There is rich knowledge encoded in online web data .	1<2	elab-addition	elab-addition
D14-1010	1-4	11-28	There is rich knowledge	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	1-10	11-28	There is rich knowledge encoded in online web data .	For example , punctuation and entity tags in Wikipedia data define some word boundaries in a sentence .	1<2	elab-example	elab-example
D14-1010	29-39	40-52	In this paper we adopt partial-label learning with conditional random fields	to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	29-52	29-52	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	1<2	enablement	enablement
D14-1010	29-39	53-64	In this paper we adopt partial-label learning with conditional random fields	The basic idea of partial-label learning is to optimize a cost function	29-52	53-78	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	1<2	elab-addition	elab-addition
D14-1010	53-64	65-73	The basic idea of partial-label learning is to optimize a cost function	that marginalizes the probability mass in the constrained space	53-78	53-78	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	1<2	elab-addition	elab-addition
D14-1010	65-73	74-78	that marginalizes the probability mass in the constrained space	that encodes this knowledge .	53-78	53-78	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge .	1<2	elab-addition	elab-addition
D14-1010	79-89	90-120	By integrating some domain adaptation techniques , such as EasyAdapt ,	our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	79-120	79-120	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	1>2	manner-means	manner-means
D14-1010	29-39	90-120	In this paper we adopt partial-label learning with conditional random fields	our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	29-52	79-120	In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation .	By integrating some domain adaptation techniques , such as EasyAdapt , our result reaches an F-measure of 95.98 % on the CTB-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .	1<2	evaluation	evaluation
D14-1011	1-10	11-28	Microblogs have recently received widespread interest from NLP researchers .	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	1-10	11-28	Microblogs have recently received widespread interest from NLP researchers .	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	1>2	contrast	contrast
D14-1011	11-28	29-33	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	We developed an annotated corpus	11-28	29-43	However , current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	1>2	bg-compare	bg-compare
D14-1011	29-33	34-38	We developed an annotated corpus	and proposed a joint model	29-43	29-43	We developed an annotated corpus and proposed a joint model for over-coming this situation .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	1<2	progression	progression
D14-1011	34-38	39-43	and proposed a joint model	for over-coming this situation .	29-43	29-43	We developed an annotated corpus and proposed a joint model for over-coming this situation .	We developed an annotated corpus and proposed a joint model for over-coming this situation .	1<2	enablement	enablement
D14-1011	29-33	44-57	We developed an annotated corpus	Our annotated corpus of microblog texts enables not only training of accurate statistical models	29-43	44-65	We developed an annotated corpus and proposed a joint model for over-coming this situation .	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	1<2	elab-aspect	elab-aspect
D14-1011	44-57	58-65	Our annotated corpus of microblog texts enables not only training of accurate statistical models	but also quantitative evaluation of their performance .	44-65	44-65	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance .	1<2	progression	progression
D14-1011	29-33	66-79	We developed an annotated corpus	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	29-43	66-79	We developed an annotated corpus and proposed a joint model for over-coming this situation .	Our joint model with lexical normalization handles the orthographic diversity of microblog texts .	1<2	elab-aspect	elab-aspect
D14-1011	80-83	84-85	We conducted an experiment	to demonstrate	80-96	80-96	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	1>2	enablement	enablement
D14-1011	84-85	86-96	to demonstrate	that the corpus and model substantially contribute to boosting accuracy .	80-96	80-96	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	1>2	attribution	attribution
D14-1011	29-33	86-96	We developed an annotated corpus	that the corpus and model substantially contribute to boosting accuracy .	29-43	80-96	We developed an annotated corpus and proposed a joint model for over-coming this situation .	We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy .	1<2	evaluation	evaluation
D14-1012	1-5	33-36,50-51	Recent work has shown success	However , fundamental problems <*> remain .	1-32	33-51	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	1>2	result	result
D14-1012	1-5	6-10	Recent work has shown success	in using continuous word embeddings	1-32	1-32	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	1<2	elab-addition	elab-addition
D14-1012	6-10	11-14	in using continuous word embeddings	learned from unlabeled data	1-32	1-32	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	1<2	elab-addition	elab-addition
D14-1012	11-14	15-16	learned from unlabeled data	as features	1-32	1-32	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	1<2	comparison	comparison
D14-1012	6-10	17-22	in using continuous word embeddings	to improve supervised NLP systems ,	1-32	1-32	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	1<2	enablement	enablement
D14-1012	17-22	23-32	to improve supervised NLP systems ,	which is regarded as a simple semi-supervised learning mechanism .	1-32	1-32	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems , which is regarded as a simple semi-supervised learning mechanism .	1<2	elab-addition	elab-addition
D14-1012	33-36,50-51	52-63	However , fundamental problems <*> remain .	In this study , we investigate and analyze three different approaches ,	33-51	52-77	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	1>2	bg-compare	bg-compare
D14-1012	33-36,50-51	37-49	However , fundamental problems <*> remain .	on effectively incorporating the word embedding features within the framework of linear models	33-51	33-51	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	However , fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain .	1<2	elab-addition	elab-addition
D14-1012	52-63	64-71	In this study , we investigate and analyze three different approaches ,	including a new proposed distributional prototype approach ,	52-77	52-77	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	1<2	elab-example	elab-example
D14-1012	64-71	72-77	including a new proposed distributional prototype approach ,	for utilizing the embedding features .	52-77	52-77	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	1<2	enablement	enablement
D14-1012	52-63	78-93	In this study , we investigate and analyze three different approaches ,	The presented approaches can be integrated into most of the classical linear models in NLP .	52-77	78-93	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	The presented approaches can be integrated into most of the classical linear models in NLP .	1<2	elab-addition	elab-addition
D14-1012	94-102	103-116	Experiments on the task of named entity recognition show	that each of the proposed approaches can better utilize the word embedding features ,	94-126	94-126	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	1>2	manner-means	manner-means
D14-1012	52-63	103-116	In this study , we investigate and analyze three different approaches ,	that each of the proposed approaches can better utilize the word embedding features ,	52-77	94-126	In this study , we investigate and analyze three different approaches , including a new proposed distributional prototype approach , for utilizing the embedding features .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	1<2	evaluation	evaluation
D14-1012	103-116	117-126	that each of the proposed approaches can better utilize the word embedding features ,	among which the distributional prototype approach performs the best .	94-126	94-126	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	1<2	comparison	comparison
D14-1012	103-116	127-137	that each of the proposed approaches can better utilize the word embedding features ,	Moreover , the combination of the approaches provides additive improvements ,	94-126	127-152	Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features , among which the distributional prototype approach performs the best .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	1<2	progression	progression
D14-1012	127-137	138-152	Moreover , the combination of the approaches provides additive improvements ,	outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	127-152	127-152	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	Moreover , the combination of the approaches provides additive improvements , outperforming the dense and continuous embedding features by nearly 2 points of F1 score .	1<2	elab-addition	elab-addition
D14-1013	1-20	21-34	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	Combining the two tasks can potentially improve the efficiency of the overall pipeline system	1-20	21-39	Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction .	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	1>2	result	result
D14-1013	21-34	40-47	Combining the two tasks can potentially improve the efficiency of the overall pipeline system	In this work , we compare various methods	21-39	40-65	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	1>2	bg-goal	bg-goal
D14-1013	21-34	35-39	Combining the two tasks can potentially improve the efficiency of the overall pipeline system	and reduce error propagation .	21-39	21-39	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation .	1<2	joint	joint
D14-1013	40-47	48-65	In this work , we compare various methods	for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	40-65	40-65	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	1<2	enablement	enablement
D14-1013	40-47	66-86	In this work , we compare various methods	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	40-65	66-86	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	1<2	elab-addition	elab-addition
D14-1013	87-93	94-106	For the cascade approach , we show	that the soft cascade method is better than the hard cascade method .	87-106	87-106	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	1>2	attribution	attribution
D14-1013	66-86	94-106	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	that the soft cascade method is better than the hard cascade method .	66-86	87-106	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	For the cascade approach , we show that the soft cascade method is better than the hard cascade method .	1<2	elab-aspect	elab-aspect
D14-1013	66-86	107-112	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	We also use the cascade models	66-86	107-138	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	1<2	elab-aspect	elab-aspect
D14-1013	107-112	113-118	We also use the cascade models	to generate an n-best list ,	107-138	107-138	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	1<2	enablement	enablement
D14-1013	107-112	119-123	We also use the cascade models	use the bi-directional cascade models	107-138	107-138	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	1<2	joint	joint
D14-1013	119-123	124-127	use the bi-directional cascade models	to perform rescoring ,	107-138	107-138	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	1<2	enablement	enablement
D14-1013	107-112	128-138	We also use the cascade models	and compare that with the results of the cascade models .	107-138	107-138	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	We also use the cascade models to generate an n-best list , use the bi-directional cascade models to perform rescoring , and compare that with the results of the cascade models .	1<2	joint	joint
D14-1013	66-86	139-170	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	66-86	139-170	We compare an isolated prediction approach with a cascade approach , a rescoring approach , and three joint model approaches .	For the joint model approach , we compare mixed-label Linear-chain Conditional Random Field ( LCRF ) , cross-product LCRF and 2-layer Factorial Conditional Random Field ( FCRF ) with soft-cascade LCRF .	1<2	elab-aspect	elab-aspect
D14-1013	171-173	174-177,182-189	Our results show	that the various methods <*> are not significantly different from one another ,	171-206	171-206	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	1>2	attribution	attribution
D14-1013	40-47	174-177,182-189	In this work , we compare various methods	that the various methods <*> are not significantly different from one another ,	40-65	171-206	In this work , we compare various methods for combining punctuation prediction ( PU ) and disfluency prediction ( DF ) on the Switchboard corpus .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	1<2	evaluation	evaluation
D14-1013	174-177,182-189	178-181	that the various methods <*> are not significantly different from one another ,	linking the two tasks	171-206	171-206	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	1<2	elab-addition	elab-addition
D14-1013	174-177,182-189	190-206	that the various methods <*> are not significantly different from one another ,	although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	171-206	171-206	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	1<2	contrast	contrast
D14-1013	174-177,182-189	207-219	that the various methods <*> are not significantly different from one another ,	Moreover , the clique order of features also shows a marked difference .	171-206	207-219	Our results show that the various methods linking the two tasks are not significantly different from one another , although they perform better than the isolated prediction method by 0.5-1.5 % in the F1 score .	Moreover , the clique order of features also shows a marked difference .	1<2	progression	progression
D14-1014	1-7	8-20	We introduce submodular optimization to the problem	of training data subset selection for statistical machine translation ( SMT ) .	1-20	1-20	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	1<2	elab-addition	elab-addition
D14-1014	21-30	31-41	By explicitly formulating data selection as a submodular program ,	we obtain fast scalable selection algorithms with mathematical performance guarantees ,	21-62	21-62	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	1>2	manner-means	manner-means
D14-1014	1-7	31-41	We introduce submodular optimization to the problem	we obtain fast scalable selection algorithms with mathematical performance guarantees ,	1-20	21-62	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	1<2	elab-aspect	elab-aspect
D14-1014	31-41	42-46	we obtain fast scalable selection algorithms with mathematical performance guarantees ,	resulting in a unified framework	21-62	21-62	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	1<2	result	result
D14-1014	42-46	47-50	resulting in a unified framework	that clarifies existing approaches	21-62	21-62	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	1<2	elab-addition	elab-addition
D14-1014	31-41	51-62	we obtain fast scalable selection algorithms with mathematical performance guarantees ,	and also makes both new and many previous approaches easily accessible .	21-62	21-62	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	By explicitly formulating data selection as a submodular program , we obtain fast scalable selection algorithms with mathematical performance guarantees , resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible .	1<2	joint	joint
D14-1014	1-7	63-70	We introduce submodular optimization to the problem	We present a new class of submodular functions	1-20	63-83	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	1<2	elab-aspect	elab-aspect
D14-1014	63-70	71-74	We present a new class of submodular functions	designed specifically for SMT	63-83	63-83	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	1<2	elab-addition	elab-addition
D14-1014	63-70	75-83	We present a new class of submodular functions	and evaluate them on two different translation tasks .	63-83	63-83	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks .	1<2	joint	joint
D14-1014	84-86	87-97	Our results show	that our best submodular method significantly outperforms several baseline methods ,	84-106	84-106	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	1>2	attribution	attribution
D14-1014	1-7	87-97	We introduce submodular optimization to the problem	that our best submodular method significantly outperforms several baseline methods ,	1-20	84-106	We introduce submodular optimization to the problem of training data subset selection for statistical machine translation ( SMT ) .	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	1<2	evaluation	evaluation
D14-1014	87-97	98-106	that our best submodular method significantly outperforms several baseline methods ,	including the widely-used cross-entropy based data selection method .	84-106	84-106	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	1<2	elab-example	elab-example
D14-1014	87-97	107-117	that our best submodular method significantly outperforms several baseline methods ,	In addition , our approach easily scales to large data sets	84-106	107-130	Our results show that our best submodular method significantly outperforms several baseline methods , including the widely-used cross-entropy based data selection method .	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	1<2	progression	progression
D14-1014	107-117	118-130	In addition , our approach easily scales to large data sets	and is applicable to other data selection problems in natural language processing .	107-130	107-130	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	In addition , our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing .	1<2	joint	joint
D14-1015	1-2	55-61	We investigate	we propose a simple and memory-efficient model	1-24	49-79	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	1>2	bg-goal	bg-goal
D14-1015	1-2	3-7	We investigate	how to improve bilingual embedding	1-24	1-24	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	1<2	attribution	attribution
D14-1015	3-7	8-24	how to improve bilingual embedding	which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	1-24	1-24	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) .	1<2	elab-addition	elab-addition
D14-1015	25-29	30-33,43-48	Despite bilingual embedding's success ,	the contextual information , <*> was ignored in previous work .	25-48	25-48	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	1>2	contrast	contrast
D14-1015	30-33,43-48	55-61	the contextual information , <*> was ignored in previous work .	we propose a simple and memory-efficient model	25-48	49-79	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	1>2	bg-compare	bg-compare
D14-1015	30-33,43-48	34-42	the contextual information , <*> was ignored in previous work .	which is of critical importance to translation quality ,	25-48	25-48	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work .	1<2	elab-addition	elab-addition
D14-1015	49-54	55-61	To employ the contextual information ,	we propose a simple and memory-efficient model	49-79	49-79	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	1>2	enablement	enablement
D14-1015	55-61	62-66	we propose a simple and memory-efficient model	for learning bilingual embedding ,	49-79	49-79	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	1<2	enablement	enablement
D14-1015	55-61	67-79	we propose a simple and memory-efficient model	taking both the source phrase and context around the phrase into account .	49-79	49-79	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	1<2	condition	condition
D14-1015	55-61	80-82,90-98	we propose a simple and memory-efficient model	Bilingual translation scores <*> are used as features in our SMT system .	49-79	80-98	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	1<2	elab-addition	elab-addition
D14-1015	80-82,90-98	83-89	Bilingual translation scores <*> are used as features in our SMT system .	generated from our proposed bilingual embedding model	80-98	80-98	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system .	1<2	elab-addition	elab-addition
D14-1015	99-101	102-114	Experimental results show	that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	99-114	99-114	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	1>2	attribution	attribution
D14-1015	55-61	102-114	we propose a simple and memory-efficient model	that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	49-79	99-114	To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account .	Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task .	1<2	evaluation	evaluation
D14-1016	1-5	69-78	We present a novel approach	In this paper , we use a weighted vote method	1-17	69-96	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	1>2	bg-goal	bg-goal
D14-1016	1-5	6-17	We present a novel approach	to improve word alignment for statistical machine translation ( SMT ) .	1-17	1-17	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	We present a novel approach to improve word alignment for statistical machine translation ( SMT ) .	1<2	enablement	enablement
D14-1016	18-25	69-78	Conventional word alignment methods allow discontinuous alignment ,	In this paper , we use a weighted vote method	18-48	69-96	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	1>2	bg-compare	bg-compare
D14-1016	26	27-43	meaning	that a source ( or target ) word links to several target ( or source ) words	18-48	18-48	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	1>2	attribution	attribution
D14-1016	18-25	27-43	Conventional word alignment methods allow discontinuous alignment ,	that a source ( or target ) word links to several target ( or source ) words	18-48	18-48	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	1<2	elab-addition	elab-addition
D14-1016	27-43	44-48	that a source ( or target ) word links to several target ( or source ) words	whose positions are discontinuous .	18-48	18-48	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	1<2	elab-addition	elab-addition
D14-1016	18-25	49-60	Conventional word alignment methods allow discontinuous alignment ,	However , we cannot extract phrase pairs from this kind of alignments	18-48	49-68	Conventional word alignment methods allow discontinuous alignment , meaning that a source ( or target ) word links to several target ( or source ) words whose positions are discontinuous .	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	1<2	contrast	contrast
D14-1016	49-60	61-68	However , we cannot extract phrase pairs from this kind of alignments	as they break the alignment consistency constraint .	49-68	49-68	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	However , we cannot extract phrase pairs from this kind of alignments as they break the alignment consistency constraint .	1<2	exp-reason	exp-reason
D14-1016	69-78	79-87	In this paper , we use a weighted vote method	to transform discontinuous word alignment to continuous alignment ,	69-96	69-96	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	1<2	enablement	enablement
D14-1016	79-87	88-96	to transform discontinuous word alignment to continuous alignment ,	which enables SMT systems extract more phrase pairs .	69-96	69-96	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	1<2	elab-addition	elab-addition
D14-1016	97-109	113-126	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	statistically significant improvements of BLEU score in both cases over the baseline systems .	97-109	110-126	We carry out experiments on large scale Chinese-to-English and German-to-English translation tasks .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	1>2	manner-means	manner-means
D14-1016	110-112	113-126	Experimental results show	statistically significant improvements of BLEU score in both cases over the baseline systems .	110-126	110-126	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	1>2	attribution	attribution
D14-1016	69-78	113-126	In this paper , we use a weighted vote method	statistically significant improvements of BLEU score in both cases over the baseline systems .	69-96	110-126	In this paper , we use a weighted vote method to transform discontinuous word alignment to continuous alignment , which enables SMT systems extract more phrase pairs .	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	1<2	evaluation	evaluation
D14-1016	113-126	127-157	statistically significant improvements of BLEU score in both cases over the baseline systems .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	110-126	127-157	Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems .	Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system , and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system .	1<2	exp-evidence	exp-evidence
D14-1017	1-16	52-69	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment ,	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 )	1-27	52-103	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	1>2	bg-compare	bg-compare
D14-1017	1-16	17-27	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment ,	and cannot explicitly represent many-to-many relationships in a bilingual text .	1-27	1-27	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	1<2	joint	joint
D14-1017	1-16	28-32	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment ,	The problem is partially solved	1-27	28-51	Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text .	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	1<2	elab-addition	elab-addition
D14-1017	28-32	33-40	The problem is partially solved	either by introducing heuristics or by agreement constraints	28-51	28-51	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	1<2	manner-means	manner-means
D14-1017	33-40	41-51	either by introducing heuristics or by agreement constraints	such that two directional word alignments agree with each other .	28-51	28-51	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other .	1<2	elab-example	elab-example
D14-1017	52-69	70-82	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 )	that can force two directional word alignment models to agree with each other	52-103	52-103	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	1<2	elab-addition	elab-addition
D14-1017	70-82	83-85	that can force two directional word alignment models to agree with each other	during training ,	52-103	52-103	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	1<2	temporal	temporal
D14-1017	70-82	86-89	that can force two directional word alignment models to agree with each other	and propose new constraints	52-103	52-103	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	1<2	joint	joint
D14-1017	86-89	90-103	and propose new constraints	that can take into account the difference between function words and content words .	52-103	52-103	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	1<2	elab-addition	elab-addition
D14-1017	52-69	104-122	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 )	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	52-103	104-122	In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words .	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	1<2	evaluation	evaluation
D14-1017	104-122	123-131	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	We also observed gains in Japanese-to-English translation tasks ,	104-122	123-144	Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline .	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	1<2	joint	joint
D14-1017	123-131	132-144	We also observed gains in Japanese-to-English translation tasks ,	which prove the effectiveness of our methods under grammatically different language pairs .	123-144	123-144	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs .	1<2	elab-addition	elab-addition
D14-1018	1-22	64-79	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	most of the prior research has focused on monolingual features of translated and original text .	1-22	48-79	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	1>2	result	result
D14-1018	1-22	23-30	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	In recent years computational methods have been developed	1-22	23-47	Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) .	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	1<2	elab-addition	elab-addition
D14-1018	23-30	31-47	In recent years computational methods have been developed	to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	23-47	23-47	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) .	1<2	enablement	enablement
D14-1018	48-56	64-79	While many characteristics of translated text are more apparent	most of the prior research has focused on monolingual features of translated and original text .	48-79	48-79	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	1>2	contrast	contrast
D14-1018	48-56	57-63	While many characteristics of translated text are more apparent	in comparison to the original text ,	48-79	48-79	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	1<2	comparison	comparison
D14-1018	64-79	80-88	most of the prior research has focused on monolingual features of translated and original text .	The contribution of this work is introducing bilingual features	48-79	80-118	While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	1>2	bg-compare	bg-compare
D14-1018	80-88	89-97	The contribution of this work is introducing bilingual features	that are capable of explaining differences in translation direction	80-118	80-118	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	1<2	elab-addition	elab-addition
D14-1018	89-97	98-108	that are capable of explaining differences in translation direction	using localized linguistic phenomena at the phrase or sentence level ,	80-118	80-118	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	1<2	manner-means	manner-means
D14-1018	98-108	109-118	using localized linguistic phenomena at the phrase or sentence level ,	rather than using monolingual statistics at the document level .	80-118	80-118	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	1<2	contrast	contrast
D14-1018	119-120	121-128	We show	that these bilingual features out-perform the monolingual features	119-147	119-147	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	1>2	attribution	attribution
D14-1018	80-88	121-128	The contribution of this work is introducing bilingual features	that these bilingual features out-perform the monolingual features	80-118	119-147	The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	1<2	evaluation	evaluation
D14-1018	121-128	129-142	that these bilingual features out-perform the monolingual features	used in prior work ( Kurokawa et al. , 2009 ) for the task	119-147	119-147	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	1<2	manner-means	manner-means
D14-1018	129-142	143-147	used in prior work ( Kurokawa et al. , 2009 ) for the task	of classifying translation direction .	119-147	119-147	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction .	1<2	elab-addition	elab-addition
D14-1019	1-7	53-62	Recently , syntactic information has helped significantly	In this paper , we propose a syntax-label clustering method	1-13	53-81	Recently , syntactic information has helped significantly to improve statistical machine translation .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	1>2	comparison	comparison
D14-1019	1-7	8-13	Recently , syntactic information has helped significantly	to improve statistical machine translation .	1-13	1-13	Recently , syntactic information has helped significantly to improve statistical machine translation .	Recently , syntactic information has helped significantly to improve statistical machine translation .	1<2	enablement	enablement
D14-1019	1-7	14-30	Recently , syntactic information has helped significantly	However , the use of syntactic information may have a negative impact on the speed of translation	1-13	14-52	Recently , syntactic information has helped significantly to improve statistical machine translation .	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	1<2	contrast	contrast
D14-1019	14-30	31-38	However , the use of syntactic information may have a negative impact on the speed of translation	because of the large number of rules ,	14-52	14-52	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	1<2	exp-reason	exp-reason
D14-1019	14-30	39-52	However , the use of syntactic information may have a negative impact on the speed of translation	especially when syntax labels are projected from a parser in syntax-augmented machine translation .	14-52	14-52	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	However , the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules , especially when syntax labels are projected from a parser in syntax-augmented machine translation .	1<2	elab-example	elab-example
D14-1019	53-62	63-67	In this paper , we propose a syntax-label clustering method	that uses an exchange algorithm	53-81	53-81	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	1<2	elab-addition	elab-addition
D14-1019	63-67	68-74	that uses an exchange algorithm	in which syntax labels are clustered together	53-81	53-81	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	1<2	elab-addition	elab-addition
D14-1019	68-74	75-81	in which syntax labels are clustered together	to reduce the number of rules .	53-81	53-81	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	1<2	enablement	enablement
D14-1019	53-62	82-86	In this paper , we propose a syntax-label clustering method	The proposed method achieves clustering	53-81	82-108	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	1<2	elab-addition	elab-addition
D14-1019	82-86	87-95	The proposed method achieves clustering	by directly maximizing the likelihood of synchronous rules ,	82-108	82-108	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	1<2	manner-means	manner-means
D14-1019	87-95	96-108	by directly maximizing the likelihood of synchronous rules ,	whereas previous work considered only the similarity of probabilistic distributions of labels .	82-108	82-108	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules , whereas previous work considered only the similarity of probabilistic distributions of labels .	1<2	contrast	contrast
D14-1019	109-119	120-125	We tested the proposed method on Japanese-English and Chinese-English translation tasks	and found order-of-magnitude higher clustering speeds	109-139	109-139	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	1>2	manner-means	manner-means
D14-1019	53-62	120-125	In this paper , we propose a syntax-label clustering method	and found order-of-magnitude higher clustering speeds	53-81	109-139	In this paper , we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	1<2	elab-addition	elab-addition
D14-1019	120-125	126-128	and found order-of-magnitude higher clustering speeds	for reducing labels	109-139	109-139	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	1<2	enablement	enablement
D14-1019	120-125	129-133	and found order-of-magnitude higher clustering speeds	and gains in translation quality	109-139	109-139	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	1<2	joint	joint
D14-1019	129-133	134-139	and gains in translation quality	compared with previous clustering method .	109-139	109-139	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method .	1<2	comparison	comparison
D14-1020	1-15	39-48	Automatic metrics are widely used in machine translation as a substitute for human assessment .	This is often measured by correlation with human judgment .	1-15	39-48	Automatic metrics are widely used in machine translation as a substitute for human assessment .	This is often measured by correlation with human judgment .	1>2	result	result
D14-1020	1-15	16-25	Automatic metrics are widely used in machine translation as a substitute for human assessment .	With the introduction of any new metric comes the question	1-15	16-38	Automatic metrics are widely used in machine translation as a substitute for human assessment .	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	1<2	elab-addition	elab-addition
D14-1020	16-25	26-38	With the introduction of any new metric comes the question	of just how well that metric mimics human assessment of translation quality .	16-38	16-38	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality .	1<2	elab-addition	elab-addition
D14-1020	39-48	77-85	This is often measured by correlation with human judgment .	In this paper , we introduce a significance test	39-48	77-101	This is often measured by correlation with human judgment .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	1>2	bg-compare	bg-compare
D14-1020	39-48	49-54	This is often measured by correlation with human judgment .	Significance tests are generally not used	39-48	49-76	This is often measured by correlation with human judgment .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	1<2	elab-addition	elab-addition
D14-1020	49-54	55-56	Significance tests are generally not used	to establish	49-76	49-76	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	1<2	enablement	enablement
D14-1020	55-56	57-67	to establish	whether improvements over existing methods such as BLEU are statistically significant	49-76	49-76	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	1<2	attribution	attribution
D14-1020	57-67	68-76	whether improvements over existing methods such as BLEU are statistically significant	or have occurred simply by chance , however .	49-76	49-76	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however .	1<2	joint	joint
D14-1020	77-85	86-101	In this paper , we introduce a significance test	for comparing correlations of two metrics , along with an open-source implementation of the test .	77-101	77-101	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	1<2	enablement	enablement
D14-1020	102-113	116-127	When applied to a range of metrics across seven language pairs ,	that for a high proportion of metrics , there is insufficient evidence	102-134	102-134	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	1>2	condition	condition
D14-1020	114-115	116-127	tests show	that for a high proportion of metrics , there is insufficient evidence	102-134	102-134	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	1>2	attribution	attribution
D14-1020	77-85	116-127	In this paper , we introduce a significance test	that for a high proportion of metrics , there is insufficient evidence	77-101	102-134	In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	1<2	evaluation	evaluation
D14-1020	116-127	128-134	that for a high proportion of metrics , there is insufficient evidence	to conclude significant improvement over BLEU .	102-134	102-134	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU .	1<2	enablement	enablement
D14-1021	10-19	20-28	In contrast to the dominant approach in the literature ,	the system does not rely on translation rules ,	10-55	10-55	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	1>2	contrast	contrast
D14-1021	1-9	20-28	We study a novel architecture for syntactic SMT .	the system does not rely on translation rules ,	1-9	10-55	We study a novel architecture for syntactic SMT .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	1<2	elab-addition	elab-addition
D14-1021	20-28	29-39	the system does not rely on translation rules ,	but treat translation as an unconstrained target sentence generation task ,	10-55	10-55	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	1<2	progression	progression
D14-1021	29-39	40-42	but treat translation as an unconstrained target sentence generation task ,	using soft features	10-55	10-55	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	1<2	manner-means	manner-means
D14-1021	40-42	43-55	using soft features	to capture lexical and syntactic correspondences between the source and target languages .	10-55	10-55	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	1<2	enablement	enablement
D14-1021	43-55	56-70	to capture lexical and syntactic correspondences between the source and target languages .	Target syntax features and bilingual translation features are trained consistently in a discriminative model .	10-55	56-70	In contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages .	Target syntax features and bilingual translation features are trained consistently in a discriminative model .	1<2	elab-addition	elab-addition
D14-1021	71-76	78-90	Experiments using the IWSLT 2010 dataset	that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	71-90	71-90	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	1>2	manner-means	manner-means
D14-1021	77	78-90	show	that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	71-90	71-90	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	1>2	attribution	attribution
D14-1021	1-9	78-90	We study a novel architecture for syntactic SMT .	that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	1-9	71-90	We study a novel architecture for syntactic SMT .	Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems .	1<2	evaluation	evaluation
D14-1022	1-7	8-18	We propose a simple and effective approach	to learn translation spans for the hierarchical phrase-based translation model .	1-18	1-18	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	1<2	enablement	enablement
D14-1022	1-7	19-21	We propose a simple and effective approach	Our model evaluates	1-18	19-45	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	1<2	elab-aspect	elab-aspect
D14-1022	19-21	22-31	Our model evaluates	if a source span should be covered by translation rules	19-45	19-45	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	1<2	attribution	attribution
D14-1022	22-31	32-34	if a source span should be covered by translation rules	during decoding ,	19-45	19-45	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	1<2	temporal	temporal
D14-1022	32-34	35-45	during decoding ,	which is integrated into the translation system as soft constraints .	19-45	19-45	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	Our model evaluates if a source span should be covered by translation rules during decoding , which is integrated into the translation system as soft constraints .	1<2	elab-addition	elab-addition
D14-1022	46-50	51-60	Compared to syntactic constraints ,	our model is directly acquired from an aligned parallel corpus	46-66	46-66	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	1>2	comparison	comparison
D14-1022	1-7	51-60	We propose a simple and effective approach	our model is directly acquired from an aligned parallel corpus	1-18	46-66	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	1<2	elab-aspect	elab-aspect
D14-1022	51-60	61-66	our model is directly acquired from an aligned parallel corpus	and does not require parsers .	46-66	46-66	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	Compared to syntactic constraints , our model is directly acquired from an aligned parallel corpus and does not require parsers .	1<2	progression	progression
D14-1022	1-7	67-83	We propose a simple and effective approach	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	1-18	67-83	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	Rich source side contextual features and advanced machine learning methods were utilized for this learning task .	1<2	elab-aspect	elab-aspect
D14-1022	84-95	96-104	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks	and showed significant improvement over the baseline system .	84-104	84-104	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	1>2	manner-means	manner-means
D14-1022	1-7	96-104	We propose a simple and effective approach	and showed significant improvement over the baseline system .	1-18	84-104	We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model .	The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system .	1<2	evaluation	evaluation
D14-1023	1-19	20-32	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) ,	how to construct efficient large LM is an important topic in SMT .	1-32	1-32	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	1>2	exp-reason	exp-reason
D14-1023	20-32	33-47	how to construct efficient large LM is an important topic in SMT .	However , most of the existing LM growing methods need an extra monolingual corpus ,	1-32	33-55	Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT .	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	1>2	contrast	contrast
D14-1023	33-47	56-71	However , most of the existing LM growing methods need an extra monolingual corpus ,	In this paper , we propose a novel neural network based bilingual LM growing method ,	33-55	56-80	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	1>2	bg-compare	bg-compare
D14-1023	33-47	48-55	However , most of the existing LM growing methods need an extra monolingual corpus ,	where additional LM adaption technology is necessary .	33-55	33-55	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary .	1<2	elab-addition	elab-addition
D14-1023	56-71	72-80	In this paper , we propose a novel neural network based bilingual LM growing method ,	only using the bilingual parallel corpus in SMT .	56-80	56-80	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	1<2	elab-addition	elab-addition
D14-1023	81-83	84-101	The results show	that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT ,	81-113	81-113	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	1>2	attribution	attribution
D14-1023	56-71	84-101	In this paper , we propose a novel neural network based bilingual LM growing method ,	that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT ,	56-80	81-113	In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	1<2	evaluation	evaluation
D14-1023	84-101	102-109	that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT ,	and significantly outperforms the existing LM growing methods	81-113	81-113	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	1<2	joint	joint
D14-1023	102-109	110-113	and significantly outperforms the existing LM growing methods	without extra corpus .	81-113	81-113	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus .	1<2	condition	condition
D14-1024	1-7	8-20	This article describes a linguistically informed method	for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	1-20	1-20	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	1<2	enablement	enablement
D14-1024	21-24	33-41	In a case study	that our method does not only improve translation quality	21-53	21-53	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	1>2	bg-general	bg-general
D14-1024	21-24	25-30	In a case study	involving English to Bulgarian SMT ,	21-53	21-53	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	1<2	elab-example	elab-example
D14-1024	31-32	33-41	we show	that our method does not only improve translation quality	21-53	21-53	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	1>2	attribution	attribution
D14-1024	1-7	33-41	This article describes a linguistically informed method	that our method does not only improve translation quality	1-20	21-53	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	1<2	elab-addition	elab-addition
D14-1024	33-41	42-46	that our method does not only improve translation quality	but also outperforms similar methods	21-53	21-53	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	1<2	progression	progression
D14-1024	42-46	47-53	but also outperforms similar methods	previously applied to the same task .	21-53	21-53	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	1<2	elab-addition	elab-addition
D14-1024	60-70	54-59,71-76	that , in contrast to previous work on the subject ,	<*> We attribute this to the fact <*> we employ detailed linguistic information .	54-76	54-76	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	1>2	attribution	attribution
D14-1024	33-41	54-59,71-76	that our method does not only improve translation quality	<*> We attribute this to the fact <*> we employ detailed linguistic information .	21-53	54-76	In a case study involving English to Bulgarian SMT , we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task .	We attribute this to the fact that , in contrast to previous work on the subject , we employ detailed linguistic information .	1<2	exp-reason	exp-reason
D14-1024	77-79	80-81,90-96	We found out	that features <*> contribute most to the better translation quality	77-101	77-101	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	1>2	attribution	attribution
D14-1024	1-7	80-81,90-96	This article describes a linguistically informed method	that features <*> contribute most to the better translation quality	1-20	77-101	This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation ( SMT ) systems .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	1<2	evaluation	evaluation
D14-1024	80-81,90-96	82-89	that features <*> contribute most to the better translation quality	which describe phrasal verbs as idiomatic or compositional	77-101	77-101	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	1<2	elab-addition	elab-addition
D14-1024	90-96	97-101	contribute most to the better translation quality	achieved by our method .	77-101	77-101	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method .	1<2	manner-means	manner-means
D14-1025	1-16	17-27	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	Existing sentence level metrics employ a limited set of features ,	1-16	17-48	Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation .	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	1>2	cause	cause
D14-1025	17-27	49-55	Existing sentence level metrics employ a limited set of features ,	This paper presents a simple linear model	17-48	49-82	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	1>2	bg-compare	bg-compare
D14-1025	17-27	28-38	Existing sentence level metrics employ a limited set of features ,	most of which are rather sparse at the sentence level ,	17-48	17-48	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	1<2	elab-addition	elab-addition
D14-1025	28-38	39-45	most of which are rather sparse at the sentence level ,	and their intricate models are rarely trained	17-48	17-48	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	1<2	joint	joint
D14-1025	39-45	46-48	and their intricate models are rarely trained	for ranking .	17-48	17-48	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	Existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking .	1<2	enablement	enablement
D14-1025	49-55	56-61	This paper presents a simple linear model	exploiting 33 relatively dense features ,	49-82	49-82	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	1<2	elab-addition	elab-addition
D14-1025	56-61	62-66	exploiting 33 relatively dense features ,	some of which are novel	49-82	49-82	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	1<2	elab-example	elab-example
D14-1025	62-66	67-70	some of which are novel	while others are known	49-82	49-82	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	1<2	contrast	contrast
D14-1025	67-70	71-74	while others are known	but seldom used ,	49-82	49-82	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	1<2	contrast	contrast
D14-1025	49-55	75-82	This paper presents a simple linear model	and train it under the learning-to-rank framework .	49-82	49-82	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	1<2	progression	progression
D14-1025	83-91	93-100	We evaluate our metric on the standard WMT12 data	that it outperforms the strong baseline METEOR .	83-100	83-100	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	1>2	manner-means	manner-means
D14-1025	92	93-100	showing	that it outperforms the strong baseline METEOR .	83-100	83-100	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	1>2	attribution	attribution
D14-1025	49-55	93-100	This paper presents a simple linear model	that it outperforms the strong baseline METEOR .	49-82	83-100	This paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework .	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	1<2	evaluation	evaluation
D14-1025	93-100	101-120	that it outperforms the strong baseline METEOR .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data ,	83-100	101-127	We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	1<2	progression	progression
D14-1025	101-120	121-127	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data ,	providing new insights into this task .	101-127	101-127	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	We also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .	1<2	enablement	enablement
D14-1026	1-6	7-17	We present a human judgments dataset	and an adapted metric for evaluation of Arabic machine translation .	1-17	1-17	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	1<2	joint	joint
D14-1026	1-6	18-33	We present a human judgments dataset	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	1-17	18-33	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	Our mediumscale dataset is the first of its kind for Arabic with high annotation quality .	1<2	evaluation	evaluation
D14-1026	1-6	34-37	We present a human judgments dataset	We use the dataset	1-17	34-45	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We use the dataset to adapt the BLEU score for Arabic .	1<2	elab-addition	elab-addition
D14-1026	34-37	38-45	We use the dataset	to adapt the BLEU score for Arabic .	34-45	34-45	We use the dataset to adapt the BLEU score for Arabic .	We use the dataset to adapt the BLEU score for Arabic .	1<2	enablement	enablement
D14-1026	38-45	46-64	to adapt the BLEU score for Arabic .	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	34-45	46-64	We use the dataset to adapt the BLEU score for Arabic .	Our score ( AL-BLEU ) provides partial credits for stem and morphological matchings of hypothesis and reference words .	1<2	elab-addition	elab-addition
D14-1026	65-76	79-88	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus	that AL-BLEU has the highest correlation with human judgments .	65-88	65-88	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	1>2	manner-means	manner-means
D14-1026	77-78	79-88	and show	that AL-BLEU has the highest correlation with human judgments .	65-88	65-88	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	1>2	attribution	attribution
D14-1026	1-6	79-88	We present a human judgments dataset	that AL-BLEU has the highest correlation with human judgments .	1-17	65-88	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We evaluate BLEU , METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments .	1<2	evaluation	evaluation
D14-1026	1-6	89-100	We present a human judgments dataset	We are releasing the dataset and software to the research community .	1-17	89-100	We present a human judgments dataset and an adapted metric for evaluation of Arabic machine translation .	We are releasing the dataset and software to the research community .	1<2	elab-aspect	elab-aspect
D14-1027	1-10	11-26	We present a pairwise learning-to-rank approach to machine translation evaluation	that learns to differentiate better from worse translations in the context of a given reference .	1-26	1-26	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	1<2	elab-addition	elab-addition
D14-1027	1-10	27-33	We present a pairwise learning-to-rank approach to machine translation evaluation	We integrate several layers of linguistic information	1-26	27-61	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	1<2	elab-aspect	elab-aspect
D14-1027	27-33	34-38	We integrate several layers of linguistic information	encapsulated in tree-based structures ,	27-61	27-61	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	1<2	elab-addition	elab-addition
D14-1027	34-38	39-50	encapsulated in tree-based structures ,	making use of both the reference and the system output simultaneously ,	27-61	27-61	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	1<2	enablement	enablement
D14-1027	39-50	51-61	making use of both the reference and the system output simultaneously ,	thus bringing our ranking closer to how humans evaluate translations .	27-61	27-61	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	We integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations .	1<2	result	result
D14-1027	62-68	76-84	Most importantly , instead of deciding upfront	we use the learning framework of preference re-ranking kernels	62-90	62-90	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	1>2	contrast	contrast
D14-1027	62-68	69-75	Most importantly , instead of deciding upfront	which types of features are important ,	62-90	62-90	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	1<2	attribution	attribution
D14-1027	1-10	76-84	We present a pairwise learning-to-rank approach to machine translation evaluation	we use the learning framework of preference re-ranking kernels	1-26	62-90	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	1<2	elab-aspect	elab-aspect
D14-1027	76-84	85-90	we use the learning framework of preference re-ranking kernels	to learn the features automatically .	62-90	62-90	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	Most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically .	1<2	enablement	enablement
D14-1027	91-94	95-105	The evaluation results show	that learning in the proposed framework yields better correlation with humans	91-117	91-117	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	1>2	attribution	attribution
D14-1027	1-10	95-105	We present a pairwise learning-to-rank approach to machine translation evaluation	that learning in the proposed framework yields better correlation with humans	1-26	91-117	We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference .	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	1<2	evaluation	evaluation
D14-1027	95-105	106-117	that learning in the proposed framework yields better correlation with humans	than computing the direct similarity over the same type of structures .	91-117	91-117	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	1<2	comparison	comparison
D14-1027	118-121	122-137	Also , we show	our structural kernel learning ( SKL ) can be a general framework for MT evaluation ,	118-148	118-148	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	1>2	attribution	attribution
D14-1027	95-105	122-137	that learning in the proposed framework yields better correlation with humans	our structural kernel learning ( SKL ) can be a general framework for MT evaluation ,	91-117	118-148	The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	1<2	joint	joint
D14-1027	122-137	138-148	our structural kernel learning ( SKL ) can be a general framework for MT evaluation ,	in which syntactic and semantic information can be naturally incorporated .	118-148	118-148	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	Also , we show our structural kernel learning ( SKL ) can be a general framework for MT evaluation , in which syntactic and semantic information can be naturally incorporated .	1<2	elab-addition	elab-addition
D14-1028	1-23	84-91	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero )	This paper introduces two improvements to LR decoding	1-40	84-102	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	1>2	bg-goal	bg-goal
D14-1028	1-23	24-30	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero )	that visits input spans in arbitrary order	1-40	1-40	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	1<2	elab-addition	elab-addition
D14-1028	1-23	31-40	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero )	producing the output translation in left to right order .	1-40	1-40	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order .	1<2	elab-addition	elab-addition
D14-1028	41-49	84-91	This leads to far fewer language model calls ,	This paper introduces two improvements to LR decoding	41-83	84-102	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	1>2	bg-compare	bg-compare
D14-1028	41-49	50-60	This leads to far fewer language model calls ,	but while LR decoding is more efficient than CKY decoding ,	41-83	41-83	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	1<2	contrast	contrast
D14-1028	50-60	61-70	but while LR decoding is more efficient than CKY decoding ,	it is unable to capture some hierarchical phrase alignments reachable	41-83	41-83	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	1<2	contrast	contrast
D14-1028	61-70	71-73	it is unable to capture some hierarchical phrase alignments reachable	using CKY decoding	41-83	41-83	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	1<2	condition	condition
D14-1028	61-70	74-83	it is unable to capture some hierarchical phrase alignments reachable	and suffers from lower translation quality as a result .	41-83	41-83	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result .	1<2	joint	joint
D14-1028	84-91	92-102	This paper introduces two improvements to LR decoding	that make it comparable in translation quality to CKY-based Hiero .	84-102	84-102	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero .	1<2	elab-addition	elab-addition
D14-1029	1-20	21-33	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	We predict reordering probabilities for every pair of source words with a model	1-20	21-42	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	1<2	elab-aspect	elab-aspect
D14-1029	21-33	34-35	We predict reordering probabilities for every pair of source words with a model	using features	21-42	21-42	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	1<2	manner-means	manner-means
D14-1029	34-35	36-42	using features	observed from the input parse forest .	21-42	21-42	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest .	1<2	elab-addition	elab-addition
D14-1029	1-20	43-56	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	Our approach naturally deals with the ambiguity present in the input parse forest ,	1-20	43-80	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	1<2	elab-aspect	elab-aspect
D14-1029	43-56	57-73	Our approach naturally deals with the ambiguity present in the input parse forest ,	but , at the same time , takes into account only the parts of the input forest	43-80	43-80	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	1<2	contrast	contrast
D14-1029	57-73	74-80	but , at the same time , takes into account only the parts of the input forest	used by the current translation hypothesis .	43-80	43-80	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	Our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis .	1<2	elab-addition	elab-addition
D14-1029	1-20	81-90	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	The method provides improvement from 0.6 up to 1.0 point	1-20	81-100	In this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model .	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	1<2	evaluation	evaluation
D14-1029	81-90	91-100	The method provides improvement from 0.6 up to 1.0 point	measured by ( Ter  Bleu ) /2 metric .	81-100	81-100	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	The method provides improvement from 0.6 up to 1.0 point measured by ( Ter  Bleu ) /2 metric .	1<2	manner-means	manner-means
D14-1030	1-9	129-149	Many statistical models for natural language processing exist ,	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) ,	1-59	129-167	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	1>2	result	result
D14-1030	1-9	10-13	Many statistical models for natural language processing exist ,	including context-based neural networks	1-59	1-59	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	1<2	elab-example	elab-example
D14-1030	10-13	14-28	including context-based neural networks	that ( 1 ) model the previously seen context as a latent feature vector ,	1-59	1-59	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	1<2	elab-addition	elab-addition
D14-1030	14-28	29-37	that ( 1 ) model the previously seen context as a latent feature vector ,	( 2 ) integrate successive words into the context	1-59	1-59	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	1<2	joint	joint
D14-1030	29-37	38-45	( 2 ) integrate successive words into the context	using some learned representation ( embedding ) ,	1-59	1-59	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	1<2	manner-means	manner-means
D14-1030	29-37	46-55	( 2 ) integrate successive words into the context	and ( 3 ) compute output probabilities for incoming words	1-59	1-59	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	1<2	joint	joint
D14-1030	46-55	56-59	and ( 3 ) compute output probabilities for incoming words	given the context .	1-59	1-59	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	1<2	condition	condition
D14-1030	1-9	60-69	Many statistical models for natural language processing exist ,	On the other hand , brain imaging studies have suggested	1-59	60-128	Many statistical models for natural language processing exist , including context-based neural networks that ( 1 ) model the previously seen context as a latent feature vector , ( 2 ) integrate successive words into the context using some learned representation ( embedding ) , and ( 3 ) compute output probabilities for incoming words given the context .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1<2	joint	joint
D14-1030	70-73	74-86	that during reading ,	the brain ( a ) continuously builds a context from the successive words	60-128	60-128	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1>2	temporal	temporal
D14-1030	60-69	74-86	On the other hand , brain imaging studies have suggested	the brain ( a ) continuously builds a context from the successive words	60-128	60-128	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1<2	elab-aspect	elab-aspect
D14-1030	74-86	87-93	the brain ( a ) continuously builds a context from the successive words	and every time it encounters a word	60-128	60-128	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1<2	elab-addition	elab-addition
D14-1030	60-69	94-102	On the other hand , brain imaging studies have suggested	it ( b ) fetches its properties from memory	60-128	60-128	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1<2	elab-aspect	elab-aspect
D14-1030	60-69	103-117	On the other hand , brain imaging studies have suggested	and ( c ) integrates it with the previous context with a degree of effort	60-128	60-128	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1<2	elab-aspect	elab-aspect
D14-1030	103-117	118-128	and ( c ) integrates it with the previous context with a degree of effort	that is inversely proportional to how probable the word is .	60-128	60-128	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	On the other hand , brain imaging studies have suggested that during reading , the brain ( a ) continuously builds a context from the successive words and every time it encounters a word it ( b ) fetches its properties from memory and ( c ) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is .	1<2	elab-addition	elab-addition
D14-1030	129-149	168-171	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) ,	We explore this parallelism	129-167	168-183	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	We explore this parallelism to better understand the brain processes and the neural networks representations .	1>2	bg-goal	bg-goal
D14-1030	129-149	150-158	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) ,	representing the incoming words ( 2 and b )	129-167	129-167	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	1<2	joint	joint
D14-1030	150-158	159-167	representing the incoming words ( 2 and b )	and integrating it ( 3 and c ) .	129-167	129-167	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	This hints to a parallelism between the neural networks and the brain in modeling context ( 1 and a ) , representing the incoming words ( 2 and b ) and integrating it ( 3 and c ) .	1<2	joint	joint
D14-1030	168-171	172-183	We explore this parallelism	to better understand the brain processes and the neural networks representations .	168-183	168-183	We explore this parallelism to better understand the brain processes and the neural networks representations .	We explore this parallelism to better understand the brain processes and the neural networks representations .	1<2	enablement	enablement
D14-1030	168-171	184-191	We explore this parallelism	We study the alignment between the latent vectors	168-183	184-210	We explore this parallelism to better understand the brain processes and the neural networks representations .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	1<2	joint	joint
D14-1030	184-191	192-198	We study the alignment between the latent vectors	used by neural networks and brain activity	184-210	184-210	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	1<2	elab-addition	elab-addition
D14-1030	192-198	199-204	used by neural networks and brain activity	observed via Magnetoencephalography ( MEG )	184-210	184-210	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	1<2	elab-addition	elab-addition
D14-1030	184-191	205-210	We study the alignment between the latent vectors	when subjects read a story .	184-210	184-210	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography ( MEG ) when subjects read a story .	1<2	temporal	temporal
D14-1030	168-171	211-222	We explore this parallelism	For that purpose we apply the neural network to the same text	168-183	211-244	We explore this parallelism to better understand the brain processes and the neural networks representations .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	1<2	result	result
D14-1030	211-222	223-227	For that purpose we apply the neural network to the same text	the subjects are reading ,	211-244	211-244	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	1<2	elab-addition	elab-addition
D14-1030	211-222	228-236	For that purpose we apply the neural network to the same text	and explore the ability of these three vector representations	211-244	211-244	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	1<2	joint	joint
D14-1030	228-236	237-244	and explore the ability of these three vector representations	to predict the observed word-by-word brain activity .	211-244	211-244	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	For that purpose we apply the neural network to the same text the subjects are reading , and explore the ability of these three vector representations to predict the observed word-by-word brain activity .	1<2	enablement	enablement
D14-1030	245-248	249-258	Our novel results show	that : before a new word i is read ,	245-289	245-289	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	1>2	attribution	attribution
D14-1030	168-171	249-258	We explore this parallelism	that : before a new word i is read ,	168-183	245-289	We explore this parallelism to better understand the brain processes and the neural networks representations .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	1<2	elab-aspect	elab-aspect
D14-1030	249-258	259-271	that : before a new word i is read ,	brain activity is well predicted by the neural network latent representation of context	245-289	245-289	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	1<2	elab-aspect	elab-aspect
D14-1030	259-271	272-275	brain activity is well predicted by the neural network latent representation of context	and the predictability decreases	245-289	245-289	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	1<2	joint	joint
D14-1030	272-275	276-281	and the predictability decreases	as the brain integrates the word	245-289	245-289	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	1<2	temporal	temporal
D14-1030	276-281	282-289	as the brain integrates the word	and changes its own representation of context .	245-289	245-289	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	1<2	joint	joint
D14-1030	249-258	290-303	that : before a new word i is read ,	Secondly , the neural network embedding of word i can predict the MEG activity	245-289	290-326	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	1<2	elab-aspect	elab-aspect
D14-1030	290-303	304-312	Secondly , the neural network embedding of word i can predict the MEG activity	when word i is presented to the subject ,	290-326	290-326	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	1<2	temporal	temporal
D14-1030	290-303	313	Secondly , the neural network embedding of word i can predict the MEG activity	revealing	290-326	290-326	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	1<2	elab-addition	elab-addition
D14-1030	313	314-326	revealing	that it is correlated with the brain's own representation of word i .	290-326	290-326	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	Secondly , the neural network embedding of word i can predict the MEG activity when word i is presented to the subject , revealing that it is correlated with the brain's own representation of word i .	1<2	attribution	attribution
D14-1030	249-258	327-330	that : before a new word i is read ,	Moreover , we obtain	245-289	327-345	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	1<2	elab-aspect	elab-aspect
D14-1030	327-330	331-345	Moreover , we obtain	that the activity is predicted in different regions of the brain with varying delay .	327-345	327-345	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	1<2	attribution	attribution
D14-1030	331-345	346-359	that the activity is predicted in different regions of the brain with varying delay .	The delay is consistent with the placement of each region on the processing pathway	327-345	346-372	Moreover , we obtain that the activity is predicted in different regions of the brain with varying delay .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	1<2	elab-addition	elab-addition
D14-1030	346-359	360-365	The delay is consistent with the placement of each region on the processing pathway	that starts in the visual cortex	346-372	346-372	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	1<2	elab-addition	elab-addition
D14-1030	360-365	366-372	that starts in the visual cortex	and moves to higher level regions .	346-372	346-372	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions .	1<2	joint	joint
D14-1030	249-258	373-376	that : before a new word i is read ,	Finally , we show	245-289	373-432	Our novel results show that : before a new word i is read , brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	elab-aspect	elab-aspect
D14-1030	373-376	377-380,386-398	Finally , we show	that the output probability <*> agrees with the brain's own assessment of the probability of word i ,	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	attribution	attribution
D14-1030	377-380,386-398	381-385	that the output probability <*> agrees with the brain's own assessment of the probability of word i ,	computed by the neural networks	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	elab-addition	elab-addition
D14-1030	386-398	399-403	agrees with the brain's own assessment of the probability of word i ,	as it can be used	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	exp-reason	exp-reason
D14-1030	399-403	404-408	as it can be used	to predict the brain activity	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	enablement	enablement
D14-1030	404-408	409-418	to predict the brain activity	after the word i's properties have been fetched from memory	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	temporal	temporal
D14-1030	399-403	419-425	as it can be used	and the brain is in the process	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	joint	joint
D14-1030	419-425	426-432	and the brain is in the process	of integrating it into the context .	373-432	373-432	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	Finally , we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i , as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context .	1<2	elab-addition	elab-addition
D14-1031	1-18	37-45	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	We present an algorithm for simultaneously learning word meanings	1-18	37-65	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	1>2	bg-goal	bg-goal
D14-1031	1-18	19-31	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	A presumed outcome of semantic development is the formation of a semantic network	1-18	19-36	Child semantic development includes learning the meaning of words as well as the semantic relations among words .	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	1<2	elab-addition	elab-addition
D14-1031	19-31	32-36	A presumed outcome of semantic development is the formation of a semantic network	that reflects this knowledge .	19-36	19-36	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge .	1<2	elab-addition	elab-addition
D14-1031	37-45	46-52	We present an algorithm for simultaneously learning word meanings	and gradually growing a semantic network ,	37-65	37-65	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	1<2	joint	joint
D14-1031	46-52	53-65	and gradually growing a semantic network ,	which adheres to the cognitive plausibility requirements of incrementality and limited computations .	37-65	37-65	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	1<2	elab-addition	elab-addition
D14-1031	66-67	68-85	We demonstrate	that the semantic connections among words in addition to their context is necessary in forming a semantic network	66-92	66-92	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	1>2	attribution	attribution
D14-1031	37-45	68-85	We present an algorithm for simultaneously learning word meanings	that the semantic connections among words in addition to their context is necessary in forming a semantic network	37-65	66-92	We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations .	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	1<2	evaluation	evaluation
D14-1031	68-85	86-92	that the semantic connections among words in addition to their context is necessary in forming a semantic network	that resembles an adult's semantic knowledge .	66-92	66-92	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adult's semantic knowledge .	1<2	elab-addition	elab-addition
D14-1032	1,12-18	29-41	Models <*> are of interest to researchers in NLP	Performance advantages of the multi-modal approach over language-only models have been clearly established	1-28	29-51	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	1>2	result	result
D14-1032	1,12-18	2-11	Models <*> are of interest to researchers in NLP	that acquire semantic representations from both linguistic and perceptual input	1-28	1-28	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	1<2	elab-addition	elab-addition
D14-1032	12-18	19-28	are of interest to researchers in NLP	because of the obvious parallels with human language learning .	1-28	1-28	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning .	1<2	exp-reason	exp-reason
D14-1032	29-41	63-71	Performance advantages of the multi-modal approach over language-only models have been clearly established	In this work , we present a new means	29-51	63-92	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	1>2	bg-compare	bg-compare
D14-1032	29-41	42-51	Performance advantages of the multi-modal approach over language-only models have been clearly established	when models are required to learn concrete noun concepts .	29-51	29-51	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	1<2	temporal	temporal
D14-1032	42-51	52-62	when models are required to learn concrete noun concepts .	However , such concepts are comparatively rare in everyday language .	29-51	52-62	Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts .	However , such concepts are comparatively rare in everyday language .	1<2	contrast	contrast
D14-1032	63-71	72-84	In this work , we present a new means	of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts	63-92	63-92	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	1<2	elab-addition	elab-addition
D14-1032	72-84	85-87	of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts	via an approach	63-92	63-92	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	1<2	manner-means	manner-means
D14-1032	85-87	88-92	via an approach	that learns multi-modal embeddings .	63-92	63-92	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	1<2	elab-addition	elab-addition
D14-1032	63-71	93-104	In this work , we present a new means	Our architecture outperforms previous approaches in combining input from distinct modalities ,	63-92	93-119	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	1<2	evaluation	evaluation
D14-1032	93-104	105-119	Our architecture outperforms previous approaches in combining input from distinct modalities ,	and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	93-119	93-119	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	Our architecture outperforms previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives .	1<2	joint	joint
D14-1032	63-71	120-126	In this work , we present a new means	We discuss the implications of our results	63-92	120-142	In this work , we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	1<2	summary	summary
D14-1032	120-126	127-142	We discuss the implications of our results	both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	120-142	120-142	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation .	1<2	elab-addition	elab-addition
D14-1033	1-9	17-27	State-of-art systems for grammar error correction often correct errors	In this paper , we describe a grammar error correction system	1-16	17-36	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	1>2	bg-compare	bg-compare
D14-1033	1-9	10-16	State-of-art systems for grammar error correction often correct errors	based on word sequences or phrases .	1-16	1-16	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	State-of-art systems for grammar error correction often correct errors based on word sequences or phrases .	1<2	bg-general	bg-general
D14-1033	17-27	28-36	In this paper , we describe a grammar error correction system	which corrects grammatical errors at tree level directly .	17-36	17-36	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	1<2	elab-addition	elab-addition
D14-1033	17-27	37-43	In this paper , we describe a grammar error correction system	We cluster all error into two groups	17-36	37-60	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	1<2	elab-addition	elab-addition
D14-1033	37-43	44-52	We cluster all error into two groups	and divide our system into two modules correspondingly :	37-60	37-60	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	1<2	joint	joint
D14-1033	44-52	53-60	and divide our system into two modules correspondingly :	the general module and the special module .	37-60	37-60	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	1<2	elab-enumember	elab-enumember
D14-1033	37-43	61-71	We cluster all error into two groups	In the general module , we propose a TreeNode Language Model	37-60	61-80	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	1<2	elab-aspect	elab-aspect
D14-1033	61-71	72-74	In the general module , we propose a TreeNode Language Model	to correct errors	61-80	61-80	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	1<2	enablement	enablement
D14-1033	72-74	75-80	to correct errors	related to verbs and nouns .	61-80	61-80	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	1<2	elab-addition	elab-addition
D14-1033	61-71	81-88	In the general module , we propose a TreeNode Language Model	The TreeNode Language Model is easy to train	61-80	81-94	In the general module , we propose a TreeNode Language Model to correct errors related to verbs and nouns .	The TreeNode Language Model is easy to train and the decoding is efficient .	1<2	elab-addition	elab-addition
D14-1033	81-88	89-94	The TreeNode Language Model is easy to train	and the decoding is efficient .	81-94	81-94	The TreeNode Language Model is easy to train and the decoding is efficient .	The TreeNode Language Model is easy to train and the decoding is efficient .	1<2	joint	joint
D14-1033	37-43	95-105	We cluster all error into two groups	In the special module , two extra classification models are trained	37-60	95-114	We cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	1<2	elab-aspect	elab-aspect
D14-1033	95-105	106-108	In the special module , two extra classification models are trained	to correct errors	95-114	95-114	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	1<2	enablement	enablement
D14-1033	106-108	109-114	to correct errors	related to determiners and prepositions .	95-114	95-114	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	In the special module , two extra classification models are trained to correct errors related to determiners and prepositions .	1<2	elab-addition	elab-addition
D14-1033	115-116	117-123	Experiments show	that our system outperforms the state-of-art systems	115-129	115-129	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	1>2	attribution	attribution
D14-1033	17-27	117-123	In this paper , we describe a grammar error correction system	that our system outperforms the state-of-art systems	17-36	115-129	In this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	1<2	evaluation	evaluation
D14-1033	117-123	124-129	that our system outperforms the state-of-art systems	and improves the F1 score .	115-129	115-129	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	Experiments show that our system outperforms the state-of-art systems and improves the F1 score .	1<2	progression	progression
D14-1034	1-14	48-60	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition	1-26	48-68	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	1>2	bg-compare	bg-compare
D14-1034	1-14	15-26	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing	and infer SCF distributions at type , rather than instance level .	1-26	1-26	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	1<2	joint	joint
D14-1034	1-14	27-34	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing	These systems suffer from poor portability across domains	1-26	27-47	Most existing systems for subcategorization frame ( SCF ) acquisition rely on supervised parsing and infer SCF distributions at type , rather than instance level .	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	1<2	elab-addition	elab-addition
D14-1034	27-34	35-40,45-47	These systems suffer from poor portability across domains	and their benefit for NLP tasks <*> is limited .	27-47	27-47	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	1<2	joint	joint
D14-1034	35-40,45-47	41-44	and their benefit for NLP tasks <*> is limited .	that involve sentence-level processing	27-47	27-47	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited .	1<2	elab-addition	elab-addition
D14-1034	48-60	61-68	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition	which is designed to address these problems .	48-68	48-68	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	1<2	elab-addition	elab-addition
D14-1034	48-60	69-79	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition	The system relies on supervised POS tagging rather than parsing ,	48-68	69-89	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	1<2	elab-addition	elab-addition
D14-1034	69-79	80-89	The system relies on supervised POS tagging rather than parsing ,	and is capable of learning SCFs at instance level .	69-89	69-89	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	The system relies on supervised POS tagging rather than parsing , and is capable of learning SCFs at instance level .	1<2	joint	joint
D14-1034	90-96	99-109	We perform evaluation against gold standard data	that our system outperforms several supervised and type-level SCF baselines .	90-109	90-109	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	1>2	elab-addition	elab-addition
D14-1034	97-98	99-109	which shows	that our system outperforms several supervised and type-level SCF baselines .	90-109	90-109	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	1>2	attribution	attribution
D14-1034	48-60	99-109	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition	that our system outperforms several supervised and type-level SCF baselines .	48-68	90-109	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines .	1<2	evaluation	evaluation
D14-1034	110-122	124-128,133-140	We also conduct task-based evaluation in the context of verb similarity prediction ,	that a vector space model <*> substantially outperforms a lexical model and a model	110-146	110-146	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	1>2	elab-addition	elab-addition
D14-1034	123	124-128,133-140	demonstrating	that a vector space model <*> substantially outperforms a lexical model and a model	110-146	110-146	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	1>2	attribution	attribution
D14-1034	48-60	124-128,133-140	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition	that a vector space model <*> substantially outperforms a lexical model and a model	48-68	110-146	We propose a new unsupervised , Markov Random Field-based model for SCF acquisition which is designed to address these problems .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	1<2	evaluation	evaluation
D14-1034	124-128,133-140	129-132	that a vector space model <*> substantially outperforms a lexical model and a model	based on our SCFs	110-146	110-146	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	1<2	bg-general	bg-general
D14-1034	133-140	141-146	substantially outperforms a lexical model and a model	based on a supervised parser .	110-146	110-146	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	We also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser .	1<2	bg-general	bg-general
D14-1035	1-18	19-25	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	We present a Bayesian model and algorithms	1-18	19-39	PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing .	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	1>2	bg-goal	bg-goal
D14-1035	19-25	26-30	We present a Bayesian model and algorithms	based on a Gibbs sampler	19-39	19-39	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	1<2	bg-general	bg-general
D14-1035	19-25	31-39	We present a Bayesian model and algorithms	for parsing with a grammar with latent annotations .	19-39	19-39	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	1<2	enablement	enablement
D14-1035	19-25	40-49	We present a Bayesian model and algorithms	For PCFG-LA , we present an additional Gibbs sampler algorithm	19-39	40-67	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	1<2	joint	joint
D14-1035	40-49	50-56	For PCFG-LA , we present an additional Gibbs sampler algorithm	to learn annotations from training data ,	40-67	40-67	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	1<2	enablement	enablement
D14-1035	50-56	57-67	to learn annotations from training data ,	which are parse trees with coarse ( unannotated ) symbols .	40-67	40-67	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	For PCFG-LA , we present an additional Gibbs sampler algorithm to learn annotations from training data , which are parse trees with coarse ( unannotated ) symbols .	1<2	elab-addition	elab-addition
D14-1035	68-69	70-85	We show	that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages	68-97	68-97	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	1>2	attribution	attribution
D14-1035	19-25	70-85	We present a Bayesian model and algorithms	that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages	19-39	68-97	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	1<2	evaluation	evaluation
D14-1035	70-85	86-88	that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages	and producing results	68-97	68-97	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	1<2	joint	joint
D14-1035	86-88	89-97	and producing results	that are on-par with or surpass previous approaches .	68-97	68-97	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches .	1<2	elab-addition	elab-addition
D14-1035	98-106	107-118	Our results for Kinyarwanda and Malagasy in particular demonstrate	that low-resource language parsing can benefit substantially from a Bayesian approach .	98-118	98-118	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	1>2	attribution	attribution
D14-1035	19-25	107-118	We present a Bayesian model and algorithms	that low-resource language parsing can benefit substantially from a Bayesian approach .	19-39	98-118	We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations .	Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach .	1<2	evaluation	evaluation
D14-1036	1-13	54-58	We introduce the task of incremental semantic role labeling ( iSRL ) ,	We propose an iSRL system	1-27	54-81	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	1>2	bg-goal	bg-goal
D14-1036	1-13	14-22	We introduce the task of incremental semantic role labeling ( iSRL ) ,	in which semantic roles are assigned to incomplete input	1-27	1-27	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	1<2	elab-addition	elab-addition
D14-1036	14-22	23-27	in which semantic roles are assigned to incomplete input	( sentence prefixes ) .	1-27	1-27	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	1<2	elab-example	elab-example
D14-1036	1-13	28-36	We introduce the task of incremental semantic role labeling ( iSRL ) ,	iSRL is the semantic equivalent of incremental parsing ,	1-27	28-53	We introduce the task of incremental semantic role labeling ( iSRL ) , in which semantic roles are assigned to incomplete input ( sentence prefixes ) .	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	1<2	elab-addition	elab-addition
D14-1036	28-36	37-53	iSRL is the semantic equivalent of incremental parsing ,	and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	28-53	28-53	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	iSRL is the semantic equivalent of incremental parsing , and is useful for language modeling , sentence completion , machine translation , and psycholinguistic modeling .	1<2	joint	joint
D14-1036	54-58	59-81	We propose an iSRL system	that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	54-81	54-81	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	1<2	elab-addition	elab-addition
D14-1036	82-97	98-103	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	It substantially outperforms a strong baseline	82-97	98-125	Our approach achieves an SRL F-score of 78.38 % on the standard CoNLL 2009 dataset .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	1>2	exp-evidence	exp-evidence
D14-1036	54-58	98-103	We propose an iSRL system	It substantially outperforms a strong baseline	54-81	98-125	We propose an iSRL system that combines an incremental TAG parser with a semantically enriched lexicon , a role propagation algorithm , and a cascade of classifiers .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	1<2	evaluation	evaluation
D14-1036	98-103	104-118	It substantially outperforms a strong baseline	that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline	98-125	98-125	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	1<2	elab-addition	elab-addition
D14-1036	104-118	119-125	that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline	based on Nivre's incremental dependency parser .	98-125	98-125	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	It substantially outperforms a strong baseline that combines gold-standard syntactic dependencies with heuristic role assignment , as well as a baseline based on Nivre's incremental dependency parser .	1<2	bg-general	bg-general
D14-1037	1-15	22-24,37-43	The informal nature of social media text renders it very difficult to be automatically processed	Text normalization , <*> provides a solution to this challenge .	1-21	22-43	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	1>2	result	result
D14-1037	1-15	16-21	The informal nature of social media text renders it very difficult to be automatically processed	by natural language processing tools .	1-21	1-21	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	The informal nature of social media text renders it very difficult to be automatically processed by natural language processing tools .	1<2	manner-means	manner-means
D14-1037	22-24,37-43	44-50	Text normalization , <*> provides a solution to this challenge .	We introduce an unsupervised text normalization approach	22-43	44-66	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	1>2	bg-goal	bg-goal
D14-1037	22-24,37-43	25-36	Text normalization , <*> provides a solution to this challenge .	which corresponds to restoring the non-standard words to their canonical forms ,	22-43	22-43	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	Text normalization , which corresponds to restoring the non-standard words to their canonical forms , provides a solution to this challenge .	1<2	elab-addition	elab-addition
D14-1037	44-50	51-66	We introduce an unsupervised text normalization approach	that utilizes not only lexical , but also contextual and grammatical features of social text .	44-66	44-66	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	1<2	elab-addition	elab-addition
D14-1037	44-50	67-78	We introduce an unsupervised text normalization approach	The contextual and grammatical features are extracted from a word association graph	44-66	67-89	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	1<2	elab-aspect	elab-aspect
D14-1037	67-78	79-89	The contextual and grammatical features are extracted from a word association graph	built by using a large unlabeled social media text corpus .	67-89	67-89	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	1<2	elab-addition	elab-addition
D14-1037	67-78	90-111	The contextual and grammatical features are extracted from a word association graph	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	67-89	90-111	The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus .	The graph encodes the relative positions of the words with respect to each other , as well as their part-of-speech tags .	1<2	elab-addition	elab-addition
D14-1037	44-50	112-116	We introduce an unsupervised text normalization approach	The lexical features are obtained	44-66	112-146	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	1<2	elab-aspect	elab-aspect
D14-1037	112-116	117-123	The lexical features are obtained	by using the longest common subsequence ratio	112-146	112-146	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	1<2	manner-means	manner-means
D14-1037	112-116	124-127	The lexical features are obtained	and edit distance measures	112-146	112-146	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	1<2	joint	joint
D14-1037	124-127	128-135	and edit distance measures	to encode the surface similarity among words ,	112-146	112-146	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	1<2	enablement	enablement
D14-1037	117-123	136-140	by using the longest common subsequence ratio	and the double metaphone algorithm	112-146	112-146	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	1<2	joint	joint
D14-1037	136-140	141-146	and the double metaphone algorithm	to represent the phonetic similarity .	112-146	112-146	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words , and the double metaphone algorithm to represent the phonetic similarity .	1<2	enablement	enablement
D14-1037	147-152	161-165	Unlike most of the recent approaches	the proposed approach performs normalization	147-178	147-178	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	1>2	contrast	contrast
D14-1037	147-152	153-160	Unlike most of the recent approaches	that are based on generating normalization dictionaries ,	147-178	147-178	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	1<2	elab-addition	elab-addition
D14-1037	44-50	161-165	We introduce an unsupervised text normalization approach	the proposed approach performs normalization	44-66	147-178	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	1<2	elab-addition	elab-addition
D14-1037	161-165	166-178	the proposed approach performs normalization	by considering the context of the non-standard words in the input text .	147-178	147-178	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	Unlike most of the recent approaches that are based on generating normalization dictionaries , the proposed approach performs normalization by considering the context of the non-standard words in the input text .	1<2	manner-means	manner-means
D14-1037	179-181	182-191	Our results show	that it achieves state-of-the-art F-score performance on standard datasets .	179-191	179-191	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	1>2	attribution	attribution
D14-1037	44-50	182-191	We introduce an unsupervised text normalization approach	that it achieves state-of-the-art F-score performance on standard datasets .	44-66	179-191	We introduce an unsupervised text normalization approach that utilizes not only lexical , but also contextual and grammatical features of social text .	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	1<2	evaluation	evaluation
D14-1037	182-191	192-199	that it achieves state-of-the-art F-score performance on standard datasets .	In addition , the system can be tuned	179-191	192-210	Our results show that it achieves state-of-the-art F-score performance on standard datasets .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	1<2	progression	progression
D14-1037	192-199	200-204	In addition , the system can be tuned	to achieve very high precision	192-210	192-210	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	1<2	enablement	enablement
D14-1037	200-204	205-210	to achieve very high precision	without sacrificing much from recall .	192-210	192-210	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	In addition , the system can be tuned to achieve very high precision without sacrificing much from recall .	1<2	condition	condition
D14-1038	1-11	75-83	Search engines are increasingly relying on large knowledge bases of facts	We describe ReNoun , an open information extraction system	1-19	75-98	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	1>2	bg-compare	bg-compare
D14-1038	1-11	12-19	Search engines are increasingly relying on large knowledge bases of facts	to provide direct answers to users' queries .	1-19	1-19	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	1<2	enablement	enablement
D14-1038	1-11	20-30	Search engines are increasingly relying on large knowledge bases of facts	However , the construction of these knowledge bases is largely manual	1-19	20-43	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	1<2	contrast	contrast
D14-1038	20-30	31-43	However , the construction of these knowledge bases is largely manual	and does not scale to the long and heavy tail of facts .	20-43	20-43	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	However , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts .	1<2	joint	joint
D14-1038	1-11	44-52	Search engines are increasingly relying on large knowledge bases of facts	Open information extraction tries to address this challenge ,	1-19	44-74	Search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users' queries .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	1<2	elab-addition	elab-addition
D14-1038	53-55	56-63	but typically assumes	that facts are expressed with verb phrases ,	44-74	44-74	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	1>2	attribution	attribution
D14-1038	44-52	56-63	Open information extraction tries to address this challenge ,	that facts are expressed with verb phrases ,	44-74	44-74	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	1<2	contrast	contrast
D14-1038	56-63	64-68	that facts are expressed with verb phrases ,	and therefore has had difficulty	44-74	44-74	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	1<2	result	result
D14-1038	64-68	69-74	and therefore has had difficulty	extracting facts for noun-based relations .	44-74	44-74	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	Open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations .	1<2	elab-addition	elab-addition
D14-1038	75-83	84-87	We describe ReNoun , an open information extraction system	that complements previous efforts	75-98	75-98	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	1<2	elab-addition	elab-addition
D14-1038	84-87	88-98	that complements previous efforts	by focusing on nominal attributes and on the long tail .	75-98	75-98	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	1<2	manner-means	manner-means
D14-1038	75-83	99-110	We describe ReNoun , an open information extraction system	ReNoun's approach is based on leveraging a large ontology of noun attributes	75-98	99-120	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	1<2	manner-means	manner-means
D14-1038	99-110	111-120	ReNoun's approach is based on leveraging a large ontology of noun attributes	mined from a text corpus and from user queries .	99-120	99-120	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	1<2	elab-addition	elab-addition
D14-1038	99-110	121-128	ReNoun's approach is based on leveraging a large ontology of noun attributes	ReNoun creates a seed set of training data	99-120	121-144	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	1<2	elab-process_step	elab-process_step
D14-1038	121-128	129-132	ReNoun creates a seed set of training data	by using specialized patterns	121-144	121-144	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	1<2	manner-means	manner-means
D14-1038	133-134	135-144	and requiring	that the facts mention an attribute in the ontology .	121-144	121-144	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	1>2	attribution	attribution
D14-1038	129-132	135-144	by using specialized patterns	that the facts mention an attribute in the ontology .	121-144	121-144	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	ReNoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontology .	1<2	joint	joint
D14-1038	99-110	145-151	ReNoun's approach is based on leveraging a large ontology of noun attributes	ReNoun then generalizes from this seed set	99-120	145-164	ReNoun's approach is based on leveraging a large ontology of noun attributes mined from a text corpus and from user queries .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	1<2	elab-process_step	elab-process_step
D14-1038	145-151	152-159	ReNoun then generalizes from this seed set	to produce a much larger set of extractions	145-164	145-164	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	1<2	enablement	enablement
D14-1038	152-159	160-164	to produce a much larger set of extractions	that are then scored .	145-164	145-164	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	ReNoun then generalizes from this seed set to produce a much larger set of extractions that are then scored .	1<2	elab-addition	elab-addition
D14-1038	165-167	170-179	We describe experiments	that we extract facts with high precision and for attributes	165-187	165-187	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	1>2	elab-addition	elab-addition
D14-1038	168-169	170-179	that show	that we extract facts with high precision and for attributes	165-187	165-187	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	1>2	attribution	attribution
D14-1038	75-83	170-179	We describe ReNoun , an open information extraction system	that we extract facts with high precision and for attributes	75-98	165-187	We describe ReNoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	1<2	evaluation	evaluation
D14-1038	170-179	180-187	that we extract facts with high precision and for attributes	that cannot be extracted with verb-based techniques .	165-187	165-187	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	We describe experiments that show that we extract facts with high precision and for attributes that cannot be extracted with verb-based techniques .	1<2	elab-addition	elab-addition
D14-1039	1-15	34-37	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	We demonstrate the effectiveness	1-15	34-82	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	1>2	bg-compare	bg-compare
D14-1039	1-15	16-26	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	These methods ignore the natural hierarchy of cells in such grids	1-15	16-33	Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids .	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	1<2	elab-addition	elab-addition
D14-1039	16-26	27-33	These methods ignore the natural hierarchy of cells in such grids	and fall afoul of independence assumptions .	16-33	16-33	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions .	1<2	joint	joint
D14-1039	34-37	38-51	We demonstrate the effectiveness	of using logistic regression models on a hierarchy of nodes in the grid ,	34-82	34-82	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	1<2	elab-addition	elab-addition
D14-1039	38-51	52-63	of using logistic regression models on a hierarchy of nodes in the grid ,	which improves upon the state of the art accuracy by several percent	34-82	34-82	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	1<2	elab-addition	elab-addition
D14-1039	52-63	64-82	which improves upon the state of the art accuracy by several percent	and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	34-82	34-82	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	1<2	joint	joint
D14-1039	83-85	86-93	We also show	that logistic regression performs feature selection effectively ,	83-100	83-100	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	1>2	attribution	attribution
D14-1039	34-37	86-93	We demonstrate the effectiveness	that logistic regression performs feature selection effectively ,	34-82	83-100	We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid , which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter , Wikipedia , and Flickr .	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	1<2	evaluation	evaluation
D14-1039	86-93	94-100	that logistic regression performs feature selection effectively ,	assigning high weights to geocentric terms .	83-100	83-100	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	We also show that logistic regression performs feature selection effectively , assigning high weights to geocentric terms .	1<2	elab-addition	elab-addition
D14-1040	1-6	7-16	We propose the first probabilistic approach	to modeling cross-lingual semantic similarity ( CLSS ) in context	1-22	1-22	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	1<2	enablement	enablement
D14-1040	7-16	17-22	to modeling cross-lingual semantic similarity ( CLSS ) in context	which requires only comparable data .	1-22	1-22	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	1<2	elab-addition	elab-addition
D14-1040	1-6	23-28	We propose the first probabilistic approach	The approach relies on an idea	1-22	23-61	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	1<2	elab-addition	elab-addition
D14-1040	23-28	29-41	The approach relies on an idea	of projecting words and sets of words into a shared latent semantic space	23-61	23-61	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	1<2	elab-addition	elab-addition
D14-1040	29-41	42-48	of projecting words and sets of words into a shared latent semantic space	spanned by language-pair independent latent semantic concepts	23-61	23-61	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	1<2	elab-addition	elab-addition
D14-1040	42-48	49-53	spanned by language-pair independent latent semantic concepts	( e.g. , cross-lingual topics	23-61	23-61	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	1<2	elab-example	elab-example
D14-1040	49-53	54-61	( e.g. , cross-lingual topics	obtained by a multilingual topic model ) .	23-61	23-61	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	1<2	elab-addition	elab-addition
D14-1040	42-48	62-71	spanned by language-pair independent latent semantic concepts	These latent cross-lingual concepts are induced from a comparable corpus	23-61	62-77	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	1<2	elab-addition	elab-addition
D14-1040	62-71	72-77	These latent cross-lingual concepts are induced from a comparable corpus	without any additional lexical resources .	62-77	62-77	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources .	1<2	elab-addition	elab-addition
D14-1040	42-48	78-90	spanned by language-pair independent latent semantic concepts	Word meaning is represented as a probability distribution over the latent concepts ,	23-61	78-108	The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	1<2	elab-addition	elab-addition
D14-1040	78-90	91-108	Word meaning is represented as a probability distribution over the latent concepts ,	and a change in meaning is represented as a change in the distribution over these latent concepts .	78-108	78-108	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts .	1<2	joint	joint
D14-1040	1-6	109-112	We propose the first probabilistic approach	We present new models	1-22	109-123	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	1<2	progression	progression
D14-1040	109-112	113-123	We present new models	that modulate the isolated out-of-context word representations with contextual knowledge .	109-123	109-123	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	We present new models that modulate the isolated out-of-context word representations with contextual knowledge .	1<2	elab-addition	elab-addition
D14-1040	1-6	124-150	We propose the first probabilistic approach	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	1-22	124-150	We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data .	Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity .	1<2	evaluation	evaluation
D14-1041	1-10	40-45	The current approaches to Semantic Role Labeling ( SRL )	In this paper , we prove	1-39	40-58	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	In this paper , we prove that different predicates in a sentence could help each other during SRL .	1>2	bg-compare	bg-compare
D14-1041	1-10	11-18	The current approaches to Semantic Role Labeling ( SRL )	usually perform role classification for each predicate separately	1-39	1-39	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	1<2	elab-addition	elab-addition
D14-1041	11-18	19-28	usually perform role classification for each predicate separately	and the interaction among individual predicate's role labeling is ignored	1-39	1-39	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	1<2	contrast	contrast
D14-1041	19-28	29-39	and the interaction among individual predicate's role labeling is ignored	if there is more than one predicate in a sentence .	1-39	1-39	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	The current approaches to Semantic Role Labeling ( SRL ) usually perform role classification for each predicate separately and the interaction among individual predicate's role labeling is ignored if there is more than one predicate in a sentence .	1<2	condition	condition
D14-1041	40-45	46-58	In this paper , we prove	that different predicates in a sentence could help each other during SRL .	40-58	40-58	In this paper , we prove that different predicates in a sentence could help each other during SRL .	In this paper , we prove that different predicates in a sentence could help each other during SRL .	1<2	attribution	attribution
D14-1041	46-58	59-70	that different predicates in a sentence could help each other during SRL .	In multi-predicate role labeling , there are mainly two key points :	40-58	59-83	In this paper , we prove that different predicates in a sentence could help each other during SRL .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	1<2	elab-addition	elab-addition
D14-1041	59-70	71-78	In multi-predicate role labeling , there are mainly two key points :	argument identification and role labeling of the arguments	59-83	59-83	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	1<2	elab-enumember	elab-enumember
D14-1041	71-78	79-83	argument identification and role labeling of the arguments	shared by multiple predicates .	59-83	59-83	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	1<2	elab-addition	elab-addition
D14-1041	84-88	89-100	To address these issues ,	in the stage of argument identification , we propose novel predicate-related features	84-141	84-141	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	1>2	enablement	enablement
D14-1041	71-78	89-100	argument identification and role labeling of the arguments	in the stage of argument identification , we propose novel predicate-related features	59-83	84-141	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	1<2	elab-aspect	elab-aspect
D14-1041	89-100	101-108	in the stage of argument identification , we propose novel predicate-related features	which help remove many argument identification errors ;	84-141	84-141	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	1<2	elab-addition	elab-addition
D14-1041	71-78	109-121	argument identification and role labeling of the arguments	in the stage of argument classification , we adopt a discriminative reranking approach	59-83	84-141	In multi-predicate role labeling , there are mainly two key points : argument identification and role labeling of the arguments shared by multiple predicates .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	1<2	elab-aspect	elab-aspect
D14-1041	109-121	122-130	in the stage of argument classification , we adopt a discriminative reranking approach	to perform role classification of the shared arguments ,	84-141	84-141	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	1<2	enablement	enablement
D14-1041	122-130	131-141	to perform role classification of the shared arguments ,	in which a large set of global features are proposed .	84-141	84-141	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	To address these issues , in the stage of argument identification , we propose novel predicate-related features which help remove many argument identification errors ; in the stage of argument classification , we adopt a discriminative reranking approach to perform role classification of the shared arguments , in which a large set of global features are proposed .	1<2	elab-addition	elab-addition
D14-1041	142-149	160-173	We conducted experiments on two standard benchmarks :	that our approach can significantly improve SRL performance , especially in Chinese PropBank .	142-155	156-173	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	1>2	manner-means	manner-means
D14-1041	142-149	150-155	We conducted experiments on two standard benchmarks :	Chinese PropBank and English PropBank .	142-155	142-155	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	We conducted experiments on two standard benchmarks : Chinese PropBank and English PropBank .	1<2	elab-enumember	elab-enumember
D14-1041	156-159	160-173	The experimental results show	that our approach can significantly improve SRL performance , especially in Chinese PropBank .	156-173	156-173	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	1>2	attribution	attribution
D14-1041	40-45	160-173	In this paper , we prove	that our approach can significantly improve SRL performance , especially in Chinese PropBank .	40-58	156-173	In this paper , we prove that different predicates in a sentence could help each other during SRL .	The experimental results show that our approach can significantly improve SRL performance , especially in Chinese PropBank .	1<2	evaluation	evaluation
D14-1042	1-10	11-22	Word-sense recognition and disambiguation ( WERD ) is the task	of identifying word phrases and their senses in natural language text .	1-22	1-22	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	1<2	elab-addition	elab-addition
D14-1042	23-33	34-45	Though it is well understood how to disambiguate noun phrases ,	this task is much less studied for verbs and verbal phrases .	23-45	23-45	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	1>2	contrast	contrast
D14-1042	1-10	34-45	Word-sense recognition and disambiguation ( WERD ) is the task	this task is much less studied for verbs and verbal phrases .	1-22	23-45	Word-sense recognition and disambiguation ( WERD ) is the task of identifying word phrases and their senses in natural language text .	Though it is well understood how to disambiguate noun phrases , this task is much less studied for verbs and verbal phrases .	1<2	elab-addition	elab-addition
D14-1042	46-62	63-68	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our framework first identifies multi-word expressions	46-62	63-88	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	1<2	elab-process_step	elab-process_step
D14-1042	63-68	69-77	Our framework first identifies multi-word expressions	based on the syntactic structure of the sentence ;	63-88	63-88	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	1<2	bg-general	bg-general
D14-1042	63-68	78-88	Our framework first identifies multi-word expressions	this allows us to recognize both contiguous and non-contiguous phrases .	63-88	63-88	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	Our framework first identifies multi-word expressions based on the syntactic structure of the sentence ; this allows us to recognize both contiguous and non-contiguous phrases .	1<2	enablement	enablement
D14-1042	46-62	89-102	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase ,	46-62	89-110	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	1<2	elab-process_step	elab-process_step
D14-1042	89-102	103-110	We then generate a list of candidate senses for each word or phrase ,	using novel syntactic and semantic pruning techniques .	89-110	89-110	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	We then generate a list of candidate senses for each word or phrase , using novel syntactic and semantic pruning techniques .	1<2	manner-means	manner-means
D14-1042	46-62	111-129	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	46-62	111-129	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	We also construct and leverage a new resource of pairs of senses for verbs and their object arguments .	1<2	elab-process_step	elab-process_step
D14-1042	46-62	130-146	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods ,	46-62	130-153	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	1<2	elab-process_step	elab-process_step
D14-1042	130-146	147-153	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods ,	and boost their precision and recall .	130-153	130-153	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	Finally , we feed the so-obtained candidate senses into standard word-sense disambiguation ( WSD ) methods , and boost their precision and recall .	1<2	joint	joint
D14-1042	154-156	157-167	Our experiments indicate	that Werdy significantly increases the performance of existing WSD methods .	154-167	154-167	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	1>2	attribution	attribution
D14-1042	46-62	157-167	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	that Werdy significantly increases the performance of existing WSD methods .	46-62	154-167	We present Werdy , a framework for WERD with particular focus on verbs and verbal phrases .	Our experiments indicate that Werdy significantly increases the performance of existing WSD methods .	1<2	evaluation	evaluation
D14-1043	1-12	25-46	Language is given meaning through its correspondence with a world representation .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	1-12	25-46	Language is given meaning through its correspondence with a world representation .	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	1>2	bg-goal	bg-goal
D14-1043	1-12	13-24	Language is given meaning through its correspondence with a world representation .	This correspondence can be at multiple levels of granularity or resolutions .	1-12	13-24	Language is given meaning through its correspondence with a world representation .	This correspondence can be at multiple levels of granularity or resolutions .	1<2	elab-addition	elab-addition
D14-1043	25-46	47-54	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We define and optimize a factored objective function	25-46	47-72	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	1<2	manner-means	manner-means
D14-1043	47-54	55-72	We define and optimize a factored objective function	that allows us to leverage discourse structure and the compositional nature of both language and game events .	47-72	47-72	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	We define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events .	1<2	elab-addition	elab-addition
D14-1043	73-74	75-87	We show	that finer resolution grounding helps coarser resolution grounding , and vice versa .	73-87	73-87	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	1>2	attribution	attribution
D14-1043	25-46	75-87	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	that finer resolution grounding helps coarser resolution grounding , and vice versa .	25-46	73-87	In this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries .	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	1<2	evaluation	evaluation
D14-1043	75-87	88-110	that finer resolution grounding helps coarser resolution grounding , and vice versa .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	73-87	88-110	We show that finer resolution grounding helps coarser resolution grounding , and vice versa .	Our method results in an F1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding .	1<2	exp-evidence	exp-evidence
D14-1044	1-29	74-89	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	We present two improvements to the use of such large corpora to augment KB inference .	1-29	74-89	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	We present two improvements to the use of such large corpora to augment KB inference .	1>2	bg-compare	bg-compare
D14-1044	30-36	37-42	While these KBs are very large ,	they are still very incomplete ,	30-52	30-52	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	1>2	contrast	contrast
D14-1044	1-29	37-42	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	they are still very incomplete ,	1-29	30-52	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	1<2	elab-addition	elab-addition
D14-1044	37-42	43-47	they are still very incomplete ,	necessitating the use of inference	30-52	30-52	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	1<2	result	result
D14-1044	43-47	48-52	necessitating the use of inference	to fill in gaps .	30-52	30-52	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	While these KBs are very large , they are still very incomplete , necessitating the use of inference to fill in gaps .	1<2	enablement	enablement
D14-1044	1-29	53-73	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	1-29	53-73	Much work in recent years has gone into the construction of large knowledge bases ( KBs ) , such as Freebase , DBPedia , NELL , and YAGO .	Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs .	1<2	elab-addition	elab-addition
D14-1044	74-89	90-96	We present two improvements to the use of such large corpora to augment KB inference .	First , we present a new technique	74-89	90-120	We present two improvements to the use of such large corpora to augment KB inference .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	1<2	elab-aspect	elab-aspect
D14-1044	90-96	97-108	First , we present a new technique	for combining KB relations and surface text into a single graph representation	90-120	90-120	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	1<2	enablement	enablement
D14-1044	97-108	109-115	for combining KB relations and surface text into a single graph representation	that is much more compact than graphs	90-120	90-120	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	1<2	elab-addition	elab-addition
D14-1044	109-115	116-120	that is much more compact than graphs	used in prior work .	90-120	90-120	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	First , we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work .	1<2	elab-addition	elab-addition
D14-1044	121-124	125-137	Second , we describe	how to incorporate vector space similarity into random walk inference over KBs ,	121-147	121-147	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	1>2	attribution	attribution
D14-1044	74-89	125-137	We present two improvements to the use of such large corpora to augment KB inference .	how to incorporate vector space similarity into random walk inference over KBs ,	74-89	121-147	We present two improvements to the use of such large corpora to augment KB inference .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	1<2	elab-aspect	elab-aspect
D14-1044	125-137	138-147	how to incorporate vector space similarity into random walk inference over KBs ,	reducing the feature sparsity inherent in using surface text .	121-147	121-147	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	Second , we describe how to incorporate vector space similarity into random walk inference over KBs , reducing the feature sparsity inherent in using surface text .	1<2	enablement	enablement
D14-1044	74-89	148-164	We present two improvements to the use of such large corpora to augment KB inference .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	74-89	148-164	We present two improvements to the use of such large corpora to augment KB inference .	This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways .	1<2	enablement	enablement
D14-1044	165-176	177-193	With experiments on many relations from two separate KBs , we show	that our methods significantly outperform prior work on KB inference , both in the size of problem	165-205	165-205	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	1>2	attribution	attribution
D14-1044	74-89	177-193	We present two improvements to the use of such large corpora to augment KB inference .	that our methods significantly outperform prior work on KB inference , both in the size of problem	74-89	165-205	We present two improvements to the use of such large corpora to augment KB inference .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	1<2	evaluation	evaluation
D14-1044	177-193	194-205	that our methods significantly outperform prior work on KB inference , both in the size of problem	our methods can handle and in the quality of predictions made .	165-205	165-205	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	With experiments on many relations from two separate KBs , we show that our methods significantly outperform prior work on KB inference , both in the size of problem our methods can handle and in the quality of predictions made .	1<2	elab-addition	elab-addition
D14-1045	1-9	49-58	State-of-the-art semantic role labelling systems require large annotated corpora	In this paper , we mitigate both of these problems	1-14	49-68	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	1>2	bg-goal	bg-goal
D14-1045	1-9	10-14	State-of-the-art semantic role labelling systems require large annotated corpora	to achieve full performance .	1-14	1-14	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	1<2	enablement	enablement
D14-1045	1-9	15-22	State-of-the-art semantic role labelling systems require large annotated corpora	Unfortunately , such corpora are expensive to produce	1-14	15-31	State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance .	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	1<2	contrast	contrast
D14-1045	15-22	23-31	Unfortunately , such corpora are expensive to produce	and often do not generalize well across do-mains .	15-31	15-31	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	1<2	joint	joint
D14-1045	23-31	32-39	and often do not generalize well across do-mains .	Even in domain , errors are often made	15-31	32-48	Unfortunately , such corpora are expensive to produce and often do not generalize well across do-mains .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	1<2	progression	progression
D14-1045	32-39	40-48	Even in domain , errors are often made	where syntactic information does not provide sufficient cues .	32-48	32-48	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	Even in domain , errors are often made where syntactic information does not provide sufficient cues .	1<2	elab-addition	elab-addition
D14-1045	49-58	59-63	In this paper , we mitigate both of these problems	by employing distributional word representations	49-68	49-68	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	1<2	manner-means	manner-means
D14-1045	59-63	64-68	by employing distributional word representations	gathered from unlabelled data .	49-68	49-68	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	1<2	elab-addition	elab-addition
D14-1045	69-79	82-86	While straight-forward word representations of predicates and arguments improve performance ,	that further gains are achieved	69-104	69-104	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	1>2	contrast	contrast
D14-1045	80-81	82-86	we show	that further gains are achieved	69-104	69-104	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	1>2	attribution	attribution
D14-1045	49-58	82-86	In this paper , we mitigate both of these problems	that further gains are achieved	49-68	69-104	In this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	1<2	evaluation	evaluation
D14-1045	82-86	87-89	that further gains are achieved	by composing representations	69-104	69-104	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	1<2	manner-means	manner-means
D14-1045	87-89	90-98	by composing representations	that model the interaction between predicate and argument ,	69-104	69-104	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	1<2	elab-addition	elab-addition
D14-1045	90-98	99-104	that model the interaction between predicate and argument ,	and capture full argument spans .	69-104	69-104	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	While straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .	1<2	joint	joint
D14-1046	1-12,30-51	13-18	This paper reports on the development of a hybrid and simple method <*> for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	based on a machine learning classifier	1-51	1-51	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	1<2	bg-general	bg-general
D14-1046	13-18	19-23	based on a machine learning classifier	( Naive Bayes ) ,	1-51	1-51	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	1<2	elab-example	elab-example
D14-1046	13-18	24-29	based on a machine learning classifier	Word Sense Disambiguation and rules ,	1-51	1-51	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	1<2	joint	joint
D14-1046	1-12,30-51	52-66	This paper reports on the development of a hybrid and simple method <*> for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	1-51	52-66	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	The system obtained an F1 score of 0.58 , with a Precision of 0.70 .	1<2	evaluation	evaluation
D14-1046	1-12,30-51	67-73	This paper reports on the development of a hybrid and simple method <*> for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	We further used the automatically assigned domains	1-51	67-85	This paper reports on the development of a hybrid and simple method based on a machine learning classifier ( Naive Bayes ) , Word Sense Disambiguation and rules , for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary , the Senso Comune De Mauro Lexicon .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	1<2	progression	progression
D14-1046	67-73	74-85	We further used the automatically assigned domains	to filter out word sense alignments between MultiWordNet and Senso Comune .	67-85	67-85	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	1<2	enablement	enablement
D14-1046	74-85	86-98	to filter out word sense alignments between MultiWordNet and Senso Comune .	This has led to an improvement in the quality of the sense alignments	67-85	86-119	We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	1<2	result	result
D14-1046	86-98	99-113	This has led to an improvement in the quality of the sense alignments	showing the validity of the approach for domain assignment and the importance of domain information	86-119	86-119	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	1<2	elab-addition	elab-addition
D14-1046	99-113	114-119	showing the validity of the approach for domain assignment and the importance of domain information	for achieving good sense alignments .	86-119	86-119	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .	1<2	enablement	enablement
D14-1047	1-17	18-27	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	We investigate the effects of context filters on thesaurus quality	1-17	18-41	Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri .	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	1>2	bg-goal	bg-goal
D14-1047	18-27	28-41	We investigate the effects of context filters on thesaurus quality	and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	18-41	18-41	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	1<2	progression	progression
D14-1047	18-27	42-52	We investigate the effects of context filters on thesaurus quality	For evaluation , we measure thesaurus agreement with WordNet and performance	18-41	42-57	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	1<2	elab-addition	elab-addition
D14-1047	42-52	53-57	For evaluation , we measure thesaurus agreement with WordNet and performance	in answering TOEFL-like questions .	42-57	42-57	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	For evaluation , we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions .	1<2	manner-means	manner-means
D14-1047	18-27	58-67	We investigate the effects of context filters on thesaurus quality	Results illustrate the sensitivity of distributional thesauri to filters .	18-41	58-67	We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion .	Results illustrate the sensitivity of distributional thesauri to filters .	1<2	evaluation	evaluation
D14-1048	1-20	21-35	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	1-20	21-35	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Such alignments will be useful for downstream extraction of semantic interpretation and generation rules .	1<2	elab-addition	elab-addition
D14-1048	1-20	36-47	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Our method involves linearizing AMR structures and performing symmetrized EM training .	1-20	36-47	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	Our method involves linearizing AMR structures and performing symmetrized EM training .	1<2	manner-means	manner-means
D14-1048	1-20	48-63	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	1-20	48-63	We align pairs of English sentences and corresponding Abstract Meaning Representations ( AMR ) , at the token level .	We obtain 86.5 % and 83.1 % alignment F score on development and test sets .	1<2	evaluation	evaluation
D14-1049	1-10	11-22	We introduce a Semantic Role Labeling ( SRL ) parser	that finds semantic roles for a predicate together with the syntactic paths	1-27	1-27	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	1<2	elab-addition	elab-addition
D14-1049	11-22	23-27	that finds semantic roles for a predicate together with the syntactic paths	linking predicates and arguments .	1-27	1-27	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	1<2	elab-addition	elab-addition
D14-1049	1-10	28-43	We introduce a Semantic Role Labeling ( SRL ) parser	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption	1-27	28-60	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	1<2	elab-aspect	elab-aspect
D14-1049	28-43	44-60	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption	that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	28-60	28-60	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	Our main contribution is to formulate SRL in terms of shortest-path inference , on the assumption that the SRL model is restricted to arc-factored features of the syntactic paths behind semantic roles .	1<2	elab-addition	elab-addition
D14-1049	1-10	61-70	We introduce a Semantic Role Labeling ( SRL ) parser	Overall , our method for SRL is a novel way	1-27	61-88	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	1<2	elab-aspect	elab-aspect
D14-1049	61-70	71-82	Overall , our method for SRL is a novel way	to exploit larger variability in the syntactic realizations of predicate-argument relations ,	61-88	61-88	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	1<2	elab-addition	elab-addition
D14-1049	71-82	83-88	to exploit larger variability in the syntactic realizations of predicate-argument relations ,	moving away from pipeline architectures .	61-88	61-88	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	Overall , our method for SRL is a novel way to exploit larger variability in the syntactic realizations of predicate-argument relations , moving away from pipeline architectures .	1<2	elab-addition	elab-addition
D14-1049	89-90	91-100	Experiments show	that our approach improves the robustness of the predictions ,	89-115	89-115	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	1>2	attribution	attribution
D14-1049	1-10	91-100	We introduce a Semantic Role Labeling ( SRL ) parser	that our approach improves the robustness of the predictions ,	1-27	89-115	We introduce a Semantic Role Labeling ( SRL ) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	1<2	evaluation	evaluation
D14-1049	91-100	101-103	that our approach improves the robustness of the predictions ,	producing arc-factored models	89-115	89-115	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	1<2	elab-addition	elab-addition
D14-1049	101-103	104-108	producing arc-factored models	that perform closely to methods	89-115	89-115	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	1<2	elab-addition	elab-addition
D14-1049	104-108	109-115	that perform closely to methods	using unrestricted features from the syntax .	89-115	89-115	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	Experiments show that our approach improves the robustness of the predictions , producing arc-factored models that perform closely to methods using unrestricted features from the syntax .	1<2	manner-means	manner-means
D14-1050	1-20	21-29	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) ,	which is an important step for semantic parsing .	1-29	1-29	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	1<2	elab-addition	elab-addition
D14-1050	1-20	30-44	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) ,	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures ,	1-29	30-84	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	elab-addition	elab-addition
D14-1050	30-44	45-50	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures ,	which we rerank with a classifier	30-84	30-84	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	elab-addition	elab-addition
D14-1050	45-50	51-59	which we rerank with a classifier	trained on two types of semantic tree kernels :	30-84	30-84	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	elab-addition	elab-addition
D14-1050	51-59	60-62	trained on two types of semantic tree kernels :	one processing structures	30-84	30-84	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	elab-enumember	elab-enumember
D14-1050	60-62	63-71	one processing structures	built with words , concepts and Brown clusters ,	30-84	30-84	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	elab-addition	elab-addition
D14-1050	60-62	72-80	one processing structures	and another one using semantic similarity among the words	30-84	30-84	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	joint	joint
D14-1050	72-80	81-84	and another one using semantic similarity among the words	composing the structure .	30-84	30-84	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures , which we rerank with a classifier trained on two types of semantic tree kernels : one processing structures built with words , concepts and Brown clusters , and another one using semantic similarity among the words composing the structure .	1<2	elab-addition	elab-addition
D14-1050	85-94	95-98,102-105	The results on a corpus from the restaurant domain show	that our semantic kernels <*> outperform state-of-the-art rerankers .	85-105	85-105	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	1>2	attribution	attribution
D14-1050	1-20	95-98,102-105	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) ,	that our semantic kernels <*> outperform state-of-the-art rerankers .	1-29	85-105	We present an empirical study on the use of semantic information for Concept Segmentation and Labeling ( CSL ) , which is an important step for semantic parsing .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	1<2	evaluation	evaluation
D14-1050	95-98,102-105	99-101	that our semantic kernels <*> outperform state-of-the-art rerankers .	exploiting similarity measures	85-105	85-105	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers .	1<2	elab-addition	elab-addition
D14-1161	1-9	17-24	Many forms of word relatedness have been developed ,	We introduce a Bayesian probabilistic tensor factorization model	1-16	17-43	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	1>2	bg-goal	bg-goal
D14-1161	1-9	10-16	Many forms of word relatedness have been developed ,	providing different perspectives on word similarity .	1-16	1-16	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	Many forms of word relatedness have been developed , providing different perspectives on word similarity .	1<2	elab-addition	elab-addition
D14-1161	17-24	25-31	We introduce a Bayesian probabilistic tensor factorization model	for synthesizing a single word vector representation	17-43	17-43	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	1<2	elab-addition	elab-addition
D14-1161	25-31	32-43	for synthesizing a single word vector representation	and per-perspective linear transformations from any number of word similarity matrices .	17-43	17-43	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	1<2	joint	joint
D14-1161	17-24	44-48,57-58	We introduce a Bayesian probabilistic tensor factorization model	The resulting word vectors , <*> approximately recreate	17-43	44-69	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	1<2	elab-addition	elab-addition
D14-1161	44-48,57-58	49-56	The resulting word vectors , <*> approximately recreate	when combined with the per-perspective linear transformation ,	44-69	44-69	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	1<2	condition	condition
D14-1161	57-58,65-69	59-64	approximately recreate <*> each word similarity perspective .	while also regularizing and generalizing ,	44-69	44-69	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	The resulting word vectors , when combined with the per-perspective linear transformation , approximately recreate while also regularizing and generalizing , each word similarity perspective .	1<2	joint	joint
D14-1161	17-24	70-87	We introduce a Bayesian probabilistic tensor factorization model	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms ,	17-43	70-102	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	1<2	elab-addition	elab-addition
D14-1161	70-87	88-102	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms ,	and is capable of generalizing to words outside the vocabulary of any particular perspective .	70-102	70-102	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms , and is capable of generalizing to words outside the vocabulary of any particular perspective .	1<2	joint	joint
D14-1161	17-24	103-112	We introduce a Bayesian probabilistic tensor factorization model	We evaluated the word embeddings with GRE antonym questions ,	17-43	103-119	We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices .	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	1<2	evaluation	evaluation
D14-1161	103-112	113-119	We evaluated the word embeddings with GRE antonym questions ,	the result achieves the state-of-the-art performance .	103-119	103-119	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	We evaluated the word embeddings with GRE antonym questions , the result achieves the state-of-the-art performance .	1<2	result	result
D14-1162	1-11	23-32	Recent methods for learning vector space representations of words have succeeded	but the origin of these regularities has remained opaque .	1-32	1-32	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	1>2	contrast	contrast
D14-1162	1-11	12-18	Recent methods for learning vector space representations of words have succeeded	in capturing fine-grained semantic and syntactic regularities	1-32	1-32	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	1<2	elab-addition	elab-addition
D14-1162	1-11	19-22	Recent methods for learning vector space representations of words have succeeded	using vector arithmetic ,	1-32	1-32	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	1<2	manner-means	manner-means
D14-1162	23-32	51-59	but the origin of these regularities has remained opaque .	The result is a new global log-bilinear regression model	1-32	51-82	Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	1>2	bg-compare	bg-compare
D14-1162	33-40	41-44	We analyze and make explicit the model properties	needed for such regularities	33-50	33-50	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	1<2	elab-addition	elab-addition
D14-1162	41-44	45-50	needed for such regularities	to emerge in word vectors .	33-50	33-50	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	1<2	elab-addition	elab-addition
D14-1162	33-40	51-59	We analyze and make explicit the model properties	The result is a new global log-bilinear regression model	33-50	51-82	We analyze and make explicit the model properties needed for such regularities to emerge in word vectors .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	1<2	result	result
D14-1162	51-59	60-73	The result is a new global log-bilinear regression model	that combines the advantages of the two major model families in the literature :	51-82	51-82	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	1<2	elab-addition	elab-addition
D14-1162	60-73	74-82	that combines the advantages of the two major model families in the literature :	global matrix factorization and local context window methods .	51-82	51-82	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	1<2	elab-enumember	elab-enumember
D14-1162	51-59	83-88	The result is a new global log-bilinear regression model	Our model efficiently leverages statistical information	51-82	83-118	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	1<2	elab-addition	elab-addition
D14-1162	83-88	89-101	Our model efficiently leverages statistical information	by training only on the nonzero elements in a word-word co-occurrence matrix ,	83-118	83-118	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	1<2	manner-means	manner-means
D14-1162	89-101	102-118	by training only on the nonzero elements in a word-word co-occurrence matrix ,	rather than on the entire sparse matrix or on individual context windows in a large corpus .	83-118	83-118	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus .	1<2	contrast	contrast
D14-1162	51-59	119-128	The result is a new global log-bilinear regression model	The model produces a vector space with meaningful sub-structure ,	51-82	119-143	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	1<2	elab-addition	elab-addition
D14-1162	119-128	129-143	The model produces a vector space with meaningful sub-structure ,	as evidenced by its performance of 75 % on a recent word analogy task .	119-143	119-143	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	The model produces a vector space with meaningful sub-structure , as evidenced by its performance of 75 % on a recent word analogy task .	1<2	exp-evidence	exp-evidence
D14-1162	51-59	144-156	The result is a new global log-bilinear regression model	It also outperforms related models on similarity tasks and named entity recognition .	51-82	144-156	The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods .	It also outperforms related models on similarity tasks and named entity recognition .	1<2	elab-addition	elab-addition
D14-1163	1-7	8-16	We introduce a novel compositional language model	that works on Predicate-Argument Structures ( PASs ) .	1-16	1-16	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	1<2	elab-addition	elab-addition
D14-1163	1-7	17-26	We introduce a novel compositional language model	Our model jointly learns word representations and their composition functions	1-16	17-32	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	1<2	elab-addition	elab-addition
D14-1163	17-26	27-32	Our model jointly learns word representations and their composition functions	using bag-of-words and dependency-based contexts .	17-32	17-32	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	Our model jointly learns word representations and their composition functions using bag-of-words and dependency-based contexts .	1<2	manner-means	manner-means
D14-1163	33-37	38-44	Unlike previous word-sequence-based models ,	our PAS-based model composes arguments into predicates	33-53	33-53	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	1>2	comparison	comparison
D14-1163	1-7	38-44	We introduce a novel compositional language model	our PAS-based model composes arguments into predicates	1-16	33-53	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	1<2	elab-addition	elab-addition
D14-1163	38-44	45-53	our PAS-based model composes arguments into predicates	by using the category information from the PAS .	33-53	33-53	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	1<2	manner-means	manner-means
D14-1163	38-44	54-63	our PAS-based model composes arguments into predicates	This enables our model to capture longrange dependencies between words	33-53	54-75	Unlike previous word-sequence-based models , our PAS-based model composes arguments into predicates by using the category information from the PAS .	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	1<2	elab-addition	elab-addition
D14-1163	54-63	64-75	This enables our model to capture longrange dependencies between words	and to better handle constructs such as verb-object and subject-verb-object relations .	54-75	54-75	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	This enables our model to capture longrange dependencies between words and to better handle constructs such as verb-object and subject-verb-object relations .	1<2	joint	joint
D14-1163	1-7	76-79	We introduce a novel compositional language model	We verify this experimentally	1-16	76-97	We introduce a novel compositional language model that works on Predicate-Argument Structures ( PASs ) .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	1<2	evaluation	evaluation
D14-1163	76-79	80-84	We verify this experimentally	using two phrase similarity datasets	76-97	76-97	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	1<2	manner-means	manner-means
D14-1163	76-79	85-97	We verify this experimentally	and achieve results comparable to or higher than the previous best results .	76-97	76-97	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	1<2	progression	progression
D14-1163	76-79	98-102	We verify this experimentally	Our system achieves these results	76-97	98-143	We verify this experimentally using two phrase similarity datasets and achieve results comparable to or higher than the previous best results .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	1<2	elab-addition	elab-addition
D14-1163	98-102	103-109	Our system achieves these results	without the need for pre-trained word vectors	98-143	98-143	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	1<2	condition	condition
D14-1163	103-109	110-117	without the need for pre-trained word vectors	and using a much smaller training corpus ;	98-143	98-143	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	1<2	joint	joint
D14-1163	98-102	118-143	Our system achieves these results	despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	98-143	98-143	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	Our system achieves these results without the need for pre-trained word vectors and using a much smaller training corpus ; despite this , for the subject-verb-object dataset our model improves upon the state of the art by as much as 10 % in relative performance .	1<2	joint	joint
D14-1164	1-10	20-23	Broad-coverage relation extraction either requires expensive supervised training data ,	We present an approach	1-19	20-42	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	1>2	bg-goal	bg-goal
D14-1164	1-10	11-19	Broad-coverage relation extraction either requires expensive supervised training data ,	or suffers from drawbacks inherent to distant supervision .	1-19	1-19	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	Broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision .	1<2	joint	joint
D14-1164	20-23	24-33	We present an approach	for providing partial supervision to a distantly supervised relation extractor	20-42	20-42	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	1<2	elab-addition	elab-addition
D14-1164	20-23	34-42	We present an approach	using a small number of carefully selected examples .	20-42	20-42	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	1<2	manner-means	manner-means
D14-1164	20-23	43-49	We present an approach	We compare against established active learning criteria	20-42	43-64	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	1<2	elab-addition	elab-addition
D14-1164	43-49	50-57	We compare against established active learning criteria	and propose a novel criterion to sample examples	43-64	43-64	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	1<2	joint	joint
D14-1164	50-57	58-64	and propose a novel criterion to sample examples	which are both uncertain and representative .	43-64	43-64	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	1<2	elab-addition	elab-addition
D14-1164	43-49	65-88	We compare against established active learning criteria	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	43-64	65-88	We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative .	In this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus .	1<2	result	result
D14-1164	20-23	89-107	We present an approach	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation ,	20-42	89-115	We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples .	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	1<2	evaluation	evaluation
D14-1164	89-107	108-115	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation ,	yielding a net F1 of 37.7 % .	89-115	89-115	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	Our approach gives a substantial increase of 3.9 % end-to-end F1 on the 2013 KBP Slot Filling evaluation , yielding a net F1 of 37.7 % .	1<2	elab-addition	elab-addition
D14-1165	1-10	42-51	While relation extraction has traditionally been viewed as a task	the performance of relation extraction can be improved significantly .	1-51	1-51	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	1>2	contrast	contrast
D14-1165	1-10	11-16	While relation extraction has traditionally been viewed as a task	relying solely on textual data ,	1-51	1-51	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	1<2	elab-addition	elab-addition
D14-1165	17-20	42-51	recent work has shown	the performance of relation extraction can be improved significantly .	1-51	1-51	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	1>2	attribution	attribution
D14-1165	21-41	42-51	that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data ,	the performance of relation extraction can be improved significantly .	1-51	1-51	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	1>2	manner-means	manner-means
D14-1165	42-51	57-66	the performance of relation extraction can be improved significantly .	we propose a tensor decomposition approach for knowledge base embedding	1-51	52-79	While relation extraction has traditionally been viewed as a task relying solely on textual data , recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data , the performance of relation extraction can be improved significantly .	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	1>2	bg-goal	bg-goal
D14-1165	52-56	57-66	Following this new paradigm ,	we propose a tensor decomposition approach for knowledge base embedding	52-79	52-79	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	1>2	bg-general	bg-general
D14-1165	57-66	67-71	we propose a tensor decomposition approach for knowledge base embedding	that is highly scalable ,	52-79	52-79	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	1<2	elab-addition	elab-addition
D14-1165	67-71	72-79	that is highly scalable ,	and is especially suitable for relation extraction .	52-79	52-79	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	1<2	joint	joint
D14-1165	80-89	90-98	By leveraging relational domain knowledge about entity type information ,	our learning algorithm is significantly faster than previous approaches	80-111	80-111	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	1>2	manner-means	manner-means
D14-1165	57-66	90-98	we propose a tensor decomposition approach for knowledge base embedding	our learning algorithm is significantly faster than previous approaches	52-79	80-111	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	1<2	evaluation	evaluation
D14-1165	90-98	99-111	our learning algorithm is significantly faster than previous approaches	and is better able to discover new relations missing from the database .	80-111	80-111	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	By leveraging relational domain knowledge about entity type information , our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database .	1<2	joint	joint
D14-1165	112-122	123-132	In addition , when applied to a relation extraction task ,	our approach alone is comparable to several existing systems ,	112-152	112-152	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	1>2	condition	condition
D14-1165	57-66	123-132	we propose a tensor decomposition approach for knowledge base embedding	our approach alone is comparable to several existing systems ,	52-79	112-152	Following this new paradigm , we propose a tensor decomposition approach for knowledge base embedding that is highly scalable , and is especially suitable for relation extraction .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	1<2	evaluation	evaluation
D14-1165	123-132	133-146	our approach alone is comparable to several existing systems ,	and improves the weighted mean average precision of a state-of-the-art method by 10 points	112-152	112-152	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	1<2	joint	joint
D14-1165	133-146	147-152	and improves the weighted mean average precision of a state-of-the-art method by 10 points	when used as a subcomponent .	112-152	112-152	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	In addition , when applied to a relation extraction task , our approach alone is comparable to several existing systems , and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent .	1<2	condition	condition
D14-1166	1-7,14-23	54-69	A promising approach to relation extraction , <*> exploits an existing database of facts as training data ,	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	1-34	54-69	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	1>2	contrast	contrast
D14-1166	1-7,14-23	8-13	A promising approach to relation extraction , <*> exploits an existing database of facts as training data ,	called weak or distant supervision ,	1-34	1-34	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	1<2	elab-addition	elab-addition
D14-1166	1-7,14-23	24-34	A promising approach to relation extraction , <*> exploits an existing database of facts as training data ,	by aligning it to an unlabeled collection of text documents .	1-34	1-34	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	1<2	manner-means	manner-means
D14-1166	35-38	39-53	Using this approach ,	the task of relation extraction can easily be scaled to hundreds of different relationships .	35-53	35-53	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	1>2	condition	condition
D14-1166	1-7,14-23	39-53	A promising approach to relation extraction , <*> exploits an existing database of facts as training data ,	the task of relation extraction can easily be scaled to hundreds of different relationships .	1-34	35-53	A promising approach to relation extraction , called weak or distant supervision , exploits an existing database of facts as training data , by aligning it to an unlabeled collection of text documents .	Using this approach , the task of relation extraction can easily be scaled to hundreds of different relationships .	1<2	elab-addition	elab-addition
D14-1166	54-69	92-109	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	In this article , we propose a new approach to the problem of weakly supervised relation extraction ,	54-69	92-120	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	1>2	bg-compare	bg-compare
D14-1166	54-69	70-83	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	Most of the proposed solutions to this problem are based on non-convex formulations ,	54-69	70-91	However , distant supervision leads to a challenging multiple instance , multiple label learning problem .	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	1<2	elab-addition	elab-addition
D14-1166	70-83	84-91	Most of the proposed solutions to this problem are based on non-convex formulations ,	and are thus prone to local min-ima .	70-91	70-91	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local min-ima .	1<2	progression	progression
D14-1166	92-109	110-113	In this article , we propose a new approach to the problem of weakly supervised relation extraction ,	based on discriminative clustering	92-120	92-120	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	1<2	bg-general	bg-general
D14-1166	92-109	114-120	In this article , we propose a new approach to the problem of weakly supervised relation extraction ,	and leading to a convex formulation .	92-120	92-120	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	1<2	elab-addition	elab-addition
D14-1166	121-122	123-132	We demonstrate	that our approach outperforms state-of-the-art methods on the challenging dataset	121-141	121-141	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	1>2	attribution	attribution
D14-1166	92-109	123-132	In this article , we propose a new approach to the problem of weakly supervised relation extraction ,	that our approach outperforms state-of-the-art methods on the challenging dataset	92-120	121-141	In this article , we propose a new approach to the problem of weakly supervised relation extraction , based on discriminative clustering and leading to a convex formulation .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	1<2	evaluation	evaluation
D14-1166	123-132	133-141	that our approach outperforms state-of-the-art methods on the challenging dataset	introduced by Riedel et al. ( 2010 ) .	121-141	121-141	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. ( 2010 ) .	1<2	elab-addition	elab-addition
D14-1167	1-5	21-38	We examine the embedding approach	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	1-20	21-38	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	1>2	bg-goal	bg-goal
D14-1167	1-5	6-20	We examine the embedding approach	to reason new relational facts from a large-scale knowledge graph and a text corpus .	1-20	1-20	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus .	1<2	enablement	enablement
D14-1167	21-38	39-62	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	21-38	39-62	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus .	1<2	elab-addition	elab-addition
D14-1167	21-38	63-82	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	21-38	63-82	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space .	1<2	elab-addition	elab-addition
D14-1167	83-93	94-106	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show	that jointly embedding brings promising improvement in the accuracy of predicting facts ,	83-115	83-115	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	1>2	attribution	attribution
D14-1167	21-38	94-106	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	that jointly embedding brings promising improvement in the accuracy of predicting facts ,	21-38	83-115	We propose a novel method of jointly embedding entities and words into the same continuous vector space .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	1<2	evaluation	evaluation
D14-1167	94-106	107-115	that jointly embedding brings promising improvement in the accuracy of predicting facts ,	compared to separately embedding knowledge graphs and text .	83-115	83-115	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	1<2	comparison	comparison
D14-1167	94-106	116-124	that jointly embedding brings promising improvement in the accuracy of predicting facts ,	Particularly , jointly embedding enables the prediction of facts	83-115	116-141	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	1<2	elab-example	elab-example
D14-1167	116-124	125-132	Particularly , jointly embedding enables the prediction of facts	containing entities out of the knowledge graph ,	116-141	116-141	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	1<2	elab-addition	elab-addition
D14-1167	125-132	133-141	containing entities out of the knowledge graph ,	which cannot be handled by previous embedding methods .	116-141	116-141	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	Particularly , jointly embedding enables the prediction of facts containing entities out of the knowledge graph , which cannot be handled by previous embedding methods .	1<2	elab-addition	elab-addition
D14-1167	142-154	162-176	At the same time , concerning the quality of the word embeddings ,	that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	142-176	142-176	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	1>2	condition	condition
D14-1167	155-161	162-176	experiments on the analogical reasoning task show	that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	142-176	142-176	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	1>2	attribution	attribution
D14-1167	94-106	162-176	that jointly embedding brings promising improvement in the accuracy of predicting facts ,	that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	83-115	142-176	Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text .	At the same time , concerning the quality of the word embeddings , experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec ( Skip-Gram ) .	1<2	elab-example	elab-example
D14-1168	1-10	11-18	We propose a novel abstractive summarization system for product reviews	by taking advantage of their discourse structure .	1-18	1-18	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	1<2	manner-means	manner-means
D14-1168	1-10	19-28	We propose a novel abstractive summarization system for product reviews	First , we apply a discourse parser to each review	1-18	19-38	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	1<2	elab-process_step	elab-process_step
D14-1168	19-28	29-38	First , we apply a discourse parser to each review	and obtain a discourse tree representation for every review .	19-38	19-38	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	1<2	joint	joint
D14-1168	19-28	39-44	First , we apply a discourse parser to each review	We then modify the discourse trees	19-38	39-55	First , we apply a discourse parser to each review and obtain a discourse tree representation for every review .	We then modify the discourse trees such that every leaf node only contains the aspect words .	1<2	elab-addition	elab-addition
D14-1168	39-44	45-55	We then modify the discourse trees	such that every leaf node only contains the aspect words .	39-55	39-55	We then modify the discourse trees such that every leaf node only contains the aspect words .	We then modify the discourse trees such that every leaf node only contains the aspect words .	1<2	elab-addition	elab-addition
D14-1168	1-10	56-63	We propose a novel abstractive summarization system for product reviews	Second , we aggregate the aspect discourse trees	1-18	56-68	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	Second , we aggregate the aspect discourse trees and generate a graph .	1<2	elab-process_step	elab-process_step
D14-1168	56-63	64-68	Second , we aggregate the aspect discourse trees	and generate a graph .	56-68	56-68	Second , we aggregate the aspect discourse trees and generate a graph .	Second , we aggregate the aspect discourse trees and generate a graph .	1<2	joint	joint
D14-1168	56-63	69-73	Second , we aggregate the aspect discourse trees	We then select a subgraph	56-68	69-99	Second , we aggregate the aspect discourse trees and generate a graph .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	1<2	elab-addition	elab-addition
D14-1168	69-73	74-84	We then select a subgraph	representing the most important aspects and the rhetorical relations between them	69-99	69-99	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	1<2	elab-addition	elab-addition
D14-1168	69-73	85-89	We then select a subgraph	using a PageRank algorithm ,	69-99	69-99	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	1<2	manner-means	manner-means
D14-1168	69-73	90-99	We then select a subgraph	and transform the selected subgraph into an aspect tree .	69-99	69-99	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm , and transform the selected subgraph into an aspect tree .	1<2	joint	joint
D14-1168	1-10	100-107	We propose a novel abstractive summarization system for product reviews	Finally , we generate a natural language summary	1-18	100-114	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	Finally , we generate a natural language summary by applying a template-based NLG framework .	1<2	elab-process_step	elab-process_step
D14-1168	100-107	108-114	Finally , we generate a natural language summary	by applying a template-based NLG framework .	100-114	100-114	Finally , we generate a natural language summary by applying a template-based NLG framework .	Finally , we generate a natural language summary by applying a template-based NLG framework .	1<2	manner-means	manner-means
D14-1168	115-129	130-139	Quantitative and qualitative analysis of the results , based on two user studies , show	that our approach significantly outperforms extractive and abstractive baselines .	115-139	115-139	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	1>2	attribution	attribution
D14-1168	1-10	130-139	We propose a novel abstractive summarization system for product reviews	that our approach significantly outperforms extractive and abstractive baselines .	1-18	115-139	We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure .	Quantitative and qualitative analysis of the results , based on two user studies , show that our approach significantly outperforms extractive and abstractive baselines .	1<2	evaluation	evaluation
D14-1169	1-16	48-61	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis	In this paper , we explore a novel idea , sentiment distribution consistency ,	1-25	48-88	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	1>2	bg-goal	bg-goal
D14-1169	1-16	17-25	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis	which is a central task in sentiment analysis .	1-25	1-25	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	Clustering aspect-related phrases in terms of product's property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis .	1<2	elab-addition	elab-addition
D14-1169	26-36	48-61	Most of existing methods for addressing this problem are context-based models	In this paper , we explore a novel idea , sentiment distribution consistency ,	26-47	48-88	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	1>2	bg-compare	bg-compare
D14-1169	37-38	39-47	which assume	that domain synonymous phrases share similar co-occurrence contexts .	26-47	26-47	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	1>2	attribution	attribution
D14-1169	26-36	39-47	Most of existing methods for addressing this problem are context-based models	that domain synonymous phrases share similar co-occurrence contexts .	26-47	26-47	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts .	1<2	elab-addition	elab-addition
D14-1169	62-63	64-88	which states	that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	48-88	48-88	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	1>2	attribution	attribution
D14-1169	48-61	64-88	In this paper , we explore a novel idea , sentiment distribution consistency ,	that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	48-88	48-88	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	1<2	elab-addition	elab-addition
D14-1169	89-97	98-112	Through formalizing sentiment distribution consistency as soft constraint ,	we propose a novel unsupervised model in the framework of Posterior Regularization ( PR )	89-117	89-117	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	1>2	manner-means	manner-means
D14-1169	48-61	98-112	In this paper , we explore a novel idea , sentiment distribution consistency ,	we propose a novel unsupervised model in the framework of Posterior Regularization ( PR )	48-88	89-117	In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e.g. "price" , "money" , "worth" , and "cost" ) of the same aspect tend to have consistent sentiment distribution .	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	1<2	elab-addition	elab-addition
D14-1169	98-112	113-117	we propose a novel unsupervised model in the framework of Posterior Regularization ( PR )	to cluster aspect-related phrases .	89-117	89-117	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	1<2	enablement	enablement
D14-1169	118-119	120-126	Experiments demonstrate	that our approach outperforms baselines remarkably .	118-126	118-126	Experiments demonstrate that our approach outperforms baselines remarkably .	Experiments demonstrate that our approach outperforms baselines remarkably .	1>2	attribution	attribution
D14-1169	98-112	120-126	we propose a novel unsupervised model in the framework of Posterior Regularization ( PR )	that our approach outperforms baselines remarkably .	89-117	118-126	Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect-related phrases .	Experiments demonstrate that our approach outperforms baselines remarkably .	1<2	evaluation	evaluation
D14-1170	16-22	23-36	Given multiple reference papers as input ,	the task aims to generate a related work section for a target paper .	16-36	16-36	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	1>2	condition	condition
D14-1170	1-15	23-36	In this paper , we investigate a challenging task of automatic related work generation .	the task aims to generate a related work section for a target paper .	1-15	16-36	In this paper , we investigate a challenging task of automatic related work generation .	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	1<2	elab-addition	elab-addition
D14-1170	23-36	37-60	the task aims to generate a related work section for a target paper .	The generated related work section can be used as a draft for the author to complete his or her final related work section .	16-36	37-60	Given multiple reference papers as input , the task aims to generate a related work section for a target paper .	The generated related work section can be used as a draft for the author to complete his or her final related work section .	1<2	elab-addition	elab-addition
D14-1170	1-15	61-68	In this paper , we investigate a challenging task of automatic related work generation .	We propose our Automatic Related Work Generation system	1-15	61-75	In this paper , we investigate a challenging task of automatic related work generation .	We propose our Automatic Related Work Generation system called ARWG to address this task .	1<2	elab-addition	elab-addition
D14-1170	61-68	69-70	We propose our Automatic Related Work Generation system	called ARWG	61-75	61-75	We propose our Automatic Related Work Generation system called ARWG to address this task .	We propose our Automatic Related Work Generation system called ARWG to address this task .	1<2	elab-addition	elab-addition
D14-1170	61-68	71-75	We propose our Automatic Related Work Generation system	to address this task .	61-75	61-75	We propose our Automatic Related Work Generation system called ARWG to address this task .	We propose our Automatic Related Work Generation system called ARWG to address this task .	1<2	enablement	enablement
D14-1170	61-68	76-81	We propose our Automatic Related Work Generation system	It first exploits a PLSA model	61-75	76-108	We propose our Automatic Related Work Generation system called ARWG to address this task .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	1<2	elab-process_step	elab-process_step
D14-1170	76-81	82-95	It first exploits a PLSA model	to split the sentence set of the given papers into different topic-biased parts ,	76-108	76-108	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	1<2	enablement	enablement
D14-1170	61-68	96-100	We propose our Automatic Related Work Generation system	and then applies regression models	61-75	76-108	We propose our Automatic Related Work Generation system called ARWG to address this task .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	1<2	elab-process_step	elab-process_step
D14-1170	96-100	101-108	and then applies regression models	to learn the importance of the sentences .	76-108	76-108	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences .	1<2	enablement	enablement
D14-1170	61-68	109-115	We propose our Automatic Related Work Generation system	At last it employs an optimization framework	61-75	109-122	We propose our Automatic Related Work Generation system called ARWG to address this task .	At last it employs an optimization framework to generate the related work section .	1<2	elab-process_step	elab-process_step
D14-1170	109-115	116-122	At last it employs an optimization framework	to generate the related work section .	109-122	109-122	At last it employs an optimization framework to generate the related work section .	At last it employs an optimization framework to generate the related work section .	1<2	enablement	enablement
D14-1170	123-139	140-153	Our evaluation results on a test set of 150 target papers along with their reference papers show	that our proposed ARWG system can generate related work sections with better quality .	123-153	123-153	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	1>2	attribution	attribution
D14-1170	61-68	140-153	We propose our Automatic Related Work Generation system	that our proposed ARWG system can generate related work sections with better quality .	61-75	123-153	We propose our Automatic Related Work Generation system called ARWG to address this task .	Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality .	1<2	evaluation	evaluation
D14-1170	154-161	162-172	A user study is also performed to show	ARWG can achieve an improvement over generic multi-document summarization baselines .	154-172	154-172	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	1>2	attribution	attribution
D14-1170	61-68	162-172	We propose our Automatic Related Work Generation system	ARWG can achieve an improvement over generic multi-document summarization baselines .	61-75	154-172	We propose our Automatic Related Work Generation system called ARWG to address this task .	A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines .	1<2	evaluation	evaluation
D14-1171	1-5	15-31	There are several NLP systems	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	1-14	15-31	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	1>2	contrast	contrast
D14-1171	1-5	6-14	There are several NLP systems	whose accuracy depends crucially on finding misspellings fast .	1-14	1-14	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	There are several NLP systems whose accuracy depends crucially on finding misspellings fast .	1<2	elab-addition	elab-addition
D14-1171	15-31	32-40	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	We present a novel algorithm for misspelling detection ,	15-31	32-55	However , the classical approach is based on a quadratic time algorithm with 80 % coverage .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	1>2	bg-compare	bg-compare
D14-1171	32-40	41-45	We present a novel algorithm for misspelling detection ,	which runs in constant time	32-55	32-55	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	1<2	elab-addition	elab-addition
D14-1171	41-45	46-55	which runs in constant time	and improves the coverage to more than 96 % .	32-55	32-55	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	1<2	joint	joint
D14-1171	32-40	56-66	We present a novel algorithm for misspelling detection ,	We use this algorithm together with a cross document coreference system	32-55	56-74	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	1<2	elab-addition	elab-addition
D14-1171	56-66	67-74	We use this algorithm together with a cross document coreference system	in order to find proper name misspellings .	56-74	56-74	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	We use this algorithm together with a cross document coreference system in order to find proper name misspellings .	1<2	enablement	enablement
D14-1171	32-40	75-86	We present a novel algorithm for misspelling detection ,	The experiments confirmed significant improvement over the state of the art .	32-55	75-86	We present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % .	The experiments confirmed significant improvement over the state of the art .	1<2	evaluation	evaluation
D14-1172	1-11	17-22	Learning from errors is a crucial aspect of improving expertise .	we discuss a robust statistical framework	1-11	12-39	Learning from errors is a crucial aspect of improving expertise .	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	1>2	bg-goal	bg-goal
D14-1172	12-16	17-22	Based on this notion ,	we discuss a robust statistical framework	12-39	12-39	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	1>2	bg-general	bg-general
D14-1172	17-22	23-39	we discuss a robust statistical framework	for analysing the impact of different error types on machine translation ( MT ) output quality .	12-39	12-39	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	1<2	elab-addition	elab-addition
D14-1172	17-22	40-48	we discuss a robust statistical framework	Our approach is based on linear mixed-effects models ,	12-39	40-75	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	1<2	elab-addition	elab-addition
D14-1172	40-48	49-67	Our approach is based on linear mixed-effects models ,	which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting	40-75	40-75	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	1<2	elab-addition	elab-addition
D14-1172	49-67	68-75	which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting	from which the empirical observations are drawn .	40-75	40-75	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn .	1<2	elab-addition	elab-addition
D14-1172	17-22	76-84	we discuss a robust statistical framework	Our experiments are carried out on different language pairs	12-39	76-94	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	1<2	elab-addition	elab-addition
D14-1172	76-84	85-94	Our experiments are carried out on different language pairs	involving Chinese , Arabic and Russian as target languages .	76-94	76-94	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages .	1<2	elab-enumember	elab-enumember
D14-1172	17-22	95-99	we discuss a robust statistical framework	Interesting findings are reported ,	12-39	95-126	Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	1<2	evaluation	evaluation
D14-1172	95-99	100-121	Interesting findings are reported ,	concerning the impact of different error types both at the level of human perception of quality and with respect to performance results	95-126	95-126	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	1<2	elab-addition	elab-addition
D14-1172	100-121	122-126	concerning the impact of different error types both at the level of human perception of quality and with respect to performance results	measured with automatic metrics .	95-126	95-126	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics .	1<2	elab-addition	elab-addition
D14-1173	1,8-20	34-52	Languages <*> often have to be segmented for statistical machine translation ( SMT ) .	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage ,	1-20	34-60	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	1>2	contrast	contrast
D14-1173	1,8-20	2-7	Languages <*> often have to be segmented for statistical machine translation ( SMT ) .	that have no explicit word delimiters	1-20	1-20	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	1<2	elab-addition	elab-addition
D14-1173	1,8-20	21-27	Languages <*> often have to be segmented for statistical machine translation ( SMT ) .	This is commonly performed by automated segmenters	1-20	21-33	Languages that have no explicit word delimiters often have to be segmented for statistical machine translation ( SMT ) .	This is commonly performed by automated segmenters trained on manually annotated corpora .	1<2	elab-addition	elab-addition
D14-1173	21-27	28-33	This is commonly performed by automated segmenters	trained on manually annotated corpora .	21-33	21-33	This is commonly performed by automated segmenters trained on manually annotated corpora .	This is commonly performed by automated segmenters trained on manually annotated corpora .	1<2	elab-addition	elab-addition
D14-1173	34-52	120-123	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage ,	We formulated an approach	34-60	120-138	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	1>2	bg-compare	bg-compare
D14-1173	34-52	53-60	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage ,	and may not be suitable for SMT .	34-60	34-60	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	1<2	joint	joint
D14-1173	34-52	61-68	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage ,	An analysis was performed to test this hypothesis	34-60	61-82	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	1<2	exp-evidence	exp-evidence
D14-1173	61-68	69-82	An analysis was performed to test this hypothesis	using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	61-82	61-82	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	An analysis was performed to test this hypothesis using a manually annotated word alignment ( WA ) corpus for Chinese-English SMT .	1<2	manner-means	manner-means
D14-1173	83-85	86-95,111-119	An analysis revealed	that 74.60 % of the sentences in the WA corpus <*> will contain conflicts with the gold WA annotations .	83-119	83-119	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	1>2	attribution	attribution
D14-1173	34-52	86-95,111-119	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage ,	that 74.60 % of the sentences in the WA corpus <*> will contain conflicts with the gold WA annotations .	34-60	83-119	However , the word segmentation ( WS ) schemes of these annotated corpora are handcrafted for general usage , and may not be suitable for SMT .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	1<2	exp-evidence	exp-evidence
D14-1173	86-95,111-119	96-101	that 74.60 % of the sentences in the WA corpus <*> will contain conflicts with the gold WA annotations .	if segmented using an automated segmenter	83-119	83-119	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	1<2	condition	condition
D14-1173	96-101	102-110	if segmented using an automated segmenter	trained on the Penn Chinese Treebank ( CTB )	83-119	83-119	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	An analysis revealed that 74.60 % of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank ( CTB ) will contain conflicts with the gold WA annotations .	1<2	elab-addition	elab-addition
D14-1173	120-123	124-133	We formulated an approach	based on word splitting with reference to the annotated WA	120-138	120-138	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	1<2	bg-general	bg-general
D14-1173	120-123	134-138	We formulated an approach	to alleviate these conflicts .	120-138	120-138	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	1<2	enablement	enablement
D14-1173	139-141	142-153	Experimental results show	that the refined WS reduced word alignment error rate by 6.82 %	139-179	139-179	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	1>2	attribution	attribution
D14-1173	120-123	142-153	We formulated an approach	that the refined WS reduced word alignment error rate by 6.82 %	120-138	139-179	We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	1<2	evaluation	evaluation
D14-1173	142-153	154-174	that the refined WS reduced word alignment error rate by 6.82 %	and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora	139-179	139-179	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	1<2	joint	joint
D14-1173	142-153	175-179	that the refined WS reduced word alignment error rate by 6.82 %	compared to related work .	139-179	139-179	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	Experimental results show that the refined WS reduced word alignment error rate by 6.82 % and achieved the highest BLEU improvement ( 0.63 on average ) on the Chinese-English open machine translation ( OpenMT ) corpora compared to related work .	1<2	comparison	comparison
D14-1174	1-15	16-25	To overcome the scarceness of bilingual corpora for some language pairs in machine translation ,	pivot-based SMT uses pivot language as a `` bridge ''	1-35	1-35	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	1>2	enablement	enablement
D14-1174	16-25	52-60	pivot-based SMT uses pivot language as a `` bridge ''	In this paper , we present a novel approach	1-35	52-77	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	1>2	bg-compare	bg-compare
D14-1174	16-25	26-35	pivot-based SMT uses pivot language as a `` bridge ''	to generate source-target translation from source-pivot and pivot-target translation .	1-35	1-35	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	1<2	elab-addition	elab-addition
D14-1174	16-25	36-51	pivot-based SMT uses pivot language as a `` bridge ''	One of the key issues is to estimate the probabilities for the generated phrase pairs .	1-35	36-51	To overcome the scarceness of bilingual corpora for some language pairs in machine translation , pivot-based SMT uses pivot language as a `` bridge '' to generate source-target translation from source-pivot and pivot-target translation .	One of the key issues is to estimate the probabilities for the generated phrase pairs .	1<2	elab-addition	elab-addition
D14-1174	52-60	61-65	In this paper , we present a novel approach	to calculate the translation probability	52-77	52-77	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	1<2	enablement	enablement
D14-1174	52-60	66-77	In this paper , we present a novel approach	by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	52-77	52-77	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	1<2	manner-means	manner-means
D14-1174	78-86	87-98	Experimental results on Europarl data and web data show	that our method leads to significant improvements over the baseline systems .	78-98	78-98	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	1>2	attribution	attribution
D14-1174	52-60	87-98	In this paper , we present a novel approach	that our method leads to significant improvements over the baseline systems .	52-77	78-98	In this paper , we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs .	Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems .	1<2	evaluation	evaluation
D14-1175	1-13	37-41	Translating into morphologically rich languages is a particularly difficult problem in machine translation	We present a general approach	1-36	37-54	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	1>2	bg-goal	bg-goal
D14-1175	1-13	14-26	Translating into morphologically rich languages is a particularly difficult problem in machine translation	due to the high degree of inflectional ambiguity in the target language ,	1-36	1-36	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	1<2	cause	cause
D14-1175	1-13	27-36	Translating into morphologically rich languages is a particularly difficult problem in machine translation	often only poorly captured by existing word translation models .	1-36	1-36	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models .	1<2	elab-addition	elab-addition
D14-1175	37-41	42-48	We present a general approach	that exploits source-side contexts of foreign words	37-54	37-54	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	1<2	elab-addition	elab-addition
D14-1175	37-41	49-54	We present a general approach	to improve translation prediction accuracy .	37-54	37-54	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	1<2	enablement	enablement
D14-1175	37-41	55-63	We present a general approach	Our approach is based on a probabilistic neural network	37-54	55-74	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	1<2	elab-addition	elab-addition
D14-1175	55-63	64-74	Our approach is based on a probabilistic neural network	which does not require linguistic annotation nor manual feature engineering .	55-74	55-74	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering .	1<2	elab-addition	elab-addition
D14-1175	37-41	75-90	We present a general approach	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	37-54	75-90	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	We report significant improvements in word translation prediction accuracy for three morphologically rich target languages .	1<2	evaluation	evaluation
D14-1175	37-41	91-117	We present a general approach	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	37-54	91-117	We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy .	In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality .	1<2	evaluation	evaluation
D14-1176	1-6	7-13	This paper presents a novel approach	to improve reordering in phrase-based machine translation	1-29	1-29	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	1<2	elab-addition	elab-addition
D14-1176	1-6	14-29	This paper presents a novel approach	by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	1-29	1-29	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	1<2	manner-means	manner-means
D14-1176	1-6	30-31,36-39	This paper presents a novel approach	Our method <*> is simple in implementation	1-29	30-48	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	1<2	elab-addition	elab-addition
D14-1176	30-31,36-39	32-35	Our method <*> is simple in implementation	to include syntactic information	30-48	30-48	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	1<2	elab-addition	elab-addition
D14-1176	36-39	40-48	is simple in implementation	and requires minimal changes in the decoding algorithm .	30-48	30-48	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm .	1<2	joint	joint
D14-1176	1-6	49-62	This paper presents a novel approach	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	1-29	49-62	This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer , syntactic representations of units of bilingual language models ( BiLMs ) .	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	1<2	evaluation	evaluation
D14-1176	49-62	63-92	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	49-62	63-92	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline , as well as over the lexicalized BiLM by Niehues et al. ( 2011 ) .	1<2	elab-example	elab-example
D14-1176	49-62	93-110	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained	49-62	93-120	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	1<2	elab-example	elab-example
D14-1176	93-110	111-120	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained	by combining our dependency BiLM with a lexicalized BiLM .	93-120	93-120	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	Further improvements of up to 0.45 BLEU for Arabic-English and up to 0.59 BLEU for Chinese-English are obtained by combining our dependency BiLM with a lexicalized BiLM .	1<2	manner-means	manner-means
D14-1176	49-62	121-138	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	49-62	121-138	The approach is evaluated in a series of Arabic-English and Chinese-English translation experiments .	An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit .	1<2	elab-example	elab-example
D14-1177	1-21	22-34	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	In this paper , we exploit two independent observations about term translations :	1-21	22-64	Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem , yet with many potential applications .	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	1>2	bg-goal	bg-goal
D14-1177	22-34	35-47	In this paper , we exploit two independent observations about term translations :	( a ) terms are often formed by corresponding sub-lexical units across languages	22-64	22-64	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	1<2	elab-enumember	elab-enumember
D14-1177	22-34	48-64	In this paper , we exploit two independent observations about term translations :	and ( b ) a term and its translation tend to appear in similar lexical context .	22-64	22-64	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	1<2	elab-enumember	elab-enumember
D14-1177	65-70	71-84	Based on the first observation ,	we develop a new character n-gram compositional method , a logistic regression classifier ,	65-94	65-94	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	1>2	bg-general	bg-general
D14-1177	35-47	71-84	( a ) terms are often formed by corresponding sub-lexical units across languages	we develop a new character n-gram compositional method , a logistic regression classifier ,	22-64	65-94	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	1<2	elab-addition	elab-addition
D14-1177	71-84	85-94	we develop a new character n-gram compositional method , a logistic regression classifier ,	for learning a string similarity measure of term translations .	65-94	65-94	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	Based on the first observation , we develop a new character n-gram compositional method , a logistic regression classifier , for learning a string similarity measure of term translations .	1<2	elab-addition	elab-addition
D14-1177	95-100	101-107	According to the second observation ,	we use an existing context-based approach .	95-107	95-107	According to the second observation , we use an existing context-based approach .	According to the second observation , we use an existing context-based approach .	1>2	bg-general	bg-general
D14-1177	48-64	101-107	and ( b ) a term and its translation tend to appear in similar lexical context .	we use an existing context-based approach .	22-64	95-107	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	According to the second observation , we use an existing context-based approach .	1<2	elab-addition	elab-addition
D14-1177	22-34	108-121	In this paper , we exploit two independent observations about term translations :	For evaluation , we investigate the performance of compositional and context-based methods on :	22-64	108-150	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	1<2	evaluation	evaluation
D14-1177	108-121	122-150	For evaluation , we investigate the performance of compositional and context-based methods on :	( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	108-150	108-150	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	For evaluation , we investigate the performance of compositional and context-based methods on : ( a ) similar and unrelated languages , ( b ) corpora of different degree of comparability and ( c ) the translation of frequent and rare terms .	1<2	elab-enumember	elab-enumember
D14-1177	22-34	151-169	In this paper , we exploit two independent observations about term translations :	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model	22-64	151-180	In this paper , we exploit two independent observations about term translations : ( a ) terms are often formed by corresponding sub-lexical units across languages and ( b ) a term and its translation tend to appear in similar lexical context .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	1<2	evaluation	evaluation
D14-1177	151-169	170-180	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model	and we show substantial improvements over the two translation signals .	151-180	151-180	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	Finally , we combine the two translation clues , namely string and contextual similarity , in a linear model and we show substantial improvements over the two translation signals .	1<2	joint	joint
D14-1178	1-10	53-68	Vector space models ( VSMs ) are mathematically well-defined frameworks	The high dimensionality of vectors , however , is a barrier to the performance of methods	1-22	53-72	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	1>2	contrast	contrast
D14-1178	1-10	11-22	Vector space models ( VSMs ) are mathematically well-defined frameworks	that have been widely used in the distributional approaches to semantics .	1-22	1-22	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	1<2	elab-addition	elab-addition
D14-1178	1-10	23-31	Vector space models ( VSMs ) are mathematically well-defined frameworks	In VSMs , high-dimensional vectors represent linguistic entities .	1-22	23-31	Vector space models ( VSMs ) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics .	In VSMs , high-dimensional vectors represent linguistic entities .	1<2	elab-addition	elab-addition
D14-1178	23-31	32-42,46-52	In VSMs , high-dimensional vectors represent linguistic entities .	In an application , the similarity of vectorsand thus the entities <*> is computed by a distance formula .	23-31	32-52	In VSMs , high-dimensional vectors represent linguistic entities .	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	1<2	elab-addition	elab-addition
D14-1178	32-42,46-52	43-45	In an application , the similarity of vectorsand thus the entities <*> is computed by a distance formula .	that they represent	32-52	32-52	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	In an application , the similarity of vectorsand thus the entities that they represent is computed by a distance formula .	1<2	elab-addition	elab-addition
D14-1178	53-68	86-91	The high dimensionality of vectors , however , is a barrier to the performance of methods	This paper introduces a novel technique	53-72	86-109	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	1>2	bg-compare	bg-compare
D14-1178	53-68	69-72	The high dimensionality of vectors , however , is a barrier to the performance of methods	that employ VSMs .	53-72	53-72	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	1<2	elab-addition	elab-addition
D14-1178	53-68	73-80	The high dimensionality of vectors , however , is a barrier to the performance of methods	Consequently , a dimensionality reduction technique is employed	53-72	73-85	The high dimensionality of vectors , however , is a barrier to the performance of methods that employ VSMs .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	1<2	result	result
D14-1178	73-80	81-85	Consequently , a dimensionality reduction technique is employed	to alleviate this problem .	73-85	73-85	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	Consequently , a dimensionality reduction technique is employed to alleviate this problem .	1<2	enablement	enablement
D14-1178	86-91	92-98	This paper introduces a novel technique	called Random Manhattan Indexing ( RMI )	86-109	86-109	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	1<2	elab-addition	elab-addition
D14-1178	86-91	99-109	This paper introduces a novel technique	for the construction of l1 normed VSMs at reduced dimensionality .	86-109	86-109	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	1<2	elab-addition	elab-addition
D14-1178	86-91	110-128	This paper introduces a novel technique	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	86-109	110-128	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	RMI combines the construction of a VSM and dimension reduction into an incremental and thus scalable two-step procedure .	1<2	elab-addition	elab-addition
D14-1178	129-135	136-143	In order to attain its goal ,	RMI employs the sparse Cauchy random projections .	129-143	129-143	In order to attain its goal , RMI employs the sparse Cauchy random projections .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	1>2	enablement	enablement
D14-1178	86-91	136-143	This paper introduces a novel technique	RMI employs the sparse Cauchy random projections .	86-109	129-143	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	In order to attain its goal , RMI employs the sparse Cauchy random projections .	1<2	elab-addition	elab-addition
D14-1178	86-91	144-154	This paper introduces a novel technique	We further introduce Random Manhattan Integer Indexing ( RMII ) :	86-109	144-161	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	1<2	elab-addition	elab-addition
D14-1178	144-154	155-161	We further introduce Random Manhattan Integer Indexing ( RMII ) :	a computationally enhanced version of RMI .	144-161	144-161	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	We further introduce Random Manhattan Integer Indexing ( RMII ) : a computationally enhanced version of RMI .	1<2	elab-addition	elab-addition
D14-1178	162-168	169-175	As shown in the reported experiments ,	RMI and RMII can be used reliably	162-190	162-190	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	1>2	bg-goal	bg-goal
D14-1178	86-91	169-175	This paper introduces a novel technique	RMI and RMII can be used reliably	86-109	162-190	This paper introduces a novel technique called Random Manhattan Indexing ( RMI ) for the construction of l1 normed VSMs at reduced dimensionality .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	1<2	evaluation	evaluation
D14-1178	169-175	176-190	RMI and RMII can be used reliably	to estimate the l1 distances between vectors in a vector space of low dimensionality .	162-190	162-190	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	As shown in the reported experiments , RMI and RMII can be used reliably to estimate the l1 distances between vectors in a vector space of low dimensionality .	1<2	enablement	enablement
D14-1179	1-11	12-15	In this paper , we propose a novel neural network model	called RNN Encoder- Decoder	1-26	1-26	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	1<2	elab-addition	elab-addition
D14-1179	1-11	16-26	In this paper , we propose a novel neural network model	that consists of two recurrent neural networks ( RNN ) .	1-26	1-26	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	1<2	elab-addition	elab-addition
D14-1179	16-26	27-39	that consists of two recurrent neural networks ( RNN ) .	One RNN encodes a sequence of symbols into a fixed-length vector representation ,	1-26	27-51	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	1<2	elab-addition	elab-addition
D14-1179	27-39	40-51	One RNN encodes a sequence of symbols into a fixed-length vector representation ,	and the other decodes the representation into another sequence of symbols .	27-51	27-51	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	One RNN encodes a sequence of symbols into a fixed-length vector representation , and the other decodes the representation into another sequence of symbols .	1<2	joint	joint
D14-1179	1-11	52-62	In this paper , we propose a novel neural network model	The encoder and decoder of the proposed model are jointly trained	1-26	52-76	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	1<2	elab-addition	elab-addition
D14-1179	52-62	63-71	The encoder and decoder of the proposed model are jointly trained	to maximize the conditional probability of a target sequence	52-76	52-76	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	1<2	enablement	enablement
D14-1179	52-62	72-76	The encoder and decoder of the proposed model are jointly trained	given a source sequence .	52-76	52-76	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .	1<2	condition	condition
D14-1179	1-11	77-89	In this paper , we propose a novel neural network model	The performance of a statistical machine translation system is empirically found to improve	1-26	77-112	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	1<2	evaluation	evaluation
D14-1179	77-89	90-97	The performance of a statistical machine translation system is empirically found to improve	by using the conditional probabilities of phrase pairs	77-112	77-112	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	1<2	manner-means	manner-means
D14-1179	90-97	98-112	by using the conditional probabilities of phrase pairs	computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	77-112	77-112	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model .	1<2	elab-addition	elab-addition
D14-1179	113-116	117-131	Qualitatively , we show	that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	113-131	113-131	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	1>2	attribution	attribution
D14-1179	1-11	117-131	In this paper , we propose a novel neural network model	that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	1-26	113-131	In this paper , we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks ( RNN ) .	Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .	1<2	evaluation	evaluation
D14-1180	1-27	28-39	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest	that represents all possible rules consistent with a fixed word alignment .	1-39	1-39	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	1<2	elab-addition	elab-addition
D14-1180	40-55	56-58,72-80	While type-based MCMC has been shown to be effective in a number of NLP applications ,	our setting , <*> presents a number of challenges to type-based inference .	40-80	40-80	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	1>2	comparison	comparison
D14-1180	1-27	56-58,72-80	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest	our setting , <*> presents a number of challenges to type-based inference .	1-39	40-80	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	1<2	elab-addition	elab-addition
D14-1180	56-58,72-80	59-71	our setting , <*> presents a number of challenges to type-based inference .	where the tree structure of the sentence is itself a hidden variable ,	40-80	40-80	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	While type-based MCMC has been shown to be effective in a number of NLP applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference .	1<2	elab-addition	elab-addition
D14-1180	1-27	81-91	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest	We describe methods for defining variable types and efficiently indexing variables	1-39	81-98	This paper applies type-based Markov Chain Monte Carlo ( MCMC ) algorithms to the problem of learning Synchronous Context-Free Grammar ( SCFG ) rules from a forest that represents all possible rules consistent with a fixed word alignment .	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	1<2	evaluation	evaluation
D14-1180	81-91	92-98	We describe methods for defining variable types and efficiently indexing variables	in order to overcome these challenges .	81-98	81-98	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	1<2	enablement	enablement
D14-1180	81-91	99-114	We describe methods for defining variable types and efficiently indexing variables	These methods lead to improvements in both log likelihood and BLEU score in our experiments .	81-98	99-114	We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges .	These methods lead to improvements in both log likelihood and BLEU score in our experiments .	1<2	elab-addition	elab-addition
D14-1181	1-14	15-26	We report on a series of experiments with convolutional neural networks ( CNN )	trained on top of pre-trained word vectors for sentence-level classification tasks .	1-26	1-26	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	1<2	elab-addition	elab-addition
D14-1181	27-28	29-46	We show	that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	27-46	27-46	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	1>2	attribution	attribution
D14-1181	1-14	29-46	We report on a series of experiments with convolutional neural networks ( CNN )	that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	1-26	27-46	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks .	1<2	evaluation	evaluation
D14-1181	1-14	47-57	We report on a series of experiments with convolutional neural networks ( CNN )	Learning task-specific vectors through fine-tuning offers further gains in performance .	1-26	47-57	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	Learning task-specific vectors through fine-tuning offers further gains in performance .	1<2	evaluation	evaluation
D14-1181	1-14	58-66	We report on a series of experiments with convolutional neural networks ( CNN )	We additionally propose a simple modification to the architecture	1-26	58-78	We report on a series of experiments with convolutional neural networks ( CNN ) trained on top of pre-trained word vectors for sentence-level classification tasks .	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	1<2	elab-addition	elab-addition
D14-1181	58-66	67-78	We additionally propose a simple modification to the architecture	to allow for the use of both task-specific and static vectors .	58-78	58-78	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	1<2	enablement	enablement
D14-1181	58-66	79-97	We additionally propose a simple modification to the architecture	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks ,	58-78	79-105	We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors .	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	1<2	evaluation	evaluation
D14-1181	79-97	98-105	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks ,	which include sentiment analysis and question classification .	79-105	79-105	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .	1<2	elab-example	elab-example
D14-1182	1-15	25-38	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models	In practice , however , it is often more convenient to cut corners ,	1-24	25-50	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them .	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	1>2	contrast	contrast
D14-1182	1-15	16-24	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models	by generating many samples and averaging over them .	1-24	1-24	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them .	Markov chain Monte Carlo ( MCMC ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them .	1<2	manner-means	manner-means
D14-1182	25-38	39-43	In practice , however , it is often more convenient to cut corners ,	using only a single sample	25-50	25-50	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	1<2	elab-addition	elab-addition
D14-1182	39-43	44-50	using only a single sample	or following a suboptimal averaging strategy .	25-50	25-50	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	1<2	joint	joint
D14-1182	25-38	51-55	In practice , however , it is often more convenient to cut corners ,	We systematically study different strategies	25-50	51-72	In practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy .	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	1<2	evaluation	evaluation
D14-1182	51-55	56-59	We systematically study different strategies	for averaging MCMC samples	51-72	51-72	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	1<2	elab-addition	elab-addition
D14-1182	60-62	63-72	and show empirically	that averaging properly leads to significant improvements in prediction .	51-72	51-72	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	1>2	attribution	attribution
D14-1182	51-55	63-72	We systematically study different strategies	that averaging properly leads to significant improvements in prediction .	51-72	51-72	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction .	1<2	progression	progression
D14-1183	1-11	37-42,47-51	Phrase reordering is a challenge for statistical machine translation systems .	However , Training this discriminative model <*> might be computationally expensive .	1-11	37-51	Phrase reordering is a challenge for statistical machine translation systems .	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	1>2	contrast	contrast
D14-1183	1-11	12-18,27-36	Phrase reordering is a challenge for statistical machine translation systems .	Posing phrase movements as a prediction problem <*> is superior to the commonly used lexicalized reordering model .	1-11	12-36	Phrase reordering is a challenge for statistical machine translation systems .	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	1<2	elab-addition	elab-addition
D14-1183	12-18,27-36	19-21	Posing phrase movements as a prediction problem <*> is superior to the commonly used lexicalized reordering model .	using contextual features	12-36	12-36	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	1<2	manner-means	manner-means
D14-1183	19-21	22-26	using contextual features	modeled by maximum entropy-based classifier	12-36	12-36	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model .	1<2	elab-addition	elab-addition
D14-1183	37-42,47-51	52-65	However , Training this discriminative model <*> might be computationally expensive .	In this paper , we explore recent advancements in solving large-scale classification problems .	37-51	52-65	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	In this paper , we explore recent advancements in solving large-scale classification problems .	1>2	bg-compare	bg-compare
D14-1183	37-42,47-51	43-46	However , Training this discriminative model <*> might be computationally expensive .	using large-scale parallel corpus	37-51	37-51	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	However , Training this discriminative model using large-scale parallel corpus might be computationally expensive .	1<2	manner-means	manner-means
D14-1183	66-74	75-81	Using the dual problem to multinomial logistic regression ,	we managed to shrink the training data	66-96	66-96	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	1>2	manner-means	manner-means
D14-1183	52-65	75-81	In this paper , we explore recent advancements in solving large-scale classification problems .	we managed to shrink the training data	52-65	66-96	In this paper , we explore recent advancements in solving large-scale classification problems .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	1<2	elab-addition	elab-addition
D14-1183	75-81	82-83	we managed to shrink the training data	while iterating	66-96	66-96	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	1<2	temporal	temporal
D14-1183	75-81	84-91	we managed to shrink the training data	and produce significant saving in computation and memory	66-96	66-96	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	1<2	joint	joint
D14-1183	84-91	92-96	and produce significant saving in computation and memory	while preserving the accuracy .	66-96	66-96	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	Using the dual problem to multinomial logistic regression , we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy .	1<2	temporal	temporal
D14-1184	1-13	14-18	In this paper , we present two improvements to the beam search approach	for solving homophonic substitution ciphers	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	elab-addition	elab-addition
D14-1184	1-13	19-27	In this paper , we present two improvements to the beam search approach	presented in Nuhn et al. ( 2013 ) :	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	elab-addition	elab-addition
D14-1184	1-13	28-37	In this paper , we present two improvements to the beam search approach	An improved rest cost estimation together with an optimized strategy	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	elab-enumember	elab-enumember
D14-1184	28-37	38-41	An improved rest cost estimation together with an optimized strategy	for obtaining the order	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	elab-addition	elab-addition
D14-1184	28-37	42-54	An improved rest cost estimation together with an optimized strategy	in which the symbols of the cipher are deciphered reduces the beam size	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	elab-addition	elab-addition
D14-1184	42-54	55-71	in which the symbols of the cipher are deciphered reduces the beam size	needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred :	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	elab-addition	elab-addition
D14-1184	55-71	72-92	needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred :	The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1-92	1-92	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	1<2	joint	joint
D14-1184	1-13	93-125	In this paper , we present two improvements to the beam search approach	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) :	1-92	93-163	In this paper , we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. ( 2013 ) : An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred : The search effort is reduced from several hours of computation time to just a few seconds on a single CPU .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	1<2	evaluation	evaluation
D14-1184	126-130	131-139	Having 182 different cipher symbols	while having a length of just 762 symbols ,	93-163	93-163	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	1>2	contrast	contrast
D14-1184	131-139	140-154	while having a length of just 762 symbols ,	the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher	93-163	93-163	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	1>2	elab-addition	elab-addition
D14-1184	93-125	140-154	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) :	the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher	93-163	93-163	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	1<2	elab-addition	elab-addition
D14-1184	140-154	155-163	the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher	( length 408 , 54 different symbols ) .	93-163	93-163	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	1<2	elab-addition	elab-addition
D14-1184	140-154	164-179	the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher	To the best of our knowledge , this cipher has not been deciphered automatically before .	93-163	164-179	These improvements allow us to successfully decipher the second part of the famous Beale cipher ( see ( Ward et al. , 1885 ) and e.g. ( King , 1993 ) ) : Having 182 different cipher symbols while having a length of just 762 symbols , the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac-408 cipher ( length 408 , 54 different symbols ) .	To the best of our knowledge , this cipher has not been deciphered automatically before .	1<2	elab-addition	elab-addition
D14-1185	1-15	35-54	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages	1-15	35-76	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	1>2	contrast	contrast
D14-1185	1-15	16-34	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	1-15	16-34	Manual analysis and decryption of enciphered documents is a tedious and error prone work .	Often even after spending large amounts of time on a particular cipher - no decipherment can be found .	1<2	elab-addition	elab-addition
D14-1185	35-54	77-84	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages	In this work , we train a classifier	35-76	77-101	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	1>2	bg-goal	bg-goal
D14-1185	35-54	55-60	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages	found in libraries and archives ,	35-76	35-76	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	1<2	elab-addition	elab-addition
D14-1185	35-54	61-76	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages	and to focus human effort only on a small but potentially interesting subset of them .	35-76	35-76	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them .	1<2	joint	joint
D14-1185	77-84	85-101	In this work , we train a classifier	that is able to predict which encipherment method has been used to generate a given ciphertext .	77-101	77-101	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	1<2	elab-addition	elab-addition
D14-1185	77-84	102-110	In this work , we train a classifier	We are able to distinguish 50 different cipher types	77-101	102-125	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	1<2	elab-addition	elab-addition
D14-1185	102-110	111-125	We are able to distinguish 50 different cipher types	( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	102-125	102-125	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	We are able to distinguish 50 different cipher types ( specified by the American Cryptogram Association ) with an accuracy of 58.5 % .	1<2	elab-addition	elab-addition
D14-1185	77-84	126-139	In this work , we train a classifier	This is a 11.2 % absolute improvement over the best previously published classifier .	77-101	126-139	In this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext .	This is a 11.2 % absolute improvement over the best previously published classifier .	1<2	evaluation	evaluation
D14-1186	1-7	40-54	Previous work often used a pipelined framework	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 )	1-20	40-70	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	1>2	bg-compare	bg-compare
D14-1186	1-7	8-20	Previous work often used a pipelined framework	where Chinese word segmentation is followed by term extraction and keyword extraction .	1-20	1-20	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	1<2	elab-addition	elab-addition
D14-1186	1-7	21-26	Previous work often used a pipelined framework	Such framework suffers from error propagation	1-20	21-39	Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction .	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	1<2	elab-addition	elab-addition
D14-1186	21-26	27-39	Such framework suffers from error propagation	and is unable to leverage information in later modules for prior components .	21-39	21-39	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	Such framework suffers from error propagation and is unable to leverage information in later modules for prior components .	1<2	joint	joint
D14-1186	40-54	55-70	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 )	to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	40-70	40-70	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	1<2	enablement	enablement
D14-1186	71-76	77-82	Based on the DP-4 model ,	a sentence-wise Gibbs sampler is adopted	71-88	71-88	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	1>2	bg-general	bg-general
D14-1186	40-54	77-82	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 )	a sentence-wise Gibbs sampler is adopted	40-70	71-88	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	1<2	elab-addition	elab-addition
D14-1186	77-82	83-88	a sentence-wise Gibbs sampler is adopted	to obtain proper segmentation results .	71-88	71-88	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	1<2	enablement	enablement
D14-1186	77-82	89-100	a sentence-wise Gibbs sampler is adopted	Meanwhile , terms and keywords are acquired in the sampling process .	71-88	89-100	Based on the DP-4 model , a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results .	Meanwhile , terms and keywords are acquired in the sampling process .	1<2	joint	joint
D14-1186	40-54	101-110	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 )	Experimental results have shown the effectiveness of our method .	40-70	101-110	In this paper , we propose a four-level Dirichlet Process based model ( DP-4 ) to jointly learn the word distributions from the corpus , domain and document levels simultaneously .	Experimental results have shown the effectiveness of our method .	1<2	evaluation	evaluation
D14-1187	1-12	13-21	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages ,	one can turn to cross-lingual transfer and crawled dictionaries	1-27	1-27	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	1>2	condition	condition
D14-1187	13-21	28-37	one can turn to cross-lingual transfer and crawled dictionaries	We cast this problem in the framework of ambiguous learning	1-27	28-47	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	1>2	bg-goal	bg-goal
D14-1187	13-21	22-27	one can turn to cross-lingual transfer and crawled dictionaries	to collect partially supervised data .	1-27	1-27	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	When Part-of-Speech annotated data is scarce , e.g. for under-resourced languages , one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data .	1<2	enablement	enablement
D14-1187	38-39	40-47	and show	how to learn an accurate history-based model .	28-47	28-47	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	1>2	attribution	attribution
D14-1187	28-37	40-47	We cast this problem in the framework of ambiguous learning	how to learn an accurate history-based model .	28-47	28-47	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	1<2	progression	progression
D14-1187	28-37	48-62	We cast this problem in the framework of ambiguous learning	Experiments on ten languages show significant improvements over prior state of the art performance .	28-47	48-62	We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model .	Experiments on ten languages show significant improvements over prior state of the art performance .	1<2	evaluation	evaluation
D14-1188	1-16	17-41	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	1-16	17-41	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	The methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles .	1<2	elab-addition	elab-addition
D14-1188	1-16	42-46	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	We experiment with translation rules	1-16	42-75	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	1<2	evaluation	evaluation
D14-1188	42-46	47-63	We experiment with translation rules	which contain the core arguments for the predicates in the source side of a MT system ,	42-75	42-75	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	1<2	elab-addition	elab-addition
D14-1188	64-65	66-75	and observe	that using these rules significantly improves the translation quality .	42-75	42-75	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	1>2	attribution	attribution
D14-1188	42-46	66-75	We experiment with translation rules	that using these rules significantly improves the translation quality .	42-75	42-75	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system , and observe that using these rules significantly improves the translation quality .	1<2	progression	progression
D14-1188	1-16	76-82	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	We also present a new semantic feature	1-16	76-88	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	We also present a new semantic feature that resembles a language model .	1<2	evaluation	evaluation
D14-1188	76-82	83-88	We also present a new semantic feature	that resembles a language model .	76-88	76-88	We also present a new semantic feature that resembles a language model .	We also present a new semantic feature that resembles a language model .	1<2	elab-addition	elab-addition
D14-1188	89-91	92-103	Our results show	that the language model feature can also significantly improve MT results .	89-103	89-103	Our results show that the language model feature can also significantly improve MT results .	Our results show that the language model feature can also significantly improve MT results .	1>2	attribution	attribution
D14-1188	1-16	92-103	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	that the language model feature can also significantly improve MT results .	1-16	89-103	We introduce new features for incorporating semantic predicate-argument structures in machine translation ( MT ) .	Our results show that the language model feature can also significantly improve MT results .	1<2	evaluation	evaluation
D14-1189	1-6	7-13	We propose a simple unsupervised approach	to detecting non-compositional components in multiword expressions	1-17	1-17	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	1<2	enablement	enablement
D14-1189	1-6	14-17	We propose a simple unsupervised approach	based on Wiktionary .	1-17	1-17	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	1<2	bg-general	bg-general
D14-1189	1-6	18-31	We propose a simple unsupervised approach	The approach makes use of the definitions , synonyms and translations in Wiktionary ,	1-17	18-51	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	1<2	elab-addition	elab-addition
D14-1189	18-31	32-43	The approach makes use of the definitions , synonyms and translations in Wiktionary ,	and is applicable to any type of MWE in any language ,	18-51	18-51	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	1<2	joint	joint
D14-1189	44	45-51	assuming	the MWE is contained in Wiktionary .	18-51	18-51	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	1>2	attribution	attribution
D14-1189	32-43	45-51	and is applicable to any type of MWE in any language ,	the MWE is contained in Wiktionary .	18-51	18-51	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary .	1<2	condition	condition
D14-1189	52-54	55-65	Our experiments show	that the proposed approach achieves higher F-score than state-of-the-art methods .	52-65	52-65	Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods .	Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods .	1>2	attribution	attribution
D14-1189	1-6	55-65	We propose a simple unsupervised approach	that the proposed approach achieves higher F-score than state-of-the-art methods .	1-17	52-65	We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary .	Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods .	1<2	evaluation	evaluation
D14-1190	1-4	5-14	We propose a model	for jointly predicting multiple emotions in natural language sentences .	1-14	1-14	We propose a model for jointly predicting multiple emotions in natural language sentences .	We propose a model for jointly predicting multiple emotions in natural language sentences .	1<2	elab-addition	elab-addition
D14-1190	1-4	15-24	We propose a model	Our model is based on a low-rank coregionalisation approach ,	1-14	15-36	We propose a model for jointly predicting multiple emotions in natural language sentences .	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	1<2	elab-addition	elab-addition
D14-1190	15-24	25-36	Our model is based on a low-rank coregionalisation approach ,	which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	15-36	15-36	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	1<2	elab-addition	elab-addition
D14-1190	37-38	39-56	We show	that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	37-56	37-56	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	1>2	attribution	attribution
D14-1190	15-24	39-56	Our model is based on a low-rank coregionalisation approach ,	that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	15-36	37-56	Our model is based on a low-rank coregionalisation approach , which combines a vector-valued Gaussian Process with a rich parameterisation scheme .	We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset .	1<2	evaluation	evaluation
D14-1190	1-4	57-68	We propose a model	The proposed model outperforms both single-task baselines and other multi-task approaches .	1-14	57-68	We propose a model for jointly predicting multiple emotions in natural language sentences .	The proposed model outperforms both single-task baselines and other multi-task approaches .	1<2	evaluation	evaluation
D14-1191	1-11	20-22	Previous work on extracting ideology from text has focused on domains	but it's unclear	1-37	1-37	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	1>2	contrast	contrast
D14-1191	1-11	12-19	Previous work on extracting ideology from text has focused on domains	where expression of political views is expected ,	1-37	1-37	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	1<2	elab-addition	elab-addition
D14-1191	20-22	38-50	but it's unclear	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments	1-37	38-65	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	1>2	bg-compare	bg-compare
D14-1191	20-22	23-29	but it's unclear	if current technology can work in domains	1-37	1-37	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	1<2	elab-addition	elab-addition
D14-1191	23-29	30-37	if current technology can work in domains	where displays of ideology are considered inappropriate .	1-37	1-37	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	Previous work on extracting ideology from text has focused on domains where expression of political views is expected , but it's unclear if current technology can work in domains where displays of ideology are considered inappropriate .	1<2	elab-addition	elab-addition
D14-1191	38-50	51-58	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments	and apply it to one such domain :	38-65	38-65	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	1<2	progression	progression
D14-1191	51-58	59-60	and apply it to one such domain :	research papers	38-65	38-65	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	1<2	elab-example	elab-example
D14-1191	59-60	61-65	research papers	written by academic economists .	38-65	38-65	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	1<2	elab-addition	elab-addition
D14-1191	66-67	68-75	We show	economists' political leanings can be correctly predicted ,	66-93	66-93	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	1>2	attribution	attribution
D14-1191	38-50	68-75	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments	economists' political leanings can be correctly predicted ,	38-65	66-93	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	1<2	evaluation	evaluation
D14-1191	68-75	76-83	economists' political leanings can be correctly predicted ,	that our predictions generalize to new domains ,	66-93	66-93	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	1<2	joint	joint
D14-1191	76-83	84-93	that our predictions generalize to new domains ,	and that they correlate with public policy-relevant research findings .	66-93	66-93	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	We show economists' political leanings can be correctly predicted , that our predictions generalize to new domains , and that they correlate with public policy-relevant research findings .	1<2	joint	joint
D14-1191	38-50	94-97	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments	We also present evidence	38-65	94-110	We present a supervised ensemble n-gram model for ideology extraction with topic adjustments and apply it to one such domain : research papers written by academic economists .	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	1<2	evaluation	evaluation
D14-1191	94-97	98-104	We also present evidence	that unsupervised models can under-perform in domains	94-110	94-110	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	1<2	elab-addition	elab-addition
D14-1191	98-104	105-110	that unsupervised models can under-perform in domains	where ideological expression is discouraged .	94-110	94-110	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	We also present evidence that unsupervised models can under-perform in domains where ideological expression is discouraged .	1<2	elab-addition	elab-addition
D14-1192	1-9	10-15	We develop a statistical model of saccadic eye movements	during reading of isolated sentences .	1-15	1-15	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	1<2	temporal	temporal
D14-1192	1-9	16-25	We develop a statistical model of saccadic eye movements	The model is focused on representing individual differences between readers	1-15	16-43	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	1<2	elab-addition	elab-addition
D14-1192	16-25	26-43	The model is focused on representing individual differences between readers	and supports the inference of the most likely reader for a novel set of eye movement patterns .	16-43	16-43	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	The model is focused on representing individual differences between readers and supports the inference of the most likely reader for a novel set of eye movement patterns .	1<2	joint	joint
D14-1192	1-9	44-52	We develop a statistical model of saccadic eye movements	We empirically study the model for biometric reader identification	1-15	44-77	We develop a statistical model of saccadic eye movements during reading of isolated sentences .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	1<2	evaluation	evaluation
D14-1192	44-52	53-55	We empirically study the model for biometric reader identification	using eye-tracking data	44-77	44-77	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	1<2	manner-means	manner-means
D14-1192	53-55	56-59	using eye-tracking data	collected from 20 individuals	44-77	44-77	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	1<2	elab-addition	elab-addition
D14-1192	60-61	62-77	and observe	that the model distinguishes between 20 readers with an accuracy of up to 98 % .	44-77	44-77	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	1>2	attribution	attribution
D14-1192	44-52	62-77	We empirically study the model for biometric reader identification	that the model distinguishes between 20 readers with an accuracy of up to 98 % .	44-77	44-77	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	We empirically study the model for biometric reader identification using eye-tracking data collected from 20 individuals and observe that the model distinguishes between 20 readers with an accuracy of up to 98 % .	1<2	progression	progression
D14-1193	1-10	64-74	Multi-label text categorization ( MTC ) is supervised learning ,	The proposed method employs PCA to capture the hidden components ,	1-24	64-87	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	1>2	bg-compare	bg-compare
D14-1193	1-10	11-24	Multi-label text categorization ( MTC ) is supervised learning ,	where a document may be assigned with multiple categories ( labels ) simultaneously .	1-24	1-24	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	1<2	elab-addition	elab-addition
D14-1193	1-10	25-31	Multi-label text categorization ( MTC ) is supervised learning ,	The labels in the MTC are correlated	1-24	25-49	Multi-label text categorization ( MTC ) is supervised learning , where a document may be assigned with multiple categories ( labels ) simultaneously .	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	1<2	elab-addition	elab-addition
D14-1193	25-31	32-40	The labels in the MTC are correlated	and the correlation results in some hidden components ,	25-49	25-49	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	1<2	progression	progression
D14-1193	32-40	41-49	and the correlation results in some hidden components ,	which represent the "share" variance of correlated labels .	25-49	25-49	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	The labels in the MTC are correlated and the correlation results in some hidden components , which represent the "share" variance of correlated labels .	1<2	elab-addition	elab-addition
D14-1193	50-63	64-74	In this paper , we propose a method with hidden components for MTC .	The proposed method employs PCA to capture the hidden components ,	50-63	64-87	In this paper , we propose a method with hidden components for MTC .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	1<2	elab-addition	elab-addition
D14-1193	64-74	75-82	The proposed method employs PCA to capture the hidden components ,	and incorporates them into a joint learning framework	64-87	64-87	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	1<2	progression	progression
D14-1193	75-82	83-87	and incorporates them into a joint learning framework	to improve the performance .	64-87	64-87	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	The proposed method employs PCA to capture the hidden components , and incorporates them into a joint learning framework to improve the performance .	1<2	enablement	enablement
D14-1193	50-63	88-103	In this paper , we propose a method with hidden components for MTC .	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	50-63	88-103	In this paper , we propose a method with hidden components for MTC .	Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method .	1<2	evaluation	evaluation
P14-1101	1-14	15-19	Learning phonetic categories is one of the first steps to learning a language ,	yet is hard to do	1-25	1-25	Learning phonetic categories is one of the first steps to learning a language , yet is hard to do using only distributional phonetic information .	Learning phonetic categories is one of the first steps to learning a language , yet is hard to do using only distributional phonetic information .	1>2	contrast	contrast
P14-1101	15-19	59-89	yet is hard to do	that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	1-25	57-89	Learning phonetic categories is one of the first steps to learning a language , yet is hard to do using only distributional phonetic information .	We show that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	1>2	bg-goal	bg-goal
P14-1101	15-19	20-25	yet is hard to do	using only distributional phonetic information .	1-25	1-25	Learning phonetic categories is one of the first steps to learning a language , yet is hard to do using only distributional phonetic information .	Learning phonetic categories is one of the first steps to learning a language , yet is hard to do using only distributional phonetic information .	1<2	manner-means	manner-means
P14-1101	26-31	41-52	Semantics could potentially be useful ,	but it is unclear how many word meanings are known to infants	26-56	26-56	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	1>2	contrast	contrast
P14-1101	26-31	32-40	Semantics could potentially be useful ,	since words with different meanings have distinct phonetics ,	26-56	26-56	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	1<2	exp-reason	exp-reason
P14-1101	15-19	41-52	yet is hard to do	but it is unclear how many word meanings are known to infants	1-25	26-56	Learning phonetic categories is one of the first steps to learning a language , yet is hard to do using only distributional phonetic information .	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	1<2	elab-addition	elab-addition
P14-1101	41-52	53-56	but it is unclear how many word meanings are known to infants	learning phonetic categories .	26-56	26-56	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories .	1<2	elab-addition	elab-addition
P14-1101	57-58	59-89	We show	that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	57-89	57-89	We show that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	We show that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	1>2	attribution	attribution
P14-1101	59-89	90-114	that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	In our model , an extension of a previous model of joint word-form and phonetic category inference , the probability of word-forms is topic-dependent ,	57-89	90-134	We show that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning .	In our model , an extension of a previous model of joint word-form and phonetic category inference , the probability of word-forms is topic-dependent , enabling the model to find significantly better phonetic vowel categories and word-forms than a model with no semantic knowledge .	1<2	elab-addition	elab-addition
P14-1101	90-114	115-134	In our model , an extension of a previous model of joint word-form and phonetic category inference , the probability of word-forms is topic-dependent ,	enabling the model to find significantly better phonetic vowel categories and word-forms than a model with no semantic knowledge .	90-134	90-134	In our model , an extension of a previous model of joint word-form and phonetic category inference , the probability of word-forms is topic-dependent , enabling the model to find significantly better phonetic vowel categories and word-forms than a model with no semantic knowledge .	In our model , an extension of a previous model of joint word-form and phonetic category inference , the probability of word-forms is topic-dependent , enabling the model to find significantly better phonetic vowel categories and word-forms than a model with no semantic knowledge .	1<2	elab-addition	elab-addition
P14-1102	1-12	17-31	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ;	that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	1-31	1-31	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	1>2	contrast	contrast
P14-1102	13-16	17-31	however recent results suggest	that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	1-31	1-31	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	1>2	attribution	attribution
P14-1102	17-31	32-41	that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Therefore , this work models filler-gap acquisition as a byproduct	1-31	32-70	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	1<2	result	result
P14-1102	32-41	42-45	Therefore , this work models filler-gap acquisition as a byproduct	of learning word orderings	32-70	32-70	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	1<2	elab-addition	elab-addition
P14-1102	32-41	46-52	Therefore , this work models filler-gap acquisition as a byproduct	( e.g. SVO vs OSV ) ,	32-70	32-70	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	1<2	elab-example	elab-example
P14-1102	32-41	53-62	Therefore , this work models filler-gap acquisition as a byproduct	which must be done at a very young age anyway	32-70	32-70	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	1<2	elab-addition	elab-addition
P14-1102	53-62	63-70	which must be done at a very young age anyway	in order to extract meaning from language .	32-70	32-70	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g. SVO vs OSV ) , which must be done at a very young age anyway in order to extract meaning from language .	1<2	enablement	enablement
P14-1102	17-31	71-75,81-98	that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Specifically , this model , <*> represents the preferred locations of semantic roles relative to a verb as Gaussian mix-tures over real numbers .	1-31	71-98	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Specifically , this model , trained on part-of-speech tags , represents the preferred locations of semantic roles relative to a verb as Gaussian mix-tures over real numbers .	1<2	elab-addition	elab-addition
P14-1102	71-75,81-98	76-80	Specifically , this model , <*> represents the preferred locations of semantic roles relative to a verb as Gaussian mix-tures over real numbers .	trained on part-of-speech tags ,	71-98	71-98	Specifically , this model , trained on part-of-speech tags , represents the preferred locations of semantic roles relative to a verb as Gaussian mix-tures over real numbers .	Specifically , this model , trained on part-of-speech tags , represents the preferred locations of semantic roles relative to a verb as Gaussian mix-tures over real numbers .	1<2	elab-addition	elab-addition
P14-1102	17-31	99-109	that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	This approach learns role assignment in filler-gap constructions in a manner	1-31	99-122	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance .	1<2	elab-addition	elab-addition
P14-1102	99-109	110-114	This approach learns role assignment in filler-gap constructions in a manner	consistent with current developmental findings	99-122	99-122	This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance .	This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance .	1<2	elab-addition	elab-addition
P14-1102	99-109	115-122	This approach learns role assignment in filler-gap constructions in a manner	and is extremely robust to initialization variance .	99-122	99-122	This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance .	This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance .	1<2	joint	joint
P14-1102	17-31	123-137	that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Additionally , this model is shown to be able to account for a characteristic error	1-31	123-155	Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives .	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	1<2	evaluation	evaluation
P14-1102	123-137	138-140	Additionally , this model is shown to be able to account for a characteristic error	made by learners	123-155	123-155	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	1<2	elab-addition	elab-addition
P14-1102	123-137	141-143	Additionally , this model is shown to be able to account for a characteristic error	during this period	123-155	123-155	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	1<2	temporal	temporal
P14-1102	123-137	144-155	Additionally , this model is shown to be able to account for a characteristic error	( A and B gorped interpreted as A gorped B ) .	123-155	123-155	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	Additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B ) .	1<2	elab-example	elab-example
P14-1103	1-4	5-19	We present a method	to jointly learn features and weights directly from distributional data in a log-linear framework .	1-19	1-19	We present a method to jointly learn features and weights directly from distributional data in a log-linear framework .	We present a method to jointly learn features and weights directly from distributional data in a log-linear framework .	1<2	elab-addition	elab-addition
P14-1103	1-4	20-27	We present a method	Specifically , we propose a non-parametric Bayesian model	1-19	20-48	We present a method to jointly learn features and weights directly from distributional data in a log-linear framework .	Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	1<2	elab-addition	elab-addition
P14-1103	20-27	28-48	Specifically , we propose a non-parametric Bayesian model	for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	20-48	20-48	Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	1<2	elab-addition	elab-addition
P14-1103	20-27	49-55	Specifically , we propose a non-parametric Bayesian model	The model uses an Indian Buffet Process	20-48	49-81	Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	1<2	elab-addition	elab-addition
P14-1103	49-55	56-61	The model uses an Indian Buffet Process	prior to learn the feature values	49-81	49-81	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	1<2	elab-addition	elab-addition
P14-1103	56-61	62-67	prior to learn the feature values	used in the log-linear method ,	49-81	49-81	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	1<2	elab-addition	elab-addition
P14-1103	49-55	68-72	The model uses an Indian Buffet Process	and is the first algorithm	49-81	49-81	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	1<2	joint	joint
P14-1103	68-72	73-76	and is the first algorithm	for learning phonological constraints	49-81	49-81	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	1<2	elab-addition	elab-addition
P14-1103	73-76	77-81	for learning phonological constraints	without presupposing constraint structure .	49-81	49-81	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure .	1<2	condition	condition
P14-1103	20-27	82-88	Specifically , we propose a non-parametric Bayesian model	The model learns a system of constraints	20-48	82-113	Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints .	1<2	elab-addition	elab-addition
P14-1103	82-88	89-107	The model learns a system of constraints	that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure	82-113	82-113	The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints .	The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints .	1<2	elab-addition	elab-addition
P14-1103	89-107	108-113	that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure	corresponding to the standard constraints .	82-113	82-113	The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints .	The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints .	1<2	elab-addition	elab-addition
P14-1103	20-27	114-130	Specifically , we propose a non-parametric Bayesian model	These results suggest an alternative data-driven source for constraints instead of a fully innate constraint set .	20-48	114-130	Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting .	These results suggest an alternative data-driven source for constraints instead of a fully innate constraint set .	1<2	evaluation	evaluation
P14-1104	1-14	22-34	Many machine learning datasets are noisy with a substantial number of mislabeled instances .	In this paper we study a large , low quality annotated dataset ,	1-14	22-46	Many machine learning datasets are noisy with a substantial number of mislabeled instances .	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	1>2	bg-compare	bg-compare
P14-1104	1-14	15-21	Many machine learning datasets are noisy with a substantial number of mislabeled instances .	This noise yields sub-optimal classification performance .	1-14	15-21	Many machine learning datasets are noisy with a substantial number of mislabeled instances .	This noise yields sub-optimal classification performance .	1<2	elab-addition	elab-addition
P14-1104	22-34	35-38	In this paper we study a large , low quality annotated dataset ,	created quickly and cheaply	22-46	22-46	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	1<2	elab-addition	elab-addition
P14-1104	22-34	39-46	In this paper we study a large , low quality annotated dataset ,	using Amazon Mechanical Turk to crowdsource annotations .	22-46	22-46	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	1<2	manner-means	manner-means
P14-1104	22-34	47-60	In this paper we study a large , low quality annotated dataset ,	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm	22-46	47-80	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost .	1<2	elab-addition	elab-addition
P14-1104	47-60	61-71	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm	that can be used to iteratively and interactively correcting mislabeled instances	47-80	47-80	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost .	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost .	1<2	elab-addition	elab-addition
P14-1104	61-71	72-80	that can be used to iteratively and interactively correcting mislabeled instances	to significantly improve annotation quality at low cost .	47-80	47-80	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost .	We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost .	1<2	enablement	enablement
P14-1104	81-89	90-102	Eight different emotion extraction experiments on Twitter data demonstrate	that our approach is just as effective as more computationally expensive techniques .	81-102	81-102	Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques .	Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques .	1>2	attribution	attribution
P14-1104	22-34	90-102	In this paper we study a large , low quality annotated dataset ,	that our approach is just as effective as more computationally expensive techniques .	22-46	81-102	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques .	1<2	evaluation	evaluation
P14-1104	22-34	103-111	In this paper we study a large , low quality annotated dataset ,	Our techniques save a considerable amount of time .	22-46	103-111	In this paper we study a large , low quality annotated dataset , created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations .	Our techniques save a considerable amount of time .	1<2	evaluation	evaluation
P14-1105	1-9	46-58	An individual's words often reveal their political ideology .	we apply a recursive neural network ( RNN ) framework to the task	1-9	29-68	An individual's words often reveal their political ideology .	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	1>2	bg-goal	bg-goal
P14-1105	10-12,18-25	46-58	Existing automated techniques <*> focus on bags of words or wordlists ,	we apply a recursive neural network ( RNN ) framework to the task	10-28	29-68	Existing automated techniques to identify ideology from text focus on bags of words or wordlists , ignoring syntax .	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	1>2	bg-compare	bg-compare
P14-1105	10-12,18-25	13-17	Existing automated techniques <*> focus on bags of words or wordlists ,	to identify ideology from text	10-28	10-28	Existing automated techniques to identify ideology from text focus on bags of words or wordlists , ignoring syntax .	Existing automated techniques to identify ideology from text focus on bags of words or wordlists , ignoring syntax .	1<2	elab-addition	elab-addition
P14-1105	10-12,18-25	26-28	Existing automated techniques <*> focus on bags of words or wordlists ,	ignoring syntax .	10-28	10-28	Existing automated techniques to identify ideology from text focus on bags of words or wordlists , ignoring syntax .	Existing automated techniques to identify ideology from text focus on bags of words or wordlists , ignoring syntax .	1<2	elab-addition	elab-addition
P14-1105	29-36	46-58	Taking inspiration from recent work in sentiment analysis	we apply a recursive neural network ( RNN ) framework to the task	29-68	29-68	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	1>2	elab-addition	elab-addition
P14-1105	29-36	37-45	Taking inspiration from recent work in sentiment analysis	that successfully models the compositional aspect of language ,	29-68	29-68	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	1<2	elab-addition	elab-addition
P14-1105	46-58	59-63	we apply a recursive neural network ( RNN ) framework to the task	of identifying the political position	29-68	29-68	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	1<2	elab-addition	elab-addition
P14-1105	59-63	64-68	of identifying the political position	evinced by a sentence .	29-68	29-68	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	1<2	elab-addition	elab-addition
P14-1105	69-77	78-88	To show the importance of modeling subsentential elements ,	we crowdsource political annotations at a phrase and sentence level .	69-88	69-88	To show the importance of modeling subsentential elements , we crowdsource political annotations at a phrase and sentence level .	To show the importance of modeling subsentential elements , we crowdsource political annotations at a phrase and sentence level .	1>2	enablement	enablement
P14-1105	46-58	78-88	we apply a recursive neural network ( RNN ) framework to the task	we crowdsource political annotations at a phrase and sentence level .	29-68	69-88	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	To show the importance of modeling subsentential elements , we crowdsource political annotations at a phrase and sentence level .	1<2	elab-addition	elab-addition
P14-1105	46-58	89-103	we apply a recursive neural network ( RNN ) framework to the task	Our model outperforms existing models on our newly annotated dataset and an existing dataset .	29-68	89-103	Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language , we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence .	Our model outperforms existing models on our newly annotated dataset and an existing dataset .	1<2	evaluation	evaluation
P14-1106	1-9	10-22	This paper explores a simple and effective unified framework	for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system :	1-53	1-53	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	1<2	evaluation	evaluation
P14-1106	1-9	23-28	This paper explores a simple and effective unified framework	1 ) a syntactic reordering model	1-53	1-53	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	1<2	elab-enumember	elab-enumember
P14-1106	23-28	29-37	1 ) a syntactic reordering model	that explores reorderings for context free grammar rules ;	1-53	1-53	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	1<2	elab-addition	elab-addition
P14-1106	23-28	38-44	1 ) a syntactic reordering model	and 2 ) a semantic reordering model	1-53	1-53	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	1<2	joint	joint
P14-1106	38-44	45-53	and 2 ) a semantic reordering model	that focuses on the reordering of predicate-argument structures .	1-53	1-53	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	1<2	elab-addition	elab-addition
P14-1106	1-9	54-61	This paper explores a simple and effective unified framework	We develop novel features based on both models	1-53	54-73	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	We develop novel features based on both models and use them as soft constraints to guide the translation process .	1<2	elab-addition	elab-addition
P14-1106	54-61	62-67	We develop novel features based on both models	and use them as soft constraints	54-73	54-73	We develop novel features based on both models and use them as soft constraints to guide the translation process .	We develop novel features based on both models and use them as soft constraints to guide the translation process .	1<2	joint	joint
P14-1106	62-67	68-73	and use them as soft constraints	to guide the translation process .	54-73	54-73	We develop novel features based on both models and use them as soft constraints to guide the translation process .	We develop novel features based on both models and use them as soft constraints to guide the translation process .	1<2	elab-addition	elab-addition
P14-1106	74-78	79-92	Experiments on Chinese-English translation show	that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	74-92	74-92	Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	1>2	attribution	attribution
P14-1106	1-9	79-92	This paper explores a simple and effective unified framework	that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	1-53	74-92	This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures .	Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	1<2	evaluation	evaluation
P14-1106	79-92	93-96,103-113	that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	However , the gain <*> is limited in the presence of the syntactic reordering model ,	74-92	93-128	Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system .	However , the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model , and we therefore provide a detailed analysis of the behavior differences between the two .	1<2	contrast	contrast
P14-1106	93-96,103-113	97-102	However , the gain <*> is limited in the presence of the syntactic reordering model ,	achieved by the semantic reordering model	93-128	93-128	However , the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model , and we therefore provide a detailed analysis of the behavior differences between the two .	However , the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model , and we therefore provide a detailed analysis of the behavior differences between the two .	1<2	elab-addition	elab-addition
P14-1106	93-96,103-113	114-128	However , the gain <*> is limited in the presence of the syntactic reordering model ,	and we therefore provide a detailed analysis of the behavior differences between the two .	93-128	93-128	However , the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model , and we therefore provide a detailed analysis of the behavior differences between the two .	However , the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model , and we therefore provide a detailed analysis of the behavior differences between the two .	1<2	progression	progression
P14-1107	1-5	38-47	Crowdsourcing is a viable mechanism	naive collection of translations from non-professionals yields low-quality results .	1-13	30-47	Crowdsourcing is a viable mechanism for creating training data for machine translation .	However , when compared to professional translation , naive collection of translations from non-professionals yields low-quality results .	1>2	contrast	contrast
P14-1107	1-5	6-13	Crowdsourcing is a viable mechanism	for creating training data for machine translation .	1-13	1-13	Crowdsourcing is a viable mechanism for creating training data for machine translation .	Crowdsourcing is a viable mechanism for creating training data for machine translation .	1<2	elab-addition	elab-addition
P14-1107	1-5	14-22	Crowdsourcing is a viable mechanism	It provides a low cost , fast turnaround way	1-13	14-29	Crowdsourcing is a viable mechanism for creating training data for machine translation .	It provides a low cost , fast turnaround way of processing large volumes of data .	1<2	elab-addition	elab-addition
P14-1107	14-22	23-29	It provides a low cost , fast turnaround way	of processing large volumes of data .	14-29	14-29	It provides a low cost , fast turnaround way of processing large volumes of data .	It provides a low cost , fast turnaround way of processing large volumes of data .	1<2	elab-addition	elab-addition
P14-1107	30-37	38-47	However , when compared to professional translation ,	naive collection of translations from non-professionals yields low-quality results .	30-47	30-47	However , when compared to professional translation , naive collection of translations from non-professionals yields low-quality results .	However , when compared to professional translation , naive collection of translations from non-professionals yields low-quality results .	1>2	condition	condition
P14-1107	38-47	59-78	naive collection of translations from non-professionals yields low-quality results .	In this paper , we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals .	30-47	59-78	However , when compared to professional translation , naive collection of translations from non-professionals yields low-quality results .	In this paper , we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals .	1>2	bg-compare	bg-compare
P14-1107	1-5	48-58	Crowdsourcing is a viable mechanism	Careful quality control is necessary for crowdsourcing to work well .	1-13	48-58	Crowdsourcing is a viable mechanism for creating training data for machine translation .	Careful quality control is necessary for crowdsourcing to work well .	1<2	elab-addition	elab-addition
P14-1107	59-78	79-83	In this paper , we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals .	We develop graph-based ranking models	59-78	79-106	In this paper , we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals .	We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .	1<2	elab-addition	elab-addition
P14-1107	79-83	84-98	We develop graph-based ranking models	that automatically select the best output from multiple redundant versions of translations and edits ,	79-106	79-106	We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .	We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .	1<2	elab-addition	elab-addition
P14-1107	84-98	99-106	that automatically select the best output from multiple redundant versions of translations and edits ,	and improves translation quality closer to professionals .	79-106	79-106	We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .	We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits , and improves translation quality closer to professionals .	1<2	joint	joint
P14-1108	1-5	6-9	We introduce a novel approach	for building language models	1-28	1-28	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	1<2	elab-addition	elab-addition
P14-1108	1-5	10-20	We introduce a novel approach	based on a systematic , recursive exploration of skip n-gram models	1-28	1-28	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	1<2	bg-general	bg-general
P14-1108	10-20	21-23	based on a systematic , recursive exploration of skip n-gram models	which are interpolated	1-28	1-28	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	1<2	elab-addition	elab-addition
P14-1108	21-23	24-28	which are interpolated	using modified Kneser-Ney smoothing .	1-28	1-28	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	1<2	manner-means	manner-means
P14-1108	1-5	29-33	We introduce a novel approach	Our approach generalizes language models	1-28	29-48	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case .	1<2	elab-addition	elab-addition
P14-1108	29-33	34-48	Our approach generalizes language models	as it contains the classical interpolation with lower order models as a special case .	29-48	29-48	Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case .	Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case .	1<2	exp-reason	exp-reason
P14-1108	1-5	49-60	We introduce a novel approach	In this paper we motivate , formalize and present our approach .	1-28	49-60	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	In this paper we motivate , formalize and present our approach .	1<2	elab-addition	elab-addition
P14-1108	61-71	72-89	In an extensive empirical experiment over English text corpora we demonstrate	that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 %	61-100	61-100	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	1>2	attribution	attribution
P14-1108	1-5	72-89	We introduce a novel approach	that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 %	1-28	61-100	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	1<2	evaluation	evaluation
P14-1108	72-89	90-95	that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 %	in comparison to traditional language models	61-100	61-100	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	1<2	comparison	comparison
P14-1108	90-95	96-100	in comparison to traditional language models	using modified Kneser-Ney smoothing .	61-100	61-100	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing .	1<2	elab-addition	elab-addition
P14-1108	1-5	101-115	We introduce a novel approach	Furthermore , we investigate the behaviour over three other languages and a domain specific corpus	1-28	101-121	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	Furthermore , we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements .	1<2	evaluation	evaluation
P14-1108	101-115	116-121	Furthermore , we investigate the behaviour over three other languages and a domain specific corpus	where we observed consistent improvements .	101-121	101-121	Furthermore , we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements .	Furthermore , we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements .	1<2	elab-addition	elab-addition
P14-1108	122-126	127-136	Finally , we also show	that the strength of our approach lies in its ability	122-145	122-145	Finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data .	Finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data .	1>2	attribution	attribution
P14-1108	1-5	127-136	We introduce a novel approach	that the strength of our approach lies in its ability	1-28	122-145	We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing .	Finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data .	1<2	evaluation	evaluation
P14-1108	127-136	137-145	that the strength of our approach lies in its ability	to cope in particular with sparse training data .	122-145	122-145	Finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data .	Finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data .	1<2	elab-addition	elab-addition
P14-1108	146-157	158-168	Using a very small training data set of only 736 KB text	we yield improvements of even 25.7 % reduction of perplexity .	146-168	146-168	Using a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity .	Using a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity .	1>2	manner-means	manner-means
P14-1108	127-136	158-168	that the strength of our approach lies in its ability	we yield improvements of even 25.7 % reduction of perplexity .	122-145	146-168	Finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data .	Using a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity .	1<2	exp-evidence	exp-evidence
P14-1109	1-10	26-49	Earnings call summarizes the financial performance of a company ,	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	1-25	26-49	Earnings call summarizes the financial performance of a company , and it is an important indicator of the future financial risks of the company .	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	1>2	bg-goal	bg-goal
P14-1109	1-10	11-25	Earnings call summarizes the financial performance of a company ,	and it is an important indicator of the future financial risks of the company .	1-25	1-25	Earnings call summarizes the financial performance of a company , and it is an important indicator of the future financial risks of the company .	Earnings call summarizes the financial performance of a company , and it is an important indicator of the future financial risks of the company .	1<2	joint	joint
P14-1109	26-49	50-59	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	In particular , we perform a text regression task :	26-49	50-83	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	1<2	elab-addition	elab-addition
P14-1109	60-67	68-77	given the transcript of an earnings call ,	we predict the volatility of stock prices from the week	50-83	50-83	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	1>2	condition	condition
P14-1109	50-59	68-77	In particular , we perform a text regression task :	we predict the volatility of stock prices from the week	50-83	50-83	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	1<2	elab-addition	elab-addition
P14-1109	68-77	78-83	we predict the volatility of stock prices from the week	after the call is made .	50-83	50-83	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	In particular , we perform a text regression task : given the transcript of an earnings call , we predict the volatility of stock prices from the week after the call is made .	1<2	temporal	temporal
P14-1109	26-49	84-90	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	We propose the use of copula :	26-49	84-124	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	1<2	elab-addition	elab-addition
P14-1109	84-90	91-94	We propose the use of copula :	a powerful statistical framework	84-124	84-124	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	1<2	elab-addition	elab-addition
P14-1109	91-94	95-107	a powerful statistical framework	that separately models the uniform marginals and their complex multivariate stochastic dependencies ,	84-124	84-124	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	1<2	elab-addition	elab-addition
P14-1109	95-107	108-124	that separately models the uniform marginals and their complex multivariate stochastic dependencies ,	while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	84-124	84-124	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	We propose the use of copula : a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies , while not requiring any prior assumptions on the distributions of the covariate and the dependent variable .	1<2	condition	condition
P14-1109	125-130	131-142	By performing probability integral transform ,	our approach moves beyond the standard count-based bag-of-words models in NLP ,	125-164	125-164	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	1>2	manner-means	manner-means
P14-1109	26-49	131-142	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	our approach moves beyond the standard count-based bag-of-words models in NLP ,	26-49	125-164	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	1<2	evaluation	evaluation
P14-1109	131-142	143-149	our approach moves beyond the standard count-based bag-of-words models in NLP ,	and improves previous work on text regression	125-164	125-164	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	1<2	joint	joint
P14-1109	143-149	150-164	and improves previous work on text regression	by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	125-164	125-164	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP , and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula .	1<2	manner-means	manner-means
P14-1109	165-169	170-187	In experiments , we show	that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings .	165-187	165-187	In experiments , we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings .	In experiments , we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings .	1>2	attribution	attribution
P14-1109	26-49	170-187	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings .	26-49	165-187	We quantitatively study how earnings calls are correlated with the financial risks , with a special focus on the financial crisis of 2009 .	In experiments , we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings .	1<2	evaluation	evaluation
P14-1110	1-6,11-15	16-24	Topic models , an unsupervised technique <*> improve machine translation quality .	However , previous work uses only the source language	1-15	16-36	Topic models , an unsupervised technique for inferring translation domains improve machine translation quality .	However , previous work uses only the source language and completely ignores the target language , which can disambiguate domains .	1>2	contrast	contrast
P14-1110	1-6,11-15	7-10	Topic models , an unsupervised technique <*> improve machine translation quality .	for inferring translation domains	1-15	1-15	Topic models , an unsupervised technique for inferring translation domains improve machine translation quality .	Topic models , an unsupervised technique for inferring translation domains improve machine translation quality .	1<2	elab-addition	elab-addition
P14-1110	16-24	37-43	However , previous work uses only the source language	We propose new polylingual tree-based topic models	16-36	37-61	However , previous work uses only the source language and completely ignores the target language , which can disambiguate domains .	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	1>2	bg-compare	bg-compare
P14-1110	16-24	25-31	However , previous work uses only the source language	and completely ignores the target language ,	16-36	16-36	However , previous work uses only the source language and completely ignores the target language , which can disambiguate domains .	However , previous work uses only the source language and completely ignores the target language , which can disambiguate domains .	1<2	joint	joint
P14-1110	25-31	32-36	and completely ignores the target language ,	which can disambiguate domains .	16-36	16-36	However , previous work uses only the source language and completely ignores the target language , which can disambiguate domains .	However , previous work uses only the source language and completely ignores the target language , which can disambiguate domains .	1<2	elab-addition	elab-addition
P14-1110	37-43	44-47	We propose new polylingual tree-based topic models	to extract domain knowledge	37-61	37-61	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	1<2	enablement	enablement
P14-1110	44-47	48-54	to extract domain knowledge	that considers both source and target languages	37-61	37-61	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	1<2	elab-enumember	elab-enumember
P14-1110	44-47	55-61	to extract domain knowledge	and derive three different inference schemes .	37-61	37-61	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	1<2	joint	joint
P14-1110	37-43	62-72	We propose new polylingual tree-based topic models	We evaluate our model on a Chinese to English translation task	37-61	62-83	We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes .	We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines .	1<2	evaluation	evaluation
P14-1110	62-72	73-83	We evaluate our model on a Chinese to English translation task	and obtain up to 1.2 BLEU improvement over strong baselines .	62-83	62-83	We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines .	We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines .	1<2	progression	progression
P14-1111	1-4	28-30	We explore the extent	how performance changes	1-25	26-45	We explore the extent to which high-resource manual annotations such as treebanks are necessary for the task of semantic role labeling ( SRL ) .	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	1>2	bg-goal	bg-goal
P14-1111	1-4	5-25	We explore the extent	to which high-resource manual annotations such as treebanks are necessary for the task of semantic role labeling ( SRL ) .	1-25	1-25	We explore the extent to which high-resource manual annotations such as treebanks are necessary for the task of semantic role labeling ( SRL ) .	We explore the extent to which high-resource manual annotations such as treebanks are necessary for the task of semantic role labeling ( SRL ) .	1<2	elab-addition	elab-addition
P14-1111	26-27	28-30	We examine	how performance changes	26-45	26-45	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	1>2	attribution	attribution
P14-1111	28-30	31-34	how performance changes	without syntactic supervision ,	26-45	26-45	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	1<2	condition	condition
P14-1111	26-27	35-40	We examine	comparing both joint and pipelined methods	26-45	26-45	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	1<2	elab-addition	elab-addition
P14-1111	35-40	41-45	comparing both joint and pipelined methods	to induce latent syntax .	26-45	26-45	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	1<2	elab-addition	elab-addition
P14-1111	28-30	46-55	how performance changes	This work highlights a new application of unsupervised grammar induction	26-45	46-68	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax .	1<2	elab-addition	elab-addition
P14-1111	46-55	56-68	This work highlights a new application of unsupervised grammar induction	and demonstrates several approaches to SRL in the absence of supervised syntax .	46-68	46-68	This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax .	This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax .	1<2	joint	joint
P14-1111	28-30	69-87	how performance changes	Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting ,	26-45	69-95	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting , reaching 72.48 % F1 averaged across languages .	1<2	evaluation	evaluation
P14-1111	69-87	88-95	Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting ,	reaching 72.48 % F1 averaged across languages .	69-95	69-95	Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting , reaching 72.48 % F1 averaged across languages .	Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting , reaching 72.48 % F1 averaged across languages .	1<2	exp-evidence	exp-evidence
P14-1111	28-30	96-107	how performance changes	We release our code for this work along with a larger toolkit	26-45	96-113	We examine how performance changes without syntactic supervision , comparing both joint and pipelined methods to induce latent syntax .	We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure .	1<2	elab-addition	elab-addition
P14-1111	96-107	108-113	We release our code for this work along with a larger toolkit	for specifying arbitrary graphical structure .	96-113	96-113	We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure .	We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure .	1<2	elab-addition	elab-addition
P14-1112	1-4	5-12	We present an approach	to training a joint syntactic and semantic parser	1-31	1-31	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	1<2	elab-addition	elab-addition
P14-1112	5-12	13-27	to training a joint syntactic and semantic parser	that combines syntactic training information from CCGbank with semantic training information from a knowledge base	1-31	1-31	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	1<2	elab-addition	elab-addition
P14-1112	13-27	28-31	that combines syntactic training information from CCGbank with semantic training information from a knowledge base	via distant supervision .	1-31	1-31	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	1<2	manner-means	manner-means
P14-1112	1-4	32-43	We present an approach	The trained parser produces a full syntactic parse of any sentence ,	1-31	32-64	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	The trained parser produces a full syntactic parse of any sentence , while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary .	1<2	elab-addition	elab-addition
P14-1112	32-43	44-53	The trained parser produces a full syntactic parse of any sentence ,	while simultaneously producing logical forms for portions of the sentence	32-64	32-64	The trained parser produces a full syntactic parse of any sentence , while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary .	The trained parser produces a full syntactic parse of any sentence , while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary .	1<2	joint	joint
P14-1112	44-53	54-64	while simultaneously producing logical forms for portions of the sentence	that have a semantic representation within the parser's predicate vocabulary .	32-64	32-64	The trained parser produces a full syntactic parse of any sentence , while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary .	The trained parser produces a full syntactic parse of any sentence , while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary .	1<2	elab-addition	elab-addition
P14-1112	1-4	65-68	We present an approach	We demonstrate our approach	1-31	65-83	We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision .	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	1<2	evaluation	evaluation
P14-1112	65-68	69-72	We demonstrate our approach	by training a parser	65-83	65-83	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	1<2	manner-means	manner-means
P14-1112	69-72	73-83	by training a parser	whose semantic representation contains 130 predicates from the NELL ontology .	65-83	65-83	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	1<2	elab-addition	elab-addition
P14-1112	84-87	88-105	A semantic evaluation demonstrates	that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach .	84-105	84-105	A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach .	A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach .	1>2	attribution	attribution
P14-1112	65-68	88-105	We demonstrate our approach	that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach .	65-83	84-105	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach .	1<2	elab-aspect	elab-aspect
P14-1112	106-111	112-123	A syntactic evaluation on CCGbank demonstrates	that the parser's dependency F-score is within 2.5 % of state-of-the-art .	106-123	106-123	A syntactic evaluation on CCGbank demonstrates that the parser's dependency F-score is within 2.5 % of state-of-the-art .	A syntactic evaluation on CCGbank demonstrates that the parser's dependency F-score is within 2.5 % of state-of-the-art .	1>2	attribution	attribution
P14-1112	65-68	112-123	We demonstrate our approach	that the parser's dependency F-score is within 2.5 % of state-of-the-art .	65-83	106-123	We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology .	A syntactic evaluation on CCGbank demonstrates that the parser's dependency F-score is within 2.5 % of state-of-the-art .	1<2	elab-aspect	elab-aspect
P14-1113	1-9	30-43	Semantic hierarchy construction aims to build structures of concepts	This paper proposes a novel and effective method for the construction of semantic hierarchies	1-15	30-60	Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym ("is-a") relations .	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	1>2	bg-goal	bg-goal
P14-1113	1-9	10-15	Semantic hierarchy construction aims to build structures of concepts	linked by hypernym-hyponym ("is-a") relations .	1-15	1-15	Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym ("is-a") relations .	Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym ("is-a") relations .	1<2	elab-addition	elab-addition
P14-1113	1-9	16-29	Semantic hierarchy construction aims to build structures of concepts	A major challenge for this task is the automatic discovery of such relations .	1-15	16-29	Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym ("is-a") relations .	A major challenge for this task is the automatic discovery of such relations .	1<2	elab-addition	elab-addition
P14-1113	30-43	44-48	This paper proposes a novel and effective method for the construction of semantic hierarchies	based on word embeddings ,	30-60	30-60	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	1<2	bg-general	bg-general
P14-1113	44-48	49-60	based on word embeddings ,	which can be used to measure the semantic relationship between words .	30-60	30-60	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	1<2	elab-addition	elab-addition
P14-1113	61-62	63-70	We identify	whether a candidate word pair has hypernym-hyponym relation	61-82	61-82	We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms .	We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms .	1>2	attribution	attribution
P14-1113	30-43	63-70	This paper proposes a novel and effective method for the construction of semantic hierarchies	whether a candidate word pair has hypernym-hyponym relation	30-60	61-82	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms .	1<2	elab-addition	elab-addition
P14-1113	61-62	71-82	We identify	by using the word-embedding-based semantic projections between words and their hypernyms .	61-82	61-82	We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms .	We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms .	1<2	manner-means	manner-means
P14-1113	30-43	83-102	This paper proposes a novel and effective method for the construction of semantic hierarchies	Our result , an F-score of 73.74 % , outperforms the state-of-the-art methods on a manually labeled test dataset .	30-60	83-102	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	Our result , an F-score of 73.74 % , outperforms the state-of-the-art methods on a manually labeled test dataset .	1<2	evaluation	evaluation
P14-1113	30-43	103-122	This paper proposes a novel and effective method for the construction of semantic hierarchies	Moreover , combining our method with a previous manually-built hierarchy extension method can further improve F-score to 80.29 % .	30-60	103-122	This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words .	Moreover , combining our method with a previous manually-built hierarchy extension method can further improve F-score to 80.29 % .	1<2	evaluation	evaluation
P14-1114	1-15	16-28	Probabilistic Soft Logic ( PSL ) is a recently developed framework for probabilistic logic .	We use PSL to combine logical and distributional representations of natural-language meaning ,	1-15	16-41	Probabilistic Soft Logic ( PSL ) is a recently developed framework for probabilistic logic .	We use PSL to combine logical and distributional representations of natural-language meaning , where distributional information is represented in the form of weighted inference rules .	1<2	elab-addition	elab-addition
P14-1114	16-28	29-41	We use PSL to combine logical and distributional representations of natural-language meaning ,	where distributional information is represented in the form of weighted inference rules .	16-41	16-41	We use PSL to combine logical and distributional representations of natural-language meaning , where distributional information is represented in the form of weighted inference rules .	We use PSL to combine logical and distributional representations of natural-language meaning , where distributional information is represented in the form of weighted inference rules .	1<2	elab-addition	elab-addition
P14-1114	1-15	42-55	Probabilistic Soft Logic ( PSL ) is a recently developed framework for probabilistic logic .	We apply this framework to the task of Semantic Textual Similarity ( STS )	1-15	42-92	Probabilistic Soft Logic ( PSL ) is a recently developed framework for probabilistic logic .	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	1<2	evaluation	evaluation
P14-1114	42-55	56-66	We apply this framework to the task of Semantic Textual Similarity ( STS )	( i.e. judging the semantic similarity of natural-language sentences ) ,	42-92	42-92	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	1<2	elab-definition	elab-definition
P14-1114	67-68	69-73	and show	that PSL gives improved results	42-92	42-92	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	1>2	attribution	attribution
P14-1114	42-55	69-73	We apply this framework to the task of Semantic Textual Similarity ( STS )	that PSL gives improved results	42-92	42-92	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	1<2	progression	progression
P14-1114	69-73	74-78,87-92	that PSL gives improved results	compared to a previous approach <*> and a purely distributional approach .	42-92	42-92	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	1<2	comparison	comparison
P14-1114	74-78,87-92	79-86	compared to a previous approach <*> and a purely distributional approach .	based on Markov Logic Networks ( MLNs )	42-92	42-92	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	We apply this framework to the task of Semantic Textual Similarity ( STS ) ( i.e. judging the semantic similarity of natural-language sentences ) , and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks ( MLNs ) and a purely distributional approach .	1<2	bg-general	bg-general
P14-1115	1-11	12-17	We propose a novel abstractive querybased summarization system for conversations ,	where queries are defined as phrases	1-23	1-23	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	1<2	elab-addition	elab-addition
P14-1115	12-17	18-23	where queries are defined as phrases	reflecting a user information needs .	1-23	1-23	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	1<2	elab-addition	elab-addition
P14-1115	1-11	24-32	We propose a novel abstractive querybased summarization system for conversations ,	We rank and extract the utterances in a conversation	1-23	24-43	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	We rank and extract the utterances in a conversation based on the overall content and the phrasal query information .	1<2	elab-process_step	elab-process_step
P14-1115	24-32	33-43	We rank and extract the utterances in a conversation	based on the overall content and the phrasal query information .	24-43	24-43	We rank and extract the utterances in a conversation based on the overall content and the phrasal query information .	We rank and extract the utterances in a conversation based on the overall content and the phrasal query information .	1<2	bg-general	bg-general
P14-1115	1-11	44-48	We propose a novel abstractive querybased summarization system for conversations ,	We cluster the selected sentences	1-23	44-68	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	1<2	elab-process_step	elab-process_step
P14-1115	44-48	49-53	We cluster the selected sentences	based on their lexical similarity	44-68	44-68	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	1<2	bg-general	bg-general
P14-1115	44-48	54-60	We cluster the selected sentences	and aggregate the sentences in each cluster	44-68	44-68	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	1<2	progression	progression
P14-1115	54-60	61-68	and aggregate the sentences in each cluster	by means of a word graph model .	44-68	44-68	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model .	1<2	manner-means	manner-means
P14-1115	1-11	69-73	We propose a novel abstractive querybased summarization system for conversations ,	We propose a ranking strategy	1-23	69-91	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster .	1<2	elab-process_step	elab-process_step
P14-1115	69-73	74-91	We propose a ranking strategy	to select the best path in the constructed graph as a query-based abstract sentence for each cluster .	69-91	69-91	We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster .	We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster .	1<2	enablement	enablement
P14-1115	1-11	92-98	We propose a novel abstractive querybased summarization system for conversations ,	A resulting summary consists of abstractive sentences	1-23	92-111	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation .	1<2	elab-addition	elab-addition
P14-1115	92-98	99-111	A resulting summary consists of abstractive sentences	representing the phrasal query information and the overall content of the conversation .	92-111	92-111	A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation .	A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation .	1<2	elab-addition	elab-addition
P14-1115	112-124	125-135	Automatic and manual evaluation results over meeting , chat and email conversations show	that our approach significantly outperforms baselines and previous extractive models .	112-135	112-135	Automatic and manual evaluation results over meeting , chat and email conversations show that our approach significantly outperforms baselines and previous extractive models .	Automatic and manual evaluation results over meeting , chat and email conversations show that our approach significantly outperforms baselines and previous extractive models .	1>2	attribution	attribution
P14-1115	1-11	125-135	We propose a novel abstractive querybased summarization system for conversations ,	that our approach significantly outperforms baselines and previous extractive models .	1-23	112-135	We propose a novel abstractive querybased summarization system for conversations , where queries are defined as phrases reflecting a user information needs .	Automatic and manual evaluation results over meeting , chat and email conversations show that our approach significantly outperforms baselines and previous extractive models .	1<2	evaluation	evaluation
P14-1116	1-21	22-36	We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation .	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem ,	1-21	22-57	We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation .	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	1<2	elab-addition	elab-addition
P14-1116	22-36	37-42	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem ,	which takes as input time-series data	22-57	22-57	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	1<2	elab-addition	elab-addition
P14-1116	22-36	43-49	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem ,	and outputs a set of templates ,	22-57	22-57	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	1<2	joint	joint
P14-1116	43-49	50-57	and outputs a set of templates ,	while capturing the dependencies between selected templates .	22-57	22-57	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates .	1<2	temporal	temporal
P14-1116	58-59	60-68	We show	that this method generates output closer to the feedback	58-95	58-95	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	1>2	attribution	attribution
P14-1116	1-21	60-68	We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation .	that this method generates output closer to the feedback	1-21	58-95	We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation .	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	1<2	evaluation	evaluation
P14-1116	60-68	69-73	that this method generates output closer to the feedback	that lecturers actually generated ,	58-95	58-95	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	1<2	elab-addition	elab-addition
P14-1116	60-68	74-87	that this method generates output closer to the feedback	achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers	58-95	58-95	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	1<2	exp-evidence	exp-evidence
P14-1116	74-87	88-95	achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers	that keep a history of selected templates .	58-95	58-95	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates .	1<2	elab-addition	elab-addition
P14-1116	1-21	96-112	We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation .	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation	1-21	96-120	We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation .	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation and using ratings from real student users .	1<2	evaluation	evaluation
P14-1116	96-112	113-120	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation	and using ratings from real student users .	96-120	96-120	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation and using ratings from real student users .	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation and using ratings from real student users .	1<2	joint	joint
P14-1116	121-122	123-130	We show	that the different methods have different benefits ,	121-157	121-157	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	1>2	attribution	attribution
P14-1116	96-112	123-130	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation	that the different methods have different benefits ,	96-120	121-157	Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation and using ratings from real student users .	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	1<2	elab-addition	elab-addition
P14-1116	123-130	131-135	that the different methods have different benefits ,	with ML being more accurate	121-157	121-157	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	1<2	elab-aspect	elab-aspect
P14-1116	131-135	136-145	with ML being more accurate	for predicting what was seen in the training data ,	121-157	121-157	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	1<2	elab-addition	elab-addition
P14-1116	123-130	146-150	that the different methods have different benefits ,	whereas RL is more exploratory	121-157	121-157	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	1<2	elab-aspect	elab-aspect
P14-1116	146-150	151-157	whereas RL is more exploratory	and slightly preferred by the students .	121-157	121-157	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students .	1<2	joint	joint
P14-1117	1-10	17-24	Sentence compression has been shown to benefit from joint inference	but this typically requires expensive integer programming .	1-24	1-24	Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming .	Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming .	1>2	contrast	contrast
P14-1117	1-10	11-16	Sentence compression has been shown to benefit from joint inference	involving both n-gram and dependency-factored objectives	1-24	1-24	Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming .	Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming .	1<2	elab-enumember	elab-enumember
P14-1117	17-24	25-32	but this typically requires expensive integer programming .	We explore instead the use of Lagrangian relaxation	1-24	25-42	Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming .	We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately .	1>2	bg-goal	bg-goal
P14-1117	25-32	33-37	We explore instead the use of Lagrangian relaxation	to decouple the two subproblems	25-42	25-42	We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately .	We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately .	1<2	elab-addition	elab-addition
P14-1117	33-37	38-42	to decouple the two subproblems	and solve them separately .	25-42	25-42	We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately .	We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately .	1<2	progression	progression
P14-1117	43-52	53-61	While dynamic programming is viable for bigram-based sentence compression ,	finding optimal compressed trees within graphs is NP-hard .	43-61	43-61	While dynamic programming is viable for bigram-based sentence compression , finding optimal compressed trees within graphs is NP-hard .	While dynamic programming is viable for bigram-based sentence compression , finding optimal compressed trees within graphs is NP-hard .	1>2	contrast	contrast
P14-1117	53-61	62-68	finding optimal compressed trees within graphs is NP-hard .	We recover approximate solutions to this problem	43-61	62-93	While dynamic programming is viable for bigram-based sentence compression , finding optimal compressed trees within graphs is NP-hard .	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	1>2	bg-goal	bg-goal
P14-1117	25-32	62-68	We explore instead the use of Lagrangian relaxation	We recover approximate solutions to this problem	25-42	62-93	We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately .	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	1<2	joint	joint
P14-1117	62-68	69-77	We recover approximate solutions to this problem	using LP relaxation and maximum spanning tree algorithms ,	62-93	62-93	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	1<2	manner-means	manner-means
P14-1117	62-68	78-79	We recover approximate solutions to this problem	yielding techniques	62-93	62-93	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	1<2	elab-addition	elab-addition
P14-1117	78-79	80-89	yielding techniques	that can be combined with the efficient bigram-based inference approach	62-93	62-93	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	1<2	elab-addition	elab-addition
P14-1117	78-79	90-93	yielding techniques	using Lagrange multipliers .	62-93	62-93	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	1<2	manner-means	manner-means
P14-1117	94-95	96-101	Experiments show	that these approximation strategies produce results	94-123	94-123	Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .	Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .	1>2	attribution	attribution
P14-1117	62-68	96-101	We recover approximate solutions to this problem	that these approximation strategies produce results	62-93	94-123	We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers .	Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .	1<2	evaluation	evaluation
P14-1117	96-101	102-123	that these approximation strategies produce results	comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .	94-123	94-123	Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .	Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .	1<2	elab-addition	elab-addition
P14-1118	1-15	16-21	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments	by ( i ) modeling classifiers	1-45	1-45	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	1<2	manner-means	manner-means
P14-1118	16-21	22-31	by ( i ) modeling classifiers	for predicting the opinion polarity and the type of comment	1-45	1-45	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	1<2	enablement	enablement
P14-1118	1-15	32-40	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments	and ( ii ) proposing robust shallow syntactic structures	1-45	1-45	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	1<2	manner-means	manner-means
P14-1118	32-40	41-45	and ( ii ) proposing robust shallow syntactic structures	for improving model adaptability .	1-45	1-45	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	1<2	enablement	enablement
P14-1118	1-15	46-52	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments	We rely on the tree kernel technology	1-45	46-65	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	We rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words .	1<2	elab-addition	elab-addition
P14-1118	46-52	53-65	We rely on the tree kernel technology	to automatically extract and learn features with better generalization power than bag-of-words .	46-65	46-65	We rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words .	We rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words .	1<2	elab-addition	elab-addition
P14-1118	1-15	66-81	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments	An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy	1-45	66-93	This paper defines a systematic approach to Opinion Mining ( OM ) on YouTube comments by ( i ) modeling classifiers for predicting the opinion polarity and the type of comment and ( ii ) proposing robust shallow syntactic structures for improving model adaptability .	An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting .	1<2	evaluation	evaluation
P14-1118	66-81	82-93	An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy	and highlights the benefits of structural models in a cross-domain setting .	66-93	66-93	An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting .	An extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting .	1<2	joint	joint
P14-1119	1-9	10-28	While automatic keyphrase extraction has been examined extensively ,	state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks .	1-28	1-28	While automatic keyphrase extraction has been examined extensively , state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks .	While automatic keyphrase extraction has been examined extensively , state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks .	1>2	contrast	contrast
P14-1119	10-28	29-43	state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks .	We present a survey of the state of the art in automatic keyphrase extraction ,	1-28	29-59	While automatic keyphrase extraction has been examined extensively , state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks .	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	1>2	bg-goal	bg-goal
P14-1119	29-43	44-49	We present a survey of the state of the art in automatic keyphrase extraction ,	examining the major sources of errors	29-59	29-59	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	1<2	elab-addition	elab-addition
P14-1119	44-49	50-53	examining the major sources of errors	made by existing systems	29-59	29-59	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	1<2	elab-addition	elab-addition
P14-1119	44-49	54-59	examining the major sources of errors	and discussing the challenges ahead .	29-59	29-59	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	We present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .	1<2	joint	joint
P14-1120	1-23	24-26,40-48	We present a new lexical resource for the study of preposition behavior , the Pattern Dictionary of English Prepositions ( PDEP ) .	This dictionary , <*> is linked to 81,509 sentences for 304 prepositions ,	1-23	24-61	We present a new lexical resource for the study of preposition behavior , the Pattern Dictionary of English Prepositions ( PDEP ) .	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	1<2	elab-addition	elab-addition
P14-1120	24-26,40-48	27-29	This dictionary , <*> is linked to 81,509 sentences for 304 prepositions ,	which follows principles	24-61	24-61	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	1<2	elab-addition	elab-addition
P14-1120	27-29	30-39	which follows principles	laid out in Hanks' theory of norms and exploitations ,	24-61	24-61	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	1<2	elab-addition	elab-addition
P14-1120	40-48	49-61	is linked to 81,509 sentences for 304 prepositions ,	which have been made available under The Preposition Project ( TPP ) .	24-61	24-61	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	1<2	elab-addition	elab-addition
P14-1120	24-26,40-48	62-66,70-77	This dictionary , <*> is linked to 81,509 sentences for 304 prepositions ,	Notably , 47,285 sentences , <*> provide a representative sample of preposition use ,	24-61	62-86	This dictionary , which follows principles laid out in Hanks' theory of norms and exploitations , is linked to 81,509 sentences for 304 prepositions , which have been made available under The Preposition Project ( TPP ) .	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	1<2	elab-addition	elab-addition
P14-1120	62-66,70-77	67-69	Notably , 47,285 sentences , <*> provide a representative sample of preposition use ,	initially untagged ,	62-86	62-86	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	1<2	elab-addition	elab-addition
P14-1120	62-66,70-77	78-81	Notably , 47,285 sentences , <*> provide a representative sample of preposition use ,	unlike the tagged sentences	62-86	62-86	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	1<2	comparison	comparison
P14-1120	78-81	82-86	unlike the tagged sentences	used in previous studies .	62-86	62-86	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	1<2	elab-addition	elab-addition
P14-1120	62-66,70-77	87-95	Notably , 47,285 sentences , <*> provide a representative sample of preposition use ,	Each sentence has been parsed with a dependency parser	62-86	87-116	Notably , 47,285 sentences , initially untagged , provide a representative sample of preposition use , unlike the tagged sentences used in previous studies .	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	1<2	elab-addition	elab-addition
P14-1120	87-95	96-103	Each sentence has been parsed with a dependency parser	and our system has near-instantaneous access to features	87-116	87-116	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	1<2	joint	joint
P14-1120	96-103	104-107	and our system has near-instantaneous access to features	developed with this parser	87-116	87-116	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	1<2	elab-addition	elab-addition
P14-1120	96-103	108-116	and our system has near-instantaneous access to features	to explore and annotate properties of individual senses .	87-116	87-116	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	1<2	elab-addition	elab-addition
P14-1120	96-103	117-124	and our system has near-instantaneous access to features	The features make extensive use of WordNet .	87-116	117-124	Each sentence has been parsed with a dependency parser and our system has near-instantaneous access to features developed with this parser to explore and annotate properties of individual senses .	The features make extensive use of WordNet .	1<2	elab-addition	elab-addition
P14-1120	1-23	125-129	We present a new lexical resource for the study of preposition behavior , the Pattern Dictionary of English Prepositions ( PDEP ) .	We have extended feature exploration	1-23	125-146	We present a new lexical resource for the study of preposition behavior , the Pattern Dictionary of English Prepositions ( PDEP ) .	We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior .	1<2	elab-addition	elab-addition
P14-1120	125-129	130-141	We have extended feature exploration	to include lookup of FrameNet lexical units and VerbNet classes for use	125-146	125-146	We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior .	We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior .	1<2	enablement	enablement
P14-1120	130-141	142-146	to include lookup of FrameNet lexical units and VerbNet classes for use	in characterizing preposition behavior .	125-146	125-146	We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior .	We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior .	1<2	elab-addition	elab-addition
P14-1120	1-23	147-151	We present a new lexical resource for the study of preposition behavior , the Pattern Dictionary of English Prepositions ( PDEP ) .	We have designed our system	1-23	147-165	We present a new lexical resource for the study of preposition behavior , the Pattern Dictionary of English Prepositions ( PDEP ) .	We have designed our system to allow public access to any of the data available in the system .	1<2	elab-addition	elab-addition
P14-1120	147-151	152-165	We have designed our system	to allow public access to any of the data available in the system .	147-165	147-165	We have designed our system to allow public access to any of the data available in the system .	We have designed our system to allow public access to any of the data available in the system .	1<2	enablement	enablement
P14-1121	1-16	22-28,33-46	The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis	However , the historical context-based projection method <*> is relatively insensitive to the sizes of each part of the comparable corpus .	1-21	22-46	The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced .	However , the historical context-based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus .	1>2	contrast	contrast
P14-1121	1-16	17-21	The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis	that corpora are balanced .	1-21	1-21	The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced .	The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced .	1<2	elab-addition	elab-addition
P14-1121	22-28,33-46	47-71	However , the historical context-based projection method <*> is relatively insensitive to the sizes of each part of the comparable corpus .	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction	22-46	47-75	However , the historical context-based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus .	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments .	1>2	bg-compare	bg-compare
P14-1121	22-28,33-46	29-32	However , the historical context-based projection method <*> is relatively insensitive to the sizes of each part of the comparable corpus .	dedicated to this task	22-46	22-46	However , the historical context-based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus .	However , the historical context-based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus .	1<2	elab-addition	elab-addition
P14-1121	47-71	72-75	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction	through different experiments .	47-75	47-75	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments .	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments .	1<2	manner-means	manner-means
P14-1121	47-71	76-83	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction	Moreover , we have introduced a regression model	47-75	76-97	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments .	Moreover , we have introduced a regression model that boosts the observations of word co-occurrences used in the context-based projection method .	1<2	elab-addition	elab-addition
P14-1121	76-83	84-90	Moreover , we have introduced a regression model	that boosts the observations of word co-occurrences	76-97	76-97	Moreover , we have introduced a regression model that boosts the observations of word co-occurrences used in the context-based projection method .	Moreover , we have introduced a regression model that boosts the observations of word co-occurrences used in the context-based projection method .	1<2	elab-addition	elab-addition
P14-1121	84-90	91-97	that boosts the observations of word co-occurrences	used in the context-based projection method .	76-97	76-97	Moreover , we have introduced a regression model that boosts the observations of word co-occurrences used in the context-based projection method .	Moreover , we have introduced a regression model that boosts the observations of word co-occurrences used in the context-based projection method .	1<2	elab-addition	elab-addition
P14-1121	98-100	101-119	Our results show	that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons .	98-119	98-119	Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons .	Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons .	1>2	attribution	attribution
P14-1121	47-71	101-119	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction	that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons .	47-75	98-119	Within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments .	Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons .	1<2	evaluation	evaluation
P14-1122	1-9	29-38	Large-scale knowledge bases are important assets in NLP .	However , manually validating these resources is prohibitively expensive ,	1-9	29-46	Large-scale knowledge bases are important assets in NLP .	However , manually validating these resources is prohibitively expensive , even when using methods such as crowdsourcing .	1>2	contrast	contrast
P14-1122	1-9	10-15	Large-scale knowledge bases are important assets in NLP .	Frequently , such resources are constructed	1-9	10-28	Large-scale knowledge bases are important assets in NLP .	Frequently , such resources are constructed through automatic mergers of complementary resources , such as WordNet and Wikipedia .	1<2	elab-addition	elab-addition
P14-1122	10-15	16-22	Frequently , such resources are constructed	through automatic mergers of complementary resources ,	10-28	10-28	Frequently , such resources are constructed through automatic mergers of complementary resources , such as WordNet and Wikipedia .	Frequently , such resources are constructed through automatic mergers of complementary resources , such as WordNet and Wikipedia .	1<2	manner-means	manner-means
P14-1122	16-22	23-28	through automatic mergers of complementary resources ,	such as WordNet and Wikipedia .	10-28	10-28	Frequently , such resources are constructed through automatic mergers of complementary resources , such as WordNet and Wikipedia .	Frequently , such resources are constructed through automatic mergers of complementary resources , such as WordNet and Wikipedia .	1<2	elab-example	elab-example
P14-1122	29-38	47-51	However , manually validating these resources is prohibitively expensive ,	We propose a cost-effective method	29-46	47-64	However , manually validating these resources is prohibitively expensive , even when using methods such as crowdsourcing .	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	1>2	bg-compare	bg-compare
P14-1122	29-38	39-46	However , manually validating these resources is prohibitively expensive ,	even when using methods such as crowdsourcing .	29-46	29-46	However , manually validating these resources is prohibitively expensive , even when using methods such as crowdsourcing .	However , manually validating these resources is prohibitively expensive , even when using methods such as crowdsourcing .	1<2	condition	condition
P14-1122	47-51	52-57	We propose a cost-effective method	of validating and extending knowledge bases	47-64	47-64	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	1<2	elab-addition	elab-addition
P14-1122	52-57	58-64	of validating and extending knowledge bases	using video games with a purpose .	47-64	47-64	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	1<2	manner-means	manner-means
P14-1122	47-51	65-69	We propose a cost-effective method	Two video games were created	47-64	65-76	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	Two video games were created to validate concept-concept and concept-image relations .	1<2	elab-addition	elab-addition
P14-1122	65-69	70-76	Two video games were created	to validate concept-concept and concept-image relations .	65-76	65-76	Two video games were created to validate concept-concept and concept-image relations .	Two video games were created to validate concept-concept and concept-image relations .	1<2	enablement	enablement
P14-1122	77-78	83-84	In experiments	we show	77-101	77-101	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	1>2	elab-addition	elab-addition
P14-1122	77-78	79-82	In experiments	comparing with crowdsourcing ,	77-101	77-101	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	1<2	elab-addition	elab-addition
P14-1122	83-84	85-94	we show	that video game-based validation consistently leads to higher-quality annotations ,	77-101	77-101	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	1>2	attribution	attribution
P14-1122	47-51	85-94	We propose a cost-effective method	that video game-based validation consistently leads to higher-quality annotations ,	47-64	77-101	We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose .	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	1<2	evaluation	evaluation
P14-1122	85-94	95-101	that video game-based validation consistently leads to higher-quality annotations ,	even when players are not compensated .	77-101	77-101	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	In experiments comparing with crowdsourcing , we show that video game-based validation consistently leads to higher-quality annotations , even when players are not compensated .	1<2	condition	condition
P14-1123	1-2,10-25	43-53	Designing measures <*> is a central task in the design of systems for automatic scoring of spontaneous speech .	We propose a novel measure of syntactic complexity for spontaneous speech	1-25	43-66	Designing measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech .	We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways .	1>2	bg-goal	bg-goal
P14-1123	1-2,10-25	3-9	Designing measures <*> is a central task in the design of systems for automatic scoring of spontaneous speech .	that capture various aspects of language ability	1-25	1-25	Designing measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech .	Designing measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech .	1<2	elab-addition	elab-addition
P14-1123	1-2,10-25	26-42	Designing measures <*> is a central task in the design of systems for automatic scoring of spontaneous speech .	In this study , we address a key aspect of language proficiency assessment - syntactic complexity .	1-25	26-42	Designing measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech .	In this study , we address a key aspect of language proficiency assessment - syntactic complexity .	1<2	elab-addition	elab-addition
P14-1123	43-53	54-66	We propose a novel measure of syntactic complexity for spontaneous speech	that shows optimum empirical performance on real world data in multiple ways .	43-66	43-66	We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways .	We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways .	1<2	elab-addition	elab-addition
P14-1123	43-53	67-75	We propose a novel measure of syntactic complexity for spontaneous speech	First , it is both robust and reliable ,	43-66	67-89	We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways .	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	1<2	evaluation	evaluation
P14-1123	67-75	76-78	First , it is both robust and reliable ,	producing automatic scores	67-89	67-89	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	1<2	elab-addition	elab-addition
P14-1123	76-78	79-84	producing automatic scores	that agree well with human rating	67-89	67-89	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	1<2	elab-addition	elab-addition
P14-1123	79-84	85-89	that agree well with human rating	compared to the state-of-the-art .	67-89	67-89	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	First , it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art .	1<2	comparison	comparison
P14-1123	43-53	90-108	We propose a novel measure of syntactic complexity for spontaneous speech	Second , the measure makes sense theoretically , both from algorithmic and native language acquisition points of view .	43-66	90-108	We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways .	Second , the measure makes sense theoretically , both from algorithmic and native language acquisition points of view .	1<2	evaluation	evaluation
P16-1101	1-21	22-32	State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .	In this paper , we introduce a novel neutral network architecture	1-21	22-53	State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	1>2	bg-compare	bg-compare
P16-1101	22-32	33-42	In this paper , we introduce a novel neutral network architecture	that benefits from both word- and character-level representations automatically ,	22-53	22-53	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	1<2	elab-addition	elab-addition
P16-1101	33-42	43-53	that benefits from both word- and character-level representations automatically ,	by using combination of bidirectional LSTM , CNN and CRF .	22-53	22-53	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	1<2	manner-means	manner-means
P16-1101	22-32	54-59	In this paper , we introduce a novel neutral network architecture	Our system is truly end-to-end ,	22-53	54-80	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	Our system is truly end-to-end , requiring no feature engineering or data preprocessing , thus making it applicable to a wide range of sequence labeling tasks .	1<2	elab-addition	elab-addition
P16-1101	54-59	60-67	Our system is truly end-to-end ,	requiring no feature engineering or data preprocessing ,	54-80	54-80	Our system is truly end-to-end , requiring no feature engineering or data preprocessing , thus making it applicable to a wide range of sequence labeling tasks .	Our system is truly end-to-end , requiring no feature engineering or data preprocessing , thus making it applicable to a wide range of sequence labeling tasks .	1<2	elab-addition	elab-addition
P16-1101	60-67	68-80	requiring no feature engineering or data preprocessing ,	thus making it applicable to a wide range of sequence labeling tasks .	54-80	54-80	Our system is truly end-to-end , requiring no feature engineering or data preprocessing , thus making it applicable to a wide range of sequence labeling tasks .	Our system is truly end-to-end , requiring no feature engineering or data preprocessing , thus making it applicable to a wide range of sequence labeling tasks .	1<2	result	result
P16-1101	22-32	81-93	In this paper , we introduce a novel neutral network architecture	We evaluate our system on two data sets for two sequence labeling tasks	22-53	81-116	In this paper , we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .	We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part-of-speech ( POS ) tagging and CoNLL 2003 corpus for named entity recognition ( NER ) .	1<2	evaluation	evaluation
P16-1101	81-93	94-116	We evaluate our system on two data sets for two sequence labeling tasks	- Penn Treebank WSJ corpus for part-of-speech ( POS ) tagging and CoNLL 2003 corpus for named entity recognition ( NER ) .	81-116	81-116	We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part-of-speech ( POS ) tagging and CoNLL 2003 corpus for named entity recognition ( NER ) .	We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part-of-speech ( POS ) tagging and CoNLL 2003 corpus for named entity recognition ( NER ) .	1<2	elab-enumember	elab-enumember
P16-1101	81-93	117-123	We evaluate our system on two data sets for two sequence labeling tasks	We obtain state-of-the-art performance on both datasets	81-116	117-137	We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part-of-speech ( POS ) tagging and CoNLL 2003 corpus for named entity recognition ( NER ) .	We obtain state-of-the-art performance on both datasets - 97.55 % accuracy for POS tagging and 91.21 % F1 for NER .	1<2	elab-addition	elab-addition
P16-1101	117-123	124-137	We obtain state-of-the-art performance on both datasets	- 97.55 % accuracy for POS tagging and 91.21 % F1 for NER .	117-137	117-137	We obtain state-of-the-art performance on both datasets - 97.55 % accuracy for POS tagging and 91.21 % F1 for NER .	We obtain state-of-the-art performance on both datasets - 97.55 % accuracy for POS tagging and 91.21 % F1 for NER .	1<2	exp-evidence	exp-evidence
P16-1102	1-9	20-24	Automatic spoken language assessment systems are becoming increasingly important	This is a challenging task	1-19	20-39	Automatic spoken language assessment systems are becoming increasingly important to meet the demand for English second language learning .	This is a challenging task due to the high error rates of , even state-of-the-art , non-native speech recognition .	1>2	elab-addition	elab-addition
P16-1102	1-9	10-19	Automatic spoken language assessment systems are becoming increasingly important	to meet the demand for English second language learning .	1-19	1-19	Automatic spoken language assessment systems are becoming increasingly important to meet the demand for English second language learning .	Automatic spoken language assessment systems are becoming increasingly important to meet the demand for English second language learning .	1<2	elab-addition	elab-addition
P16-1102	20-24	40-48	This is a challenging task	Consequently current systems primarily assess fluency and pronunciation .	20-39	40-48	This is a challenging task due to the high error rates of , even state-of-the-art , non-native speech recognition .	Consequently current systems primarily assess fluency and pronunciation .	1>2	result	result
P16-1102	20-24	25-39	This is a challenging task	due to the high error rates of , even state-of-the-art , non-native speech recognition .	20-39	20-39	This is a challenging task due to the high error rates of , even state-of-the-art , non-native speech recognition .	This is a challenging task due to the high error rates of , even state-of-the-art , non-native speech recognition .	1<2	exp-reason	exp-reason
P16-1102	40-48	49-58	Consequently current systems primarily assess fluency and pronunciation .	However , content assessment is essential for full automation .	40-48	49-58	Consequently current systems primarily assess fluency and pronunciation .	However , content assessment is essential for full automation .	1>2	contrast	contrast
P16-1102	49-58	101-103,114-119	However , content assessment is essential for full automation .	An alternative framework <*> is proposed in this paper .	49-58	101-119	However , content assessment is essential for full automation .	An alternative framework based on Recurrent Neural Network Language Models ( RNNLM ) is proposed in this paper .	1>2	bg-goal	bg-goal
P16-1102	59-67	83-95	As a first stage it is important to judge	Standard approaches to off-topic response detection assess similarity between the response and question	59-82	83-100	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations .	1>2	elab-addition	elab-addition
P16-1102	59-67	68-73	As a first stage it is important to judge	whether the speaker responds on topic	59-82	59-82	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	1<2	elab-addition	elab-addition
P16-1102	68-73	74-76	whether the speaker responds on topic	to test questions	59-82	59-82	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	1<2	enablement	enablement
P16-1102	74-76	77-82	to test questions	designed to elicit spontaneous speech .	59-82	59-82	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech .	1<2	elab-addition	elab-addition
P16-1102	83-95	101-103,114-119	Standard approaches to off-topic response detection assess similarity between the response and question	An alternative framework <*> is proposed in this paper .	83-100	101-119	Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations .	An alternative framework based on Recurrent Neural Network Language Models ( RNNLM ) is proposed in this paper .	1>2	bg-compare	bg-compare
P16-1102	83-95	96-100	Standard approaches to off-topic response detection assess similarity between the response and question	based on bag-of-words representations .	83-100	83-100	Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations .	Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations .	1<2	bg-general	bg-general
P16-1102	101-103,114-119	104-113	An alternative framework <*> is proposed in this paper .	based on Recurrent Neural Network Language Models ( RNNLM )	101-119	101-119	An alternative framework based on Recurrent Neural Network Language Models ( RNNLM ) is proposed in this paper .	An alternative framework based on Recurrent Neural Network Language Models ( RNNLM ) is proposed in this paper .	1<2	bg-general	bg-general
P16-1102	101-103,114-119	120-131	An alternative framework <*> is proposed in this paper .	The RNNLM is adapted to the topic of each test question .	101-119	120-131	An alternative framework based on Recurrent Neural Network Language Models ( RNNLM ) is proposed in this paper .	The RNNLM is adapted to the topic of each test question .	1<2	elab-addition	elab-addition
P16-1102	120-131	132-145	The RNNLM is adapted to the topic of each test question .	It learns to associate example responses to questions with points in a topic space	120-131	132-151	The RNNLM is adapted to the topic of each test question .	It learns to associate example responses to questions with points in a topic space constructed using these example responses .	1<2	elab-addition	elab-addition
P16-1102	132-145	146-151	It learns to associate example responses to questions with points in a topic space	constructed using these example responses .	132-151	132-151	It learns to associate example responses to questions with points in a topic space constructed using these example responses .	It learns to associate example responses to questions with points in a topic space constructed using these example responses .	1<2	elab-addition	elab-addition
P16-1102	120-131	152-154	The RNNLM is adapted to the topic of each test question .	Classification is done	120-131	152-164	The RNNLM is adapted to the topic of each test question .	Classification is done by ranking the topic-conditional posterior probabilities of a response .	1<2	elab-addition	elab-addition
P16-1102	152-154	155-164	Classification is done	by ranking the topic-conditional posterior probabilities of a response .	152-164	152-164	Classification is done by ranking the topic-conditional posterior probabilities of a response .	Classification is done by ranking the topic-conditional posterior probabilities of a response .	1<2	manner-means	manner-means
P16-1102	120-131	165-176	The RNNLM is adapted to the topic of each test question .	The RNNLMs associate a broad range of responses with each topic ,	120-131	165-191	The RNNLM is adapted to the topic of each test question .	The RNNLMs associate a broad range of responses with each topic , incorporate sequence information and scale better with additional training data , unlike standard methods .	1<2	elab-addition	elab-addition
P16-1102	165-176	177-191	The RNNLMs associate a broad range of responses with each topic ,	incorporate sequence information and scale better with additional training data , unlike standard methods .	165-191	165-191	The RNNLMs associate a broad range of responses with each topic , incorporate sequence information and scale better with additional training data , unlike standard methods .	The RNNLMs associate a broad range of responses with each topic , incorporate sequence information and scale better with additional training data , unlike standard methods .	1<2	elab-addition	elab-addition
P16-1102	192-193	206-211	On experiments	this approach outperforms standard approaches .	192-211	192-211	On experiments conducted on data from the Business Language Testing Service ( BULATS ) this approach outperforms standard approaches .	On experiments conducted on data from the Business Language Testing Service ( BULATS ) this approach outperforms standard approaches .	1>2	elab-addition	elab-addition
P16-1102	192-193	194-205	On experiments	conducted on data from the Business Language Testing Service ( BULATS )	192-211	192-211	On experiments conducted on data from the Business Language Testing Service ( BULATS ) this approach outperforms standard approaches .	On experiments conducted on data from the Business Language Testing Service ( BULATS ) this approach outperforms standard approaches .	1<2	elab-addition	elab-addition
P16-1102	101-103,114-119	206-211	An alternative framework <*> is proposed in this paper .	this approach outperforms standard approaches .	101-119	192-211	An alternative framework based on Recurrent Neural Network Language Models ( RNNLM ) is proposed in this paper .	On experiments conducted on data from the Business Language Testing Service ( BULATS ) this approach outperforms standard approaches .	1<2	evaluation	evaluation
P16-1103	1-15	16-17	Most machine translation systems construct translations from a closed vocabulary of target word forms ,	posing problems	1-27	1-27	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	1>2	result	result
P16-1103	16-17	28-34	posing problems	We present a simple and effective approach	1-27	28-43	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	We present a simple and effective approach that deals with this problem in two phases .	1>2	bg-goal	bg-goal
P16-1103	16-17	18-21	posing problems	for translating into languages	1-27	1-27	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	1<2	elab-addition	elab-addition
P16-1103	18-21	22-27	for translating into languages	that have productive compounding processes .	1-27	1-27	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes .	1<2	elab-addition	elab-addition
P16-1103	28-34	35-43	We present a simple and effective approach	that deals with this problem in two phases .	28-43	28-43	We present a simple and effective approach that deals with this problem in two phases .	We present a simple and effective approach that deals with this problem in two phases .	1<2	elab-addition	elab-addition
P16-1103	28-34	44-49	We present a simple and effective approach	First , we build a classifier	28-43	44-70	We present a simple and effective approach that deals with this problem in two phases .	First , we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language .	1<2	elab-process_step	elab-process_step
P16-1103	44-49	50-56	First , we build a classifier	that identifies spans of the input text	44-70	44-70	First , we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language .	First , we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language .	1<2	elab-addition	elab-addition
P16-1103	50-56	57-70	that identifies spans of the input text	that can be translated into a single compound word in the target language .	44-70	44-70	First , we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language .	First , we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language .	1<2	elab-addition	elab-addition
P16-1103	28-34	71-84	We present a simple and effective approach	Then , for each identified span , we generate a pool of possible compounds	28-43	71-98	We present a simple and effective approach that deals with this problem in two phases .	Then , for each identified span , we generate a pool of possible compounds which are added to the translation model as `` synthetic '' phrase translations .	1<2	elab-process_step	elab-process_step
P16-1103	71-84	85-98	Then , for each identified span , we generate a pool of possible compounds	which are added to the translation model as `` synthetic '' phrase translations .	71-98	71-98	Then , for each identified span , we generate a pool of possible compounds which are added to the translation model as `` synthetic '' phrase translations .	Then , for each identified span , we generate a pool of possible compounds which are added to the translation model as `` synthetic '' phrase translations .	1<2	elab-addition	elab-addition
P16-1103	99-100	101-114	Experiments reveal	that ( i ) we can effectively predict what spans can be compounded ;	99-141	99-141	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	1>2	attribution	attribution
P16-1103	28-34	101-114	We present a simple and effective approach	that ( i ) we can effectively predict what spans can be compounded ;	28-43	99-141	We present a simple and effective approach that deals with this problem in two phases .	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	1<2	evaluation	evaluation
P16-1103	101-114	115-125	that ( i ) we can effectively predict what spans can be compounded ;	( ii ) our compound generation model produces good compounds ;	99-141	99-141	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	1<2	joint	joint
P16-1103	101-114	126-141	that ( i ) we can effectively predict what spans can be compounded ;	and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	99-141	99-141	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks .	1<2	joint	joint
P16-1103	28-34	142-160	We present a simple and effective approach	We additionally introduce KomposEval , a new multi-reference dataset of English phrases and their translations into German compounds .	28-43	142-160	We present a simple and effective approach that deals with this problem in two phases .	We additionally introduce KomposEval , a new multi-reference dataset of English phrases and their translations into German compounds .	1<2	elab-addition	elab-addition
P16-1104	1-9	10-25	In this paper , we propose a novel mechanism	for enriching the feature vector , for the task of sarcasm detection , with cognitive features	1-33	1-33	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	1<2	elab-addition	elab-addition
P16-1104	10-25	26-33	for enriching the feature vector , for the task of sarcasm detection , with cognitive features	extracted from eye-movement patterns of human readers .	1-33	1-33	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	1<2	elab-addition	elab-addition
P16-1104	1-9	34-42	In this paper , we propose a novel mechanism	Sarcasm detection has been a challenging research problem ,	1-33	34-62	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	Sarcasm detection has been a challenging research problem , and its importance for NLP applications such as review summarization , dialog systems and sentiment analysis is well recognized .	1<2	bg-goal	bg-goal
P16-1104	34-42	43-48,59-62	Sarcasm detection has been a challenging research problem ,	and its importance for NLP applications <*> is well recognized .	34-62	34-62	Sarcasm detection has been a challenging research problem , and its importance for NLP applications such as review summarization , dialog systems and sentiment analysis is well recognized .	Sarcasm detection has been a challenging research problem , and its importance for NLP applications such as review summarization , dialog systems and sentiment analysis is well recognized .	1<2	joint	joint
P16-1104	43-48,59-62	49-58	and its importance for NLP applications <*> is well recognized .	such as review summarization , dialog systems and sentiment analysis	34-62	34-62	Sarcasm detection has been a challenging research problem , and its importance for NLP applications such as review summarization , dialog systems and sentiment analysis is well recognized .	Sarcasm detection has been a challenging research problem , and its importance for NLP applications such as review summarization , dialog systems and sentiment analysis is well recognized .	1<2	elab-example	elab-example
P16-1104	1-9	63-69	In this paper , we propose a novel mechanism	Sarcasm can often be traced to incongruity	1-33	63-78	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds .	1<2	elab-addition	elab-addition
P16-1104	63-69	70-72	Sarcasm can often be traced to incongruity	that becomes apparent	63-78	63-78	Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds .	Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds .	1<2	elab-addition	elab-addition
P16-1104	70-72	73-78	that becomes apparent	as the full sentence unfolds .	63-78	63-78	Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds .	Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds .	1<2	temporal	temporal
P16-1104	63-69	79-88	Sarcasm can often be traced to incongruity	This presence of incongruity- implicit or explicit- affects the way	63-78	79-95	Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds .	This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text .	1<2	elab-addition	elab-addition
P16-1104	79-88	89-95	This presence of incongruity- implicit or explicit- affects the way	readers eyes move through the text .	79-95	79-95	This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text .	This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text .	1<2	elab-addition	elab-addition
P16-1104	89-95	96-106	readers eyes move through the text .	We observe the difference in the behaviour of the eye ,	79-95	96-114	This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text .	We observe the difference in the behaviour of the eye , while reading sarcastic and non sarcastic sentences .	1<2	elab-addition	elab-addition
P16-1104	96-106	107-114	We observe the difference in the behaviour of the eye ,	while reading sarcastic and non sarcastic sentences .	96-114	96-114	We observe the difference in the behaviour of the eye , while reading sarcastic and non sarcastic sentences .	We observe the difference in the behaviour of the eye , while reading sarcastic and non sarcastic sentences .	1<2	temporal	temporal
P16-1104	115-119	120-133	Motivated by this observation ,	we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features	115-140	115-140	Motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data .	Motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data .	1>2	elab-addition	elab-addition
P16-1104	96-106	120-133	We observe the difference in the behaviour of the eye ,	we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features	96-114	115-140	We observe the difference in the behaviour of the eye , while reading sarcastic and non sarcastic sentences .	Motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data .	1<2	elab-addition	elab-addition
P16-1104	120-133	134-140	we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features	obtained from readers eye movement data .	115-140	115-140	Motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data .	Motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data .	1<2	elab-addition	elab-addition
P16-1104	1-9	141-144	In this paper , we propose a novel mechanism	We perform statistical classification	1-33	141-152	In this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cognitive features extracted from eye-movement patterns of human readers .	We perform statistical classification using the enhanced feature set so obtained .	1<2	evaluation	evaluation
P16-1104	141-144	145-149	We perform statistical classification	using the enhanced feature set	141-152	141-152	We perform statistical classification using the enhanced feature set so obtained .	We perform statistical classification using the enhanced feature set so obtained .	1<2	manner-means	manner-means
P16-1104	145-149	150-152	using the enhanced feature set	so obtained .	141-152	141-152	We perform statistical classification using the enhanced feature set so obtained .	We perform statistical classification using the enhanced feature set so obtained .	1<2	elab-addition	elab-addition
P16-1104	141-144	153-178	We perform statistical classification	The augmented cognitive features improve sarcasm detection by 3.7 % ( in terms of Fscore ) , over the performance of the best reported system .	141-152	153-178	We perform statistical classification using the enhanced feature set so obtained .	The augmented cognitive features improve sarcasm detection by 3.7 % ( in terms of Fscore ) , over the performance of the best reported system .	1<2	exp-evidence	exp-evidence
P16-1105	1-7	8-15	We present a novel end-to-end neural model	to extract entities and relations between them .	1-15	1-15	We present a novel end-to-end neural model to extract entities and relations between them .	We present a novel end-to-end neural model to extract entities and relations between them .	1<2	enablement	enablement
P16-1105	1-7	16-30	We present a novel end-to-end neural model	Our recurrent neural network based model captures both word sequence and dependency tree substructure information	1-15	16-40	We present a novel end-to-end neural model to extract entities and relations between them .	Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs .	1<2	elab-addition	elab-addition
P16-1105	16-30	31-40	Our recurrent neural network based model captures both word sequence and dependency tree substructure information	by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs .	16-40	16-40	Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs .	Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs .	1<2	manner-means	manner-means
P16-1105	16-30	41-59	Our recurrent neural network based model captures both word sequence and dependency tree substructure information	This allows our model to jointly represent both entities and relations with shared parameters in a single model .	16-40	41-59	Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs .	This allows our model to jointly represent both entities and relations with shared parameters in a single model .	1<2	elab-addition	elab-addition
P16-1105	1-7	60-65	We present a novel end-to-end neural model	We further encourage detection of entities	1-15	60-82	We present a novel end-to-end neural model to extract entities and relations between them .	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	1<2	elab-addition	elab-addition
P16-1105	60-65	66-67	We further encourage detection of entities	during training	60-82	60-82	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	1<2	temporal	temporal
P16-1105	60-65	68-75	We further encourage detection of entities	and use of entity information in relation extraction	60-82	60-82	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	1<2	joint	joint
P16-1105	68-75	76-82	and use of entity information in relation extraction	via entity pretraining and scheduled sampling .	60-82	60-82	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .	1<2	manner-means	manner-means
P16-1105	1-7	83-95	We present a novel end-to-end neural model	Our model improves over the stateof-the-art feature-based model on end-toend relation extraction ,	1-15	83-113	We present a novel end-to-end neural model to extract entities and relations between them .	Our model improves over the stateof-the-art feature-based model on end-toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1-score on ACE2005 and ACE2004 , respectively .	1<2	evaluation	evaluation
P16-1105	83-95	96-113	Our model improves over the stateof-the-art feature-based model on end-toend relation extraction ,	achieving 12.1 % and 5.7 % relative error reductions in F1-score on ACE2005 and ACE2004 , respectively .	83-113	83-113	Our model improves over the stateof-the-art feature-based model on end-toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1-score on ACE2005 and ACE2004 , respectively .	Our model improves over the stateof-the-art feature-based model on end-toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1-score on ACE2005 and ACE2004 , respectively .	1<2	exp-evidence	exp-evidence
P16-1105	114-116	117-143	We also show	that our LSTMRNN based model compares favorably to the state-of-the-art CNN based model ( in F1-score ) on nominal relation classification ( SemEval-2010 Task 8 ) .	114-143	114-143	We also show that our LSTMRNN based model compares favorably to the state-of-the-art CNN based model ( in F1-score ) on nominal relation classification ( SemEval-2010 Task 8 ) .	We also show that our LSTMRNN based model compares favorably to the state-of-the-art CNN based model ( in F1-score ) on nominal relation classification ( SemEval-2010 Task 8 ) .	1>2	attribution	attribution
P16-1105	1-7	117-143	We present a novel end-to-end neural model	that our LSTMRNN based model compares favorably to the state-of-the-art CNN based model ( in F1-score ) on nominal relation classification ( SemEval-2010 Task 8 ) .	1-15	114-143	We present a novel end-to-end neural model to extract entities and relations between them .	We also show that our LSTMRNN based model compares favorably to the state-of-the-art CNN based model ( in F1-score ) on nominal relation classification ( SemEval-2010 Task 8 ) .	1<2	evaluation	evaluation
P16-1105	1-7	144-156	We present a novel end-to-end neural model	Finally , we present an extensive ablation analysis of several model components .	1-15	144-156	We present a novel end-to-end neural model to extract entities and relations between them .	Finally , we present an extensive ablation analysis of several model components .	1<2	evaluation	evaluation
P16-1106	1-5	6-13	We present a new proof	that O2 is a multiple context-free language .	1-13	1-13	We present a new proof that O2 is a multiple context-free language .	We present a new proof that O2 is a multiple context-free language .	1<2	elab-addition	elab-addition
P16-1106	1-5	14-29	We present a new proof	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts	1-13	14-43	We present a new proof that O2 is a multiple context-free language .	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts that seem specific to two-dimensional geometry , such as the complex exponential function .	1<2	contrast	contrast
P16-1106	14-29	30-36	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts	that seem specific to two-dimensional geometry ,	14-43	14-43	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts that seem specific to two-dimensional geometry , such as the complex exponential function .	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts that seem specific to two-dimensional geometry , such as the complex exponential function .	1<2	elab-addition	elab-addition
P16-1106	14-29	37-43	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts	such as the complex exponential function .	14-43	14-43	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts that seem specific to two-dimensional geometry , such as the complex exponential function .	It contrasts with a recent proof by Salvati ( 2015 ) in its avoidance of concepts that seem specific to two-dimensional geometry , such as the complex exponential function .	1<2	elab-enumember	elab-enumember
P16-1106	1-5	44-49	We present a new proof	Our simple proof creates realistic prospects	1-13	44-57	We present a new proof that O2 is a multiple context-free language .	Our simple proof creates realistic prospects of widening the results to higher dimensions .	1<2	elab-addition	elab-addition
P16-1106	44-49	50-57	Our simple proof creates realistic prospects	of widening the results to higher dimensions .	44-57	44-57	Our simple proof creates realistic prospects of widening the results to higher dimensions .	Our simple proof creates realistic prospects of widening the results to higher dimensions .	1<2	elab-addition	elab-addition
P16-1106	1-5	58-75	We present a new proof	This finding is of central importance to the relation between extreme free word order and classes of grammars	1-13	58-84	We present a new proof that O2 is a multiple context-free language .	This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language .	1<2	evaluation	evaluation
P16-1106	58-75	76-84	This finding is of central importance to the relation between extreme free word order and classes of grammars	used to describe the syntax of natural language .	58-84	58-84	This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language .	This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language .	1<2	elab-addition	elab-addition
P16-1107	1-3	11-22	Context is crucial	but many argument mining methods make little use of contextual features .	1-22	1-22	Context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contextual features .	Context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contextual features .	1>2	contrast	contrast
P16-1107	1-3	4-10	Context is crucial	for identifying argumentative relations in text ,	1-22	1-22	Context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contextual features .	Context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contextual features .	1<2	elab-addition	elab-addition
P16-1107	11-22	23-29	but many argument mining methods make little use of contextual features .	This paper presents contextaware argumentative relation mining	1-22	23-45	Context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contextual features .	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	1>2	bg-compare	bg-compare
P16-1107	23-29	30-32	This paper presents contextaware argumentative relation mining	that uses features	23-45	23-45	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	1<2	elab-addition	elab-addition
P16-1107	30-32	33-36	that uses features	extracted from writing topics	23-45	23-45	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	1<2	elab-addition	elab-addition
P16-1107	33-36	37-45	extracted from writing topics	as well as from windows of context sentences .	23-45	23-45	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	1<2	joint	joint
P16-1107	46-50	51-64	Experiments on student essays demonstrate	that the proposed features improve predictive performance in two argumentative relation classification tasks .	46-64	46-64	Experiments on student essays demonstrate that the proposed features improve predictive performance in two argumentative relation classification tasks .	Experiments on student essays demonstrate that the proposed features improve predictive performance in two argumentative relation classification tasks .	1>2	attribution	attribution
P16-1107	23-29	51-64	This paper presents contextaware argumentative relation mining	that the proposed features improve predictive performance in two argumentative relation classification tasks .	23-45	46-64	This paper presents contextaware argumentative relation mining that uses features extracted from writing topics as well as from windows of context sentences .	Experiments on student essays demonstrate that the proposed features improve predictive performance in two argumentative relation classification tasks .	1<2	evaluation	evaluation
P16-1108	1-8	9-14	We present several methods for stemming and lemmatization	based on discriminative string transduction .	1-14	1-14	We present several methods for stemming and lemmatization based on discriminative string transduction .	We present several methods for stemming and lemmatization based on discriminative string transduction .	1<2	bg-general	bg-general
P16-1108	1-8	15-23	We present several methods for stemming and lemmatization	We exploit the paradigmatic regularity of semi-structured inflection tables	1-14	15-36	We present several methods for stemming and lemmatization based on discriminative string transduction .	We exploit the paradigmatic regularity of semi-structured inflection tables to identify stems in an unsupervised manner with over 85 % accuracy .	1<2	elab-addition	elab-addition
P16-1108	15-23	24-36	We exploit the paradigmatic regularity of semi-structured inflection tables	to identify stems in an unsupervised manner with over 85 % accuracy .	15-36	15-36	We exploit the paradigmatic regularity of semi-structured inflection tables to identify stems in an unsupervised manner with over 85 % accuracy .	We exploit the paradigmatic regularity of semi-structured inflection tables to identify stems in an unsupervised manner with over 85 % accuracy .	1<2	enablement	enablement
P16-1108	37-44	45-53	Experiments on English , Dutch and German show	that our stemmers substantially outperform Snowball and Morfessor ,	37-62	37-62	Experiments on English , Dutch and German show that our stemmers substantially outperform Snowball and Morfessor , and approach the accuracy of a supervised model .	Experiments on English , Dutch and German show that our stemmers substantially outperform Snowball and Morfessor , and approach the accuracy of a supervised model .	1>2	attribution	attribution
P16-1108	1-8	45-53	We present several methods for stemming and lemmatization	that our stemmers substantially outperform Snowball and Morfessor ,	1-14	37-62	We present several methods for stemming and lemmatization based on discriminative string transduction .	Experiments on English , Dutch and German show that our stemmers substantially outperform Snowball and Morfessor , and approach the accuracy of a supervised model .	1<2	evaluation	evaluation
P16-1108	45-53	54-62	that our stemmers substantially outperform Snowball and Morfessor ,	and approach the accuracy of a supervised model .	37-62	37-62	Experiments on English , Dutch and German show that our stemmers substantially outperform Snowball and Morfessor , and approach the accuracy of a supervised model .	Experiments on English , Dutch and German show that our stemmers substantially outperform Snowball and Morfessor , and approach the accuracy of a supervised model .	1<2	joint	joint
P16-1108	1-8	63-72	We present several methods for stemming and lemmatization	Furthermore , the generated stems are more consistent than those	1-14	63-76	We present several methods for stemming and lemmatization based on discriminative string transduction .	Furthermore , the generated stems are more consistent than those annotated by experts .	1<2	evaluation	evaluation
P16-1108	63-72	73-76	Furthermore , the generated stems are more consistent than those	annotated by experts .	63-76	63-76	Furthermore , the generated stems are more consistent than those annotated by experts .	Furthermore , the generated stems are more consistent than those annotated by experts .	1<2	elab-addition	elab-addition
P16-1108	1-8	77-91	We present several methods for stemming and lemmatization	Our direct lemmatization model is more accurate than Morfette and Lemming on most datasets .	1-14	77-91	We present several methods for stemming and lemmatization based on discriminative string transduction .	Our direct lemmatization model is more accurate than Morfette and Lemming on most datasets .	1<2	evaluation	evaluation
P16-1108	1-8	92-108	We present several methods for stemming and lemmatization	Finally , we test our methods on the data from the shared task on morphological reinflection .	1-14	92-108	We present several methods for stemming and lemmatization based on discriminative string transduction .	Finally , we test our methods on the data from the shared task on morphological reinflection .	1<2	evaluation	evaluation
P16-1109	1-24	25-46	A key goal in natural language generation ( NLG ) is to enable fast generation even with large vocabularies , grammars and worlds .	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	1-24	25-46	A key goal in natural language generation ( NLG ) is to enable fast generation even with large vocabularies , grammars and worlds .	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	1>2	bg-goal	bg-goal
P16-1109	25-46	47-53	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	We describe four enhancements to this system	25-46	47-108	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	1<2	elab-addition	elab-addition
P16-1109	47-53	54-60	We describe four enhancements to this system	: ( i ) pruning the grammar	47-108	47-108	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	1<2	elab-enumember	elab-enumember
P16-1109	54-60	61-69	: ( i ) pruning the grammar	based on the world and the communicative goal ,	47-108	47-108	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	1<2	bg-general	bg-general
P16-1109	54-60	70-83	: ( i ) pruning the grammar	( ii ) intelligently caching and pruning the combinatorial space of semantic bindings ,	47-108	47-108	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	1<2	joint	joint
P16-1109	54-60	84-96	: ( i ) pruning the grammar	( iii ) reusing the lookahead search tree at different search depths ,	47-108	47-108	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	1<2	joint	joint
P16-1109	54-60	97-108	: ( i ) pruning the grammar	and ( iv ) learning and using a search control heuristic .	47-108	47-108	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	We describe four enhancements to this system : ( i ) pruning the grammar based on the world and the communicative goal , ( ii ) intelligently caching and pruning the combinatorial space of semantic bindings , ( iii ) reusing the lookahead search tree at different search depths , and ( iv ) learning and using a search control heuristic .	1<2	joint	joint
P16-1109	25-46	109-122	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	We evaluate the resulting system on three datasets of increasing size and complexity ,	25-46	109-154	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	We evaluate the resulting system on three datasets of increasing size and complexity , the largest of which has a vocabulary of about 10K words , a grammar of about 32K lexicalized trees and a world with about 11K entities and 23K relations between them .	1<2	evaluation	evaluation
P16-1109	109-122	123-154	We evaluate the resulting system on three datasets of increasing size and complexity ,	the largest of which has a vocabulary of about 10K words , a grammar of about 32K lexicalized trees and a world with about 11K entities and 23K relations between them .	109-154	109-154	We evaluate the resulting system on three datasets of increasing size and complexity , the largest of which has a vocabulary of about 10K words , a grammar of about 32K lexicalized trees and a world with about 11K entities and 23K relations between them .	We evaluate the resulting system on three datasets of increasing size and complexity , the largest of which has a vocabulary of about 10K words , a grammar of about 32K lexicalized trees and a world with about 11K entities and 23K relations between them .	1<2	elab-addition	elab-addition
P16-1109	155-157	158-167	Our results show	that the system has a median generation time of 8.5s	155-177	155-177	Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s .	Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s .	1>2	attribution	attribution
P16-1109	25-46	158-167	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	that the system has a median generation time of 8.5s	25-46	155-177	In this work , we build upon a recently proposed NLG system , Sentence Tree Realization with UCT ( STRUCT ) .	Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s .	1<2	evaluation	evaluation
P16-1109	158-167	168-177	that the system has a median generation time of 8.5s	and finds the best sentence on average within 25s .	155-177	155-177	Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s .	Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s .	1<2	joint	joint
P16-1109	158-167	178-187	that the system has a median generation time of 8.5s	These results are based on a sequential , interpreted implementation	155-177	178-203	Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s .	These results are based on a sequential , interpreted implementation and are significantly better than the state of the art for planning based NLG systems .	1<2	elab-addition	elab-addition
P16-1109	178-187	188-203	These results are based on a sequential , interpreted implementation	and are significantly better than the state of the art for planning based NLG systems .	178-203	178-203	These results are based on a sequential , interpreted implementation and are significantly better than the state of the art for planning based NLG systems .	These results are based on a sequential , interpreted implementation and are significantly better than the state of the art for planning based NLG systems .	1<2	joint	joint
P16-1110	1-11	12-21	Effective text classification requires experts to annotate data with labels ;	these training data are time-consuming and expensive to obtain .	1-21	1-21	Effective text classification requires experts to annotate data with labels ; these training data are time-consuming and expensive to obtain .	Effective text classification requires experts to annotate data with labels ; these training data are time-consuming and expensive to obtain .	1>2	elab-addition	elab-addition
P16-1110	12-21	41-49	these training data are time-consuming and expensive to obtain .	However , establishing the label set remains difficult .	1-21	41-49	Effective text classification requires experts to annotate data with labels ; these training data are time-consuming and expensive to obtain .	However , establishing the label set remains difficult .	1>2	elab-addition	elab-addition
P16-1110	22-29	30-40	If you know what labels you want ,	active learning can reduce the number of labeled documents needed .	22-40	22-40	If you know what labels you want , active learning can reduce the number of labeled documents needed .	If you know what labels you want , active learning can reduce the number of labeled documents needed .	1>2	condition	condition
P16-1110	30-40	41-49	active learning can reduce the number of labeled documents needed .	However , establishing the label set remains difficult .	22-40	41-49	If you know what labels you want , active learning can reduce the number of labeled documents needed .	However , establishing the label set remains difficult .	1>2	contrast	contrast
P16-1110	41-49	63-75	However , establishing the label set remains difficult .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system	41-49	63-104	However , establishing the label set remains difficult .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	1>2	bg-goal	bg-goal
P16-1110	41-49	50-55	However , establishing the label set remains difficult .	Annotators often lack the global knowledge	41-49	50-62	However , establishing the label set remains difficult .	Annotators often lack the global knowledge needed to induce a label set .	1<2	exp-reason	exp-reason
P16-1110	50-55	56-62	Annotators often lack the global knowledge	needed to induce a label set .	50-62	50-62	Annotators often lack the global knowledge needed to induce a label set .	Annotators often lack the global knowledge needed to induce a label set .	1<2	elab-addition	elab-addition
P16-1110	63-75	76-81	We introduce ALTO : Active Learning with Topic Overviews , an interactive system	to help humans annotate documents :	63-104	63-104	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	1<2	enablement	enablement
P16-1110	76-81	82-87	to help humans annotate documents :	topic models provide a global overview	63-104	63-104	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	1<2	elab-addition	elab-addition
P16-1110	82-87	88-92	topic models provide a global overview	of what labels to create	63-104	63-104	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	1<2	elab-addition	elab-addition
P16-1110	82-87	93-101	topic models provide a global overview	and active learning directs them to the right documents	63-104	63-104	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	1<2	joint	joint
P16-1110	93-101	102-104	and active learning directs them to the right documents	to label .	63-104	63-104	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	1<2	elab-addition	elab-addition
P16-1110	105-109	123-135	Our forty-annotator user study shows	topic models ( even by themselves ) lead to better label sets ,	105-143	105-143	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	1>2	attribution	attribution
P16-1110	110-122	123-135	that while active learning alone is best in extremely resource limited conditions ,	topic models ( even by themselves ) lead to better label sets ,	105-143	105-143	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	1>2	temporal	temporal
P16-1110	63-75	123-135	We introduce ALTO : Active Learning with Topic Overviews , an interactive system	topic models ( even by themselves ) lead to better label sets ,	63-104	105-143	We introduce ALTO : Active Learning with Topic Overviews , an interactive system to help humans annotate documents : topic models provide a global overview of what labels to create and active learning directs them to the right documents to label .	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	1<2	evaluation	evaluation
P16-1110	123-135	136-143	topic models ( even by themselves ) lead to better label sets ,	and ALTO 's combination is best overall .	105-143	105-143	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions , topic models ( even by themselves ) lead to better label sets , and ALTO 's combination is best overall .	1<2	joint	joint
P16-1111	1-6,17-26	27-34	Computationally modeling the evolution of science <*> has important implications for research funding and public policy .	However , little is known about the mechanisms	1-26	27-40	Computationally modeling the evolution of science by tracking how scientific topics rise and fall over time has important implications for research funding and public policy .	However , little is known about the mechanisms underlying topic growth and decline .	1>2	contrast	contrast
P16-1111	1-6,17-26	7-16	Computationally modeling the evolution of science <*> has important implications for research funding and public policy .	by tracking how scientific topics rise and fall over time	1-26	1-26	Computationally modeling the evolution of science by tracking how scientific topics rise and fall over time has important implications for research funding and public policy .	Computationally modeling the evolution of science by tracking how scientific topics rise and fall over time has important implications for research funding and public policy .	1<2	manner-means	manner-means
P16-1111	27-34	41-48	However , little is known about the mechanisms	We investigate the role of rhetorical framing :	27-40	41-80	However , little is known about the mechanisms underlying topic growth and decline .	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	1>2	bg-goal	bg-goal
P16-1111	27-34	35-40	However , little is known about the mechanisms	underlying topic growth and decline .	27-40	27-40	However , little is known about the mechanisms underlying topic growth and decline .	However , little is known about the mechanisms underlying topic growth and decline .	1<2	elab-addition	elab-addition
P16-1111	41-48	49-54,72-80	We investigate the role of rhetorical framing :	whether the rhetorical role or function <*> relates to the historical trajectory of the topics .	41-80	41-80	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	1<2	elab-addition	elab-addition
P16-1111	49-54,72-80	55-71	whether the rhetorical role or function <*> relates to the historical trajectory of the topics .	that authors ascribe to topics ( as methods , as goals , as results , etc. )	41-80	41-80	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	1<2	elab-addition	elab-addition
P16-1111	41-48	81-89	We investigate the role of rhetorical framing :	We train topic models and a rhetorical function classifier	41-80	81-109	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010 .	1<2	elab-addition	elab-addition
P16-1111	81-89	90-109	We train topic models and a rhetorical function classifier	to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010 .	81-109	81-109	We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010 .	We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010 .	1<2	enablement	enablement
P16-1111	110-111	112-127	We find	that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	110-127	110-127	We find that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	We find that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	1>2	attribution	attribution
P16-1111	41-48	112-127	We investigate the role of rhetorical framing :	that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	41-80	110-127	We investigate the role of rhetorical framing : whether the rhetorical role or function that authors ascribe to topics ( as methods , as goals , as results , etc. ) relates to the historical trajectory of the topics .	We find that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	1<2	elab-addition	elab-addition
P16-1111	112-127	128-131,138-143	that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	For example , topics <*> tend to be in decline ,	110-127	128-158	We find that a topic 's rhetorical function is highly predictive of its eventual growth or decline .	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	1<2	elab-example	elab-example
P16-1111	128-131,138-143	132-137	For example , topics <*> tend to be in decline ,	that are rhetorically described as results	128-158	128-158	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	1<2	elab-addition	elab-addition
P16-1111	128-131,138-143	144-145,150-158	For example , topics <*> tend to be in decline ,	while topics <*> tend to be in early phases of growth .	128-158	128-158	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	1<2	joint	joint
P16-1111	144-145,150-158	146-149	while topics <*> tend to be in early phases of growth .	that function as methods	128-158	128-158	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	For example , topics that are rhetorically described as results tend to be in decline , while topics that function as methods tend to be in early phases of growth .	1<2	elab-addition	elab-addition
P16-1112	1-9	10-23	In this paper , we present the first experiments	using neural network models for the task of error detection in learner writing .	1-23	1-23	In this paper , we present the first experiments using neural network models for the task of error detection in learner writing .	In this paper , we present the first experiments using neural network models for the task of error detection in learner writing .	1<2	manner-means	manner-means
P16-1112	1-9	24-32	In this paper , we present the first experiments	We perform a systematic comparison of alternative compositional architectures	1-23	24-44	In this paper , we present the first experiments using neural network models for the task of error detection in learner writing .	We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs .	1<2	elab-addition	elab-addition
P16-1112	24-32	33-39	We perform a systematic comparison of alternative compositional architectures	and propose a framework for error detection	24-44	24-44	We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs .	We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs .	1<2	joint	joint
P16-1112	33-39	40-44	and propose a framework for error detection	based on bidirectional LSTMs .	24-44	24-44	We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs .	We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs .	1<2	bg-general	bg-general
P16-1112	45-52	53-60	Experiments on the CoNLL-14 shared task dataset show	the model is able to outperform other participants	45-67	45-67	Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing .	Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing .	1>2	attribution	attribution
P16-1112	1-9	53-60	In this paper , we present the first experiments	the model is able to outperform other participants	1-23	45-67	In this paper , we present the first experiments using neural network models for the task of error detection in learner writing .	Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing .	1<2	evaluation	evaluation
P16-1112	53-60	61-67	the model is able to outperform other participants	on detecting errors in learner writing .	45-67	45-67	Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing .	Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing .	1<2	elab-addition	elab-addition
P16-1112	1-9	68-80	In this paper , we present the first experiments	Finally , the model is integrated with a publicly deployed self-assessment system ,	1-23	68-88	In this paper , we present the first experiments using neural network models for the task of error detection in learner writing .	Finally , the model is integrated with a publicly deployed self-assessment system , leading to performance comparable to human annotators .	1<2	evaluation	evaluation
P16-1112	68-80	81-83	Finally , the model is integrated with a publicly deployed self-assessment system ,	leading to performance	68-88	68-88	Finally , the model is integrated with a publicly deployed self-assessment system , leading to performance comparable to human annotators .	Finally , the model is integrated with a publicly deployed self-assessment system , leading to performance comparable to human annotators .	1<2	elab-addition	elab-addition
P16-1112	81-83	84-88	leading to performance	comparable to human annotators .	68-88	68-88	Finally , the model is integrated with a publicly deployed self-assessment system , leading to performance comparable to human annotators .	Finally , the model is integrated with a publicly deployed self-assessment system , leading to performance comparable to human annotators .	1<2	comparison	comparison
P16-1113	1-10	11-19	This paper introduces a novel model for semantic role labeling	that makes use of neural sequence modeling techniques .	1-19	1-19	This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques .	This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques .	1<2	elab-addition	elab-addition
P16-1113	1-10	20-26	This paper introduces a novel model for semantic role labeling	Our approach is motivated by the observation	1-19	20-50	This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques .	Our approach is motivated by the observation that complex syntactic structures and related phenomena , such as nested subordinations and nominal predicates , are not handled well by existing models .	1<2	elab-addition	elab-addition
P16-1113	20-26	27-34,43-50	Our approach is motivated by the observation	that complex syntactic structures and related phenomena , <*> are not handled well by existing models .	20-50	20-50	Our approach is motivated by the observation that complex syntactic structures and related phenomena , such as nested subordinations and nominal predicates , are not handled well by existing models .	Our approach is motivated by the observation that complex syntactic structures and related phenomena , such as nested subordinations and nominal predicates , are not handled well by existing models .	1<2	elab-addition	elab-addition
P16-1113	27-34,43-50	35-42	that complex syntactic structures and related phenomena , <*> are not handled well by existing models .	such as nested subordinations and nominal predicates ,	20-50	20-50	Our approach is motivated by the observation that complex syntactic structures and related phenomena , such as nested subordinations and nominal predicates , are not handled well by existing models .	Our approach is motivated by the observation that complex syntactic structures and related phenomena , such as nested subordinations and nominal predicates , are not handled well by existing models .	1<2	elab-example	elab-example
P16-1113	1-10	51-61	This paper introduces a novel model for semantic role labeling	Our model treats such instances as subsequences of lexicalized dependency paths	1-19	51-67	This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques .	Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations .	1<2	elab-addition	elab-addition
P16-1113	51-61	62-67	Our model treats such instances as subsequences of lexicalized dependency paths	and learns suitable embedding representations .	51-67	51-67	Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations .	Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations .	1<2	joint	joint
P16-1113	68-70	71-83	We experimentally demonstrate	that such embeddings can improve results over previous state-of-the-art semantic role labelers ,	68-92	68-92	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	1>2	attribution	attribution
P16-1113	1-10	71-83	This paper introduces a novel model for semantic role labeling	that such embeddings can improve results over previous state-of-the-art semantic role labelers ,	1-19	68-92	This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques .	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	1<2	evaluation	evaluation
P16-1113	71-83	84-87	that such embeddings can improve results over previous state-of-the-art semantic role labelers ,	and showcase qualitative improvements	68-92	68-92	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	1<2	joint	joint
P16-1113	84-87	88-92	and showcase qualitative improvements	obtained by our method .	68-92	68-92	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers , and showcase qualitative improvements obtained by our method .	1<2	elab-addition	elab-addition
P16-1114	1-6,11-22	23-36	Intelligent assistants on mobile devices , <*> have recently gained considerable attention as novel applications of dialogue technologies .	A tremendous amount of real users of intelligent assistants provide us with an opportunity	1-22	23-55	Intelligent assistants on mobile devices , such as Siri , have recently gained considerable attention as novel applications of dialogue technologies .	A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future .	1>2	elab-addition	elab-addition
P16-1114	1-6,11-22	7-10	Intelligent assistants on mobile devices , <*> have recently gained considerable attention as novel applications of dialogue technologies .	such as Siri ,	1-22	1-22	Intelligent assistants on mobile devices , such as Siri , have recently gained considerable attention as novel applications of dialogue technologies .	Intelligent assistants on mobile devices , such as Siri , have recently gained considerable attention as novel applications of dialogue technologies .	1<2	elab-example	elab-example
P16-1114	23-36	56-63	A tremendous amount of real users of intelligent assistants provide us with an opportunity	We developed prediction models of prospective user engagement	23-55	56-75	A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future .	We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant .	1>2	bg-goal	bg-goal
P16-1114	23-36	37-41	A tremendous amount of real users of intelligent assistants provide us with an opportunity	to explore a novel task	23-55	23-55	A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future .	A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future .	1<2	elab-addition	elab-addition
P16-1114	37-41	42-55	to explore a novel task	of predicting whether users will continually use their intelligent assistants in the future .	23-55	23-55	A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future .	A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future .	1<2	elab-addition	elab-addition
P16-1114	56-63	64-68	We developed prediction models of prospective user engagement	by using large-scale user logs	56-75	56-75	We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant .	We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant .	1<2	manner-means	manner-means
P16-1114	64-68	69-75	by using large-scale user logs	obtained from a commercial intelligent assistant .	56-75	56-75	We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant .	We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant .	1<2	elab-addition	elab-addition
P16-1114	76-77	78-88	Experiments demonstrated	that our models can predict prospective user engagement reasonably well ,	76-100	76-100	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	1>2	attribution	attribution
P16-1114	56-63	78-88	We developed prediction models of prospective user engagement	that our models can predict prospective user engagement reasonably well ,	56-75	76-100	We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant .	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	1<2	evaluation	evaluation
P16-1114	78-88	89-93	that our models can predict prospective user engagement reasonably well ,	and outperforms a strong baseline	76-100	76-100	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	1<2	joint	joint
P16-1114	89-93	94-100	and outperforms a strong baseline	that makes prediction based past utterance frequency	76-100	76-100	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency	1<2	elab-addition	elab-addition
P16-1115	1-13	14-26	A common use of language is to refer to visually present objects .	Modelling it in computers requires modelling the link between language and perception .	1-13	14-26	A common use of language is to refer to visually present objects .	Modelling it in computers requires modelling the link between language and perception .	1>2	elab-addition	elab-addition
P16-1115	14-26	27-44	Modelling it in computers requires modelling the link between language and perception .	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts ,	14-26	27-61	Modelling it in computers requires modelling the link between language and perception .	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	1>2	bg-goal	bg-goal
P16-1115	27-44	45-51	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts ,	and composes the meaning of a phrase	27-61	27-61	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	1<2	joint	joint
P16-1115	45-51	52-61	and composes the meaning of a phrase	through composition of the denotations of its component words .	27-61	27-61	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	1<2	manner-means	manner-means
P16-1115	27-44	62-80	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts ,	It was recently shown to perform well in a game-playing scenario with a small number of object types .	27-61	62-80	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	It was recently shown to perform well in a game-playing scenario with a small number of object types .	1<2	evaluation	evaluation
P16-1115	27-44	81-90	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts ,	We apply it to two large sets of real-world photographs	27-61	81-107	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available .	1<2	elab-addition	elab-addition
P16-1115	81-90	91-99	We apply it to two large sets of real-world photographs	that contain a much larger variety of object types	81-107	81-107	We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available .	We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available .	1<2	elab-addition	elab-addition
P16-1115	91-99	100-107	that contain a much larger variety of object types	and for which referring expressions are available .	81-107	81-107	We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available .	We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available .	1<2	joint	joint
P16-1115	108-113	120-126	Using a pre-trained convolutional neural network	and augmenting these with positional information ,	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1>2	progression	progression
P16-1115	108-113	114-119	Using a pre-trained convolutional neural network	to extract image region features ,	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1<2	enablement	enablement
P16-1115	120-126	129-145	and augmenting these with positional information ,	that the model achieves performance competitive with the state of the art in a reference resolution task	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1>2	manner-means	manner-means
P16-1115	127-128	129-145	we show	that the model achieves performance competitive with the state of the art in a reference resolution task	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1>2	attribution	attribution
P16-1115	27-44	129-145	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts ,	that the model achieves performance competitive with the state of the art in a reference resolution task	27-61	108-170	The `` words as classifiers '' model of grounded semantics views words as classifiers of perceptual contexts , and composes the meaning of a phrase through composition of the denotations of its component words .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1<2	evaluation	evaluation
P16-1115	146-149	150-157	( given expression ,	find bounding box of its referent ) ,	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1>2	condition	condition
P16-1115	129-145	150-157	that the model achieves performance competitive with the state of the art in a reference resolution task	find bounding box of its referent ) ,	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1<2	elab-addition	elab-addition
P16-1115	129-145	158-159,164-170	that the model achieves performance competitive with the state of the art in a reference resolution task	while , <*> being conceptually simpler and more flexible .	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1<2	joint	joint
P16-1115	160-163	164-170	as we argue ,	being conceptually simpler and more flexible .	108-170	108-170	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	Using a pre-trained convolutional neural network to extract image region features , and augmenting these with positional information , we show that the model achieves performance competitive with the state of the art in a reference resolution task ( given expression , find bounding box of its referent ) , while , as we argue , being conceptually simpler and more flexible .	1>2	attribution	attribution
P16-1116	1-10	91-102	Event extraction is a particularly challenging information extraction task ,	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	1-24	91-102	Event extraction is a particularly challenging information extraction task , which intends to identify and classify event triggers and arguments from raw text .	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	1>2	bg-goal	bg-goal
P16-1116	1-10	11-24	Event extraction is a particularly challenging information extraction task ,	which intends to identify and classify event triggers and arguments from raw text .	1-24	1-24	Event extraction is a particularly challenging information extraction task , which intends to identify and classify event triggers and arguments from raw text .	Event extraction is a particularly challenging information extraction task , which intends to identify and classify event triggers and arguments from raw text .	1<2	elab-addition	elab-addition
P16-1116	25-37	38-47	In recent works , when determining event types ( trigger classification ) ,	most of the works are either pattern-only or feature-only .	25-47	25-47	In recent works , when determining event types ( trigger classification ) , most of the works are either pattern-only or feature-only .	In recent works , when determining event types ( trigger classification ) , most of the works are either pattern-only or feature-only .	1>2	temporal	temporal
P16-1116	38-47	91-102	most of the works are either pattern-only or feature-only .	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	25-47	91-102	In recent works , when determining event types ( trigger classification ) , most of the works are either pattern-only or feature-only .	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	1>2	bg-compare	bg-compare
P16-1116	48-59	60-67	However , although patterns cannot cover all representations of an event ,	it is still a very important feature .	48-67	48-67	However , although patterns cannot cover all representations of an event , it is still a very important feature .	However , although patterns cannot cover all representations of an event , it is still a very important feature .	1>2	contrast	contrast
P16-1116	25-37	60-67	In recent works , when determining event types ( trigger classification ) ,	it is still a very important feature .	25-47	48-67	In recent works , when determining event types ( trigger classification ) , most of the works are either pattern-only or feature-only .	However , although patterns cannot cover all representations of an event , it is still a very important feature .	1<2	elab-addition	elab-addition
P16-1116	68-76	77-83	In addition , when identifying and classifying arguments ,	previous works consider each candidate argument separately	68-90	68-90	In addition , when identifying and classifying arguments , previous works consider each candidate argument separately while ignoring the relationship between arguments .	In addition , when identifying and classifying arguments , previous works consider each candidate argument separately while ignoring the relationship between arguments .	1>2	temporal	temporal
P16-1116	25-37	77-83	In recent works , when determining event types ( trigger classification ) ,	previous works consider each candidate argument separately	25-47	68-90	In recent works , when determining event types ( trigger classification ) , most of the works are either pattern-only or feature-only .	In addition , when identifying and classifying arguments , previous works consider each candidate argument separately while ignoring the relationship between arguments .	1<2	elab-addition	elab-addition
P16-1116	77-83	84-90	previous works consider each candidate argument separately	while ignoring the relationship between arguments .	68-90	68-90	In addition , when identifying and classifying arguments , previous works consider each candidate argument separately while ignoring the relationship between arguments .	In addition , when identifying and classifying arguments , previous works consider each candidate argument separately while ignoring the relationship between arguments .	1<2	joint	joint
P16-1116	103-110	111-127	Inspired by the progress in representation learning ,	we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification	103-141	103-141	Inspired by the progress in representation learning , we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced .	Inspired by the progress in representation learning , we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced .	1>2	elab-addition	elab-addition
P16-1116	91-102	111-127	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification	91-102	103-141	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	Inspired by the progress in representation learning , we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced .	1<2	elab-addition	elab-addition
P16-1116	111-127	128-141	we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification	so that the effect of patterns and other useful features can be balanced .	103-141	103-141	Inspired by the progress in representation learning , we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced .	Inspired by the progress in representation learning , we use trigger embedding , sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced .	1<2	cause	cause
P16-1116	91-102	142-149	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	In addition , RBPB uses a regularization method	91-102	142-158	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	In addition , RBPB uses a regularization method to take advantage of the relationship between arguments .	1<2	elab-addition	elab-addition
P16-1116	142-149	150-158	In addition , RBPB uses a regularization method	to take advantage of the relationship between arguments .	142-158	142-158	In addition , RBPB uses a regularization method to take advantage of the relationship between arguments .	In addition , RBPB uses a regularization method to take advantage of the relationship between arguments .	1<2	enablement	enablement
P16-1116	159-160	161-170	Experiments show	that we achieve results better than current state-of-art equivalents .	159-170	159-170	Experiments show that we achieve results better than current state-of-art equivalents .	Experiments show that we achieve results better than current state-of-art equivalents .	1>2	attribution	attribution
P16-1116	91-102	161-170	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	that we achieve results better than current state-of-art equivalents .	91-102	159-170	This paper proposes a Regularization-Based Pattern Balancing Method ( RBPB ) .	Experiments show that we achieve results better than current state-of-art equivalents .	1<2	evaluation	evaluation
P16-1117	1-15	16-22	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis	based on a neural network framework .	1-22	1-22	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis based on a neural network framework .	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis based on a neural network framework .	1<2	bg-general	bg-general
P16-1117	1-15	23-27	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis	Japanese PAS analysis is challenging	1-22	23-45	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis based on a neural network framework .	Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language , such as case disappearance and argument omission .	1<2	elab-addition	elab-addition
P16-1117	23-27	28-37	Japanese PAS analysis is challenging	due to the tangled characteristics of the Japanese language ,	23-45	23-45	Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language , such as case disappearance and argument omission .	Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language , such as case disappearance and argument omission .	1<2	exp-reason	exp-reason
P16-1117	28-37	38-45	due to the tangled characteristics of the Japanese language ,	such as case disappearance and argument omission .	23-45	23-45	Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language , such as case disappearance and argument omission .	Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language , such as case disappearance and argument omission .	1<2	elab-example	elab-example
P16-1117	46-50	51-60	To unravel this problem ,	we learn selectional preferences from a large raw corpus ,	46-82	46-82	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	1>2	enablement	enablement
P16-1117	23-27	51-60	Japanese PAS analysis is challenging	we learn selectional preferences from a large raw corpus ,	23-45	46-82	Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language , such as case disappearance and argument omission .	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	1<2	elab-addition	elab-addition
P16-1117	51-60	61-70	we learn selectional preferences from a large raw corpus ,	and incorporate them into a SOTA PAS analysis model ,	46-82	46-82	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	1<2	joint	joint
P16-1117	61-70	71-82	and incorporate them into a SOTA PAS analysis model ,	which considers the consistency of all PASs in a given sentence .	46-82	46-82	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	To unravel this problem , we learn selectional preferences from a large raw corpus , and incorporate them into a SOTA PAS analysis model , which considers the consistency of all PASs in a given sentence .	1<2	elab-addition	elab-addition
P16-1117	83-84	85-97	We demonstrate	that the proposed PAS analysis model significantly outperforms the base SOTA system .	83-97	83-97	We demonstrate that the proposed PAS analysis model significantly outperforms the base SOTA system .	We demonstrate that the proposed PAS analysis model significantly outperforms the base SOTA system .	1>2	attribution	attribution
P16-1117	1-15	85-97	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis	that the proposed PAS analysis model significantly outperforms the base SOTA system .	1-22	83-97	This paper presents a novel model for Japanese predicate argument structure ( PAS ) analysis based on a neural network framework .	We demonstrate that the proposed PAS analysis model significantly outperforms the base SOTA system .	1<2	evaluation	evaluation
P16-1118	1-23	24-53	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	To that end , we first create ( for experimental purposes ) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system , ENT ,	1-23	24-66	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	To that end , we first create ( for experimental purposes ) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system , ENT , that is effective ( out of the box ) on two domains .	1<2	elab-process_step	elab-process_step
P16-1118	24-53	54-66	To that end , we first create ( for experimental purposes ) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system , ENT ,	that is effective ( out of the box ) on two domains .	24-66	24-66	To that end , we first create ( for experimental purposes ) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system , ENT , that is effective ( out of the box ) on two domains .	To that end , we first create ( for experimental purposes ) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system , ENT , that is effective ( out of the box ) on two domains .	1<2	elab-addition	elab-addition
P16-1118	1-23	67-74	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	We then explore self-training and active learning strategies	1-23	67-82	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	We then explore self-training and active learning strategies to address the lack of labeled data .	1<2	elab-process_step	elab-process_step
P16-1118	67-74	75-82	We then explore self-training and active learning strategies	to address the lack of labeled data .	67-82	67-82	We then explore self-training and active learning strategies to address the lack of labeled data .	We then explore self-training and active learning strategies to address the lack of labeled data .	1<2	enablement	enablement
P16-1118	83-85	86-90	With self-training ,	we successfully exploit unlabeled data	83-111	83-111	With self-training , we successfully exploit unlabeled data to improve over ENT by 15 % F-score on the newswire domain , and 13 % F-score on clinical data .	With self-training , we successfully exploit unlabeled data to improve over ENT by 15 % F-score on the newswire domain , and 13 % F-score on clinical data .	1>2	manner-means	manner-means
P16-1118	1-23	86-90	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	we successfully exploit unlabeled data	1-23	83-111	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	With self-training , we successfully exploit unlabeled data to improve over ENT by 15 % F-score on the newswire domain , and 13 % F-score on clinical data .	1<2	evaluation	evaluation
P16-1118	86-90	91-111	we successfully exploit unlabeled data	to improve over ENT by 15 % F-score on the newswire domain , and 13 % F-score on clinical data .	83-111	83-111	With self-training , we successfully exploit unlabeled data to improve over ENT by 15 % F-score on the newswire domain , and 13 % F-score on clinical data .	With self-training , we successfully exploit unlabeled data to improve over ENT by 15 % F-score on the newswire domain , and 13 % F-score on clinical data .	1<2	enablement	enablement
P16-1118	112-121	122-131	On the other hand , our active learning experiments demonstrate	that we can match ( and even beat ) ENT	112-157	112-157	On the other hand , our active learning experiments demonstrate that we can match ( and even beat ) ENT using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .	On the other hand , our active learning experiments demonstrate that we can match ( and even beat ) ENT using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .	1>2	attribution	attribution
P16-1118	1-23	122-131	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	that we can match ( and even beat ) ENT	1-23	112-157	We seek to address the lack of labeled data ( and high cost of annotation ) for textual entailment in some domains .	On the other hand , our active learning experiments demonstrate that we can match ( and even beat ) ENT using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .	1<2	evaluation	evaluation
P16-1118	122-131	132-157	that we can match ( and even beat ) ENT	using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .	112-157	112-157	On the other hand , our active learning experiments demonstrate that we can match ( and even beat ) ENT using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .	On the other hand , our active learning experiments demonstrate that we can match ( and even beat ) ENT using only 6.6 % of the training data in the clinical domain , and only 5.8 % of the training data in the newswire domain .	1<2	manner-means	manner-means
P16-1119	1-18	19-33	The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics .	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments ,	1-18	19-42	The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics .	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments , which were found beneficial by several recent works .	1>2	elab-addition	elab-addition
P16-1119	19-33	71-88	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments ,	In this work we devise a novel crowdsourcing annotation methodology , and an accompanying large scale corpus .	19-42	71-88	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments , which were found beneficial by several recent works .	In this work we devise a novel crowdsourcing annotation methodology , and an accompanying large scale corpus .	1>2	bg-goal	bg-goal
P16-1119	19-33	34-42	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments ,	which were found beneficial by several recent works .	19-42	19-42	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments , which were found beneficial by several recent works .	Automatically identifying non-restrictive modifiers can provide NLP applications with shorter , more salient arguments , which were found beneficial by several recent works .	1<2	elab-addition	elab-addition
P16-1119	43-46	47-55	While previous work showed	that restrictiveness can be annotated with high agreement ,	43-70	43-70	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	1>2	attribution	attribution
P16-1119	47-55	56-62	that restrictiveness can be annotated with high agreement ,	no large scale corpus was created ,	43-70	43-70	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	1>2	contrast	contrast
P16-1119	56-62	71-88	no large scale corpus was created ,	In this work we devise a novel crowdsourcing annotation methodology , and an accompanying large scale corpus .	43-70	71-88	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	In this work we devise a novel crowdsourcing annotation methodology , and an accompanying large scale corpus .	1>2	bg-compare	bg-compare
P16-1119	56-62	63-70	no large scale corpus was created ,	hindering the development of suitable classification algorithms .	43-70	43-70	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	While previous work showed that restrictiveness can be annotated with high agreement , no large scale corpus was created , hindering the development of suitable classification algorithms .	1<2	cause	cause
P16-1119	71-88	89-96	In this work we devise a novel crowdsourcing annotation methodology , and an accompanying large scale corpus .	Then , we present a robust automated system	71-88	89-107	In this work we devise a novel crowdsourcing annotation methodology , and an accompanying large scale corpus .	Then , we present a robust automated system which identifies non-restrictive modifiers , notably improving over prior methods .	1<2	elab-addition	elab-addition
P16-1119	89-96	97-101	Then , we present a robust automated system	which identifies non-restrictive modifiers ,	89-107	89-107	Then , we present a robust automated system which identifies non-restrictive modifiers , notably improving over prior methods .	Then , we present a robust automated system which identifies non-restrictive modifiers , notably improving over prior methods .	1<2	elab-addition	elab-addition
P16-1119	89-96	102-107	Then , we present a robust automated system	notably improving over prior methods .	89-107	89-107	Then , we present a robust automated system which identifies non-restrictive modifiers , notably improving over prior methods .	Then , we present a robust automated system which identifies non-restrictive modifiers , notably improving over prior methods .	1<2	elab-addition	elab-addition
P16-1120	1-12	13-16	This study proposes the bilingual segmented topic model ( BiSTM ) ,	which hierarchically models documents	1-30	1-30	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	1<2	elab-addition	elab-addition
P16-1120	13-16	17-26	which hierarchically models documents	by treating each document as a set of segments ,	1-30	1-30	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	1<2	manner-means	manner-means
P16-1120	17-26	27-30	by treating each document as a set of segments ,	e.g. , sections .	1-30	1-30	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	1<2	elab-example	elab-example
P16-1120	31-36,60-67	68-80	While previous bilingual topic models , <*> consider only cross-lingual alignments between entire documents ,	the proposed model considers cross-lingual alignments between segments in addition to document-level alignments	31-90	31-90	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	1>2	contrast	contrast
P16-1120	31-36,60-67	37-59	While previous bilingual topic models , <*> consider only cross-lingual alignments between entire documents ,	such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) ,	31-90	31-90	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	1<2	elab-example	elab-example
P16-1120	1-12	68-80	This study proposes the bilingual segmented topic model ( BiSTM ) ,	the proposed model considers cross-lingual alignments between segments in addition to document-level alignments	1-30	31-90	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	1<2	elab-addition	elab-addition
P16-1120	68-80	81-90	the proposed model considers cross-lingual alignments between segments in addition to document-level alignments	and assigns the same topic distribution to aligned segments .	31-90	31-90	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	While previous bilingual topic models , such as bilingual latent Dirichlet allocation ( BiLDA ) ( Mimno et al. , 2009 ; Ni et al. , 2009 ) , consider only cross-lingual alignments between entire documents , the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments .	1<2	joint	joint
P16-1120	1-12	91-96	This study proposes the bilingual segmented topic model ( BiSTM ) ,	This study also presents a method	1-30	91-119	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	This study also presents a method for simultaneously inferring latent topics and segmentation boundaries , incorporating unsupervised topic segmentation ( Du et al. , 2013 ) into BiSTM .	1<2	elab-addition	elab-addition
P16-1120	91-96	97-105	This study also presents a method	for simultaneously inferring latent topics and segmentation boundaries ,	91-119	91-119	This study also presents a method for simultaneously inferring latent topics and segmentation boundaries , incorporating unsupervised topic segmentation ( Du et al. , 2013 ) into BiSTM .	This study also presents a method for simultaneously inferring latent topics and segmentation boundaries , incorporating unsupervised topic segmentation ( Du et al. , 2013 ) into BiSTM .	1<2	elab-addition	elab-addition
P16-1120	97-105	106-119	for simultaneously inferring latent topics and segmentation boundaries ,	incorporating unsupervised topic segmentation ( Du et al. , 2013 ) into BiSTM .	91-119	91-119	This study also presents a method for simultaneously inferring latent topics and segmentation boundaries , incorporating unsupervised topic segmentation ( Du et al. , 2013 ) into BiSTM .	This study also presents a method for simultaneously inferring latent topics and segmentation boundaries , incorporating unsupervised topic segmentation ( Du et al. , 2013 ) into BiSTM .	1<2	manner-means	manner-means
P16-1120	120-122	123-133	Experimental results show	that the proposed model significantly outperforms BiLDA in terms of perplexity	120-149	120-149	Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction ( up to +0.083 extraction accuracy ) .	Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction ( up to +0.083 extraction accuracy ) .	1>2	attribution	attribution
P16-1120	1-12	123-133	This study proposes the bilingual segmented topic model ( BiSTM ) ,	that the proposed model significantly outperforms BiLDA in terms of perplexity	1-30	120-149	This study proposes the bilingual segmented topic model ( BiSTM ) , which hierarchically models documents by treating each document as a set of segments , e.g. , sections .	Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction ( up to +0.083 extraction accuracy ) .	1<2	evaluation	evaluation
P16-1120	123-133	134-149	that the proposed model significantly outperforms BiLDA in terms of perplexity	and demonstrates improved performance in translation pair extraction ( up to +0.083 extraction accuracy ) .	120-149	120-149	Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction ( up to +0.083 extraction accuracy ) .	Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction ( up to +0.083 extraction accuracy ) .	1<2	joint	joint
P16-1121	1-20	21-24	This paper connects a vector-based composition model to a formal semantics , the Dependency-based Compositional Semantics ( DCS ) .	We show theoretical evidence	1-20	21-38	This paper connects a vector-based composition model to a formal semantics , the Dependency-based Compositional Semantics ( DCS ) .	We show theoretical evidence that the vector compositions in our model conform to the logic of DCS .	1<2	elab-aspect	elab-aspect
P16-1121	21-24	25-38	We show theoretical evidence	that the vector compositions in our model conform to the logic of DCS .	21-38	21-38	We show theoretical evidence that the vector compositions in our model conform to the logic of DCS .	We show theoretical evidence that the vector compositions in our model conform to the logic of DCS .	1<2	elab-addition	elab-addition
P16-1121	39-42	43-49	Experimentally , we show	that vector-based composition brings a strong ability	39-88	39-88	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	1>2	attribution	attribution
P16-1121	1-20	43-49	This paper connects a vector-based composition model to a formal semantics , the Dependency-based Compositional Semantics ( DCS ) .	that vector-based composition brings a strong ability	1-20	39-88	This paper connects a vector-based composition model to a formal semantics , the Dependency-based Compositional Semantics ( DCS ) .	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	1<2	elab-aspect	elab-aspect
P16-1121	43-49	50-57	that vector-based composition brings a strong ability	to calculate similar phrases as similar vectors ,	39-88	39-88	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	1<2	elab-addition	elab-addition
P16-1121	43-49	58-72	that vector-based composition brings a strong ability	achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ;	39-88	39-88	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	1<2	elab-addition	elab-addition
P16-1121	43-49	73-82	that vector-based composition brings a strong ability	meanwhile , DCS can guide building vectors for structured queries	39-88	39-88	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	1<2	joint	joint
P16-1121	73-82	83-88	meanwhile , DCS can guide building vectors for structured queries	that can be directly executed .	39-88	39-88	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification ; meanwhile , DCS can guide building vectors for structured queries that can be directly executed .	1<2	elab-addition	elab-addition
P16-1121	1-20	89-96	This paper connects a vector-based composition model to a formal semantics , the Dependency-based Compositional Semantics ( DCS ) .	We evaluate this utility on sentence completion task	1-20	89-102	This paper connects a vector-based composition model to a formal semantics , the Dependency-based Compositional Semantics ( DCS ) .	We evaluate this utility on sentence completion task and report a new state-of-the-art .	1<2	evaluation	evaluation
P16-1121	89-96	97-102	We evaluate this utility on sentence completion task	and report a new state-of-the-art .	89-102	89-102	We evaluate this utility on sentence completion task and report a new state-of-the-art .	We evaluate this utility on sentence completion task and report a new state-of-the-art .	1<2	joint	joint
P16-1122	1-8	43-50	Attention based recurrent neural networks have shown advantages	external attention information was added to hidden representations	1-33	34-57	Attention based recurrent neural networks have shown advantages in representing natural language sentences ( Hermann et al. , 2015 ; Rocktaschel et al. , 2015 ; Tan et al. , 2015 ) .	Based on recurrent neural networks ( RNN ) , external attention information was added to hidden representations to get an attentive sentence representation .	1>2	elab-addition	elab-addition
P16-1122	1-8	9-33	Attention based recurrent neural networks have shown advantages	in representing natural language sentences ( Hermann et al. , 2015 ; Rocktaschel et al. , 2015 ; Tan et al. , 2015 ) .	1-33	1-33	Attention based recurrent neural networks have shown advantages in representing natural language sentences ( Hermann et al. , 2015 ; Rocktaschel et al. , 2015 ; Tan et al. , 2015 ) .	Attention based recurrent neural networks have shown advantages in representing natural language sentences ( Hermann et al. , 2015 ; Rocktaschel et al. , 2015 ; Tan et al. , 2015 ) .	1<2	elab-addition	elab-addition
P16-1122	34-42	43-50	Based on recurrent neural networks ( RNN ) ,	external attention information was added to hidden representations	34-57	34-57	Based on recurrent neural networks ( RNN ) , external attention information was added to hidden representations to get an attentive sentence representation .	Based on recurrent neural networks ( RNN ) , external attention information was added to hidden representations to get an attentive sentence representation .	1>2	bg-general	bg-general
P16-1122	43-50	65-74	external attention information was added to hidden representations	the attention mechanism under RNN is not well studied .	34-57	58-74	Based on recurrent neural networks ( RNN ) , external attention information was added to hidden representations to get an attentive sentence representation .	Despite the improvement over nonattentive models , the attention mechanism under RNN is not well studied .	1>2	elab-addition	elab-addition
P16-1122	43-50	51-57	external attention information was added to hidden representations	to get an attentive sentence representation .	34-57	34-57	Based on recurrent neural networks ( RNN ) , external attention information was added to hidden representations to get an attentive sentence representation .	Based on recurrent neural networks ( RNN ) , external attention information was added to hidden representations to get an attentive sentence representation .	1<2	enablement	enablement
P16-1122	58-64	65-74	Despite the improvement over nonattentive models ,	the attention mechanism under RNN is not well studied .	58-74	58-74	Despite the improvement over nonattentive models , the attention mechanism under RNN is not well studied .	Despite the improvement over nonattentive models , the attention mechanism under RNN is not well studied .	1>2	contrast	contrast
P16-1122	65-74	75-92	the attention mechanism under RNN is not well studied .	In this work , we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively .	58-74	75-92	Despite the improvement over nonattentive models , the attention mechanism under RNN is not well studied .	In this work , we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively .	1>2	bg-compare	bg-compare
P16-1122	75-92	93-99	In this work , we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively .	Then we present three new RNN models	75-92	93-124	In this work , we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively .	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	1<2	elab-addition	elab-addition
P16-1122	93-99	100-108	Then we present three new RNN models	that add attention information before RNN hidden representation ,	93-124	93-124	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	1<2	elab-addition	elab-addition
P16-1122	100-108	109-114	that add attention information before RNN hidden representation ,	which shows advantage in representing sentence	93-124	93-124	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	1<2	elab-addition	elab-addition
P16-1122	109-114	115-124	which shows advantage in representing sentence	and achieves new state-of-art results in answer selection task .	93-124	93-124	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task .	1<2	joint	joint
P16-1123	1-11	20-31	Relation classification is a crucial ingredient in numerous information extraction systems	We propose a novel convolutional neural network architecture for this task ,	1-19	20-47	Relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text .	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	1>2	bg-goal	bg-goal
P16-1123	1-11	12-19	Relation classification is a crucial ingredient in numerous information extraction systems	seeking to mine structured facts from text .	1-19	1-19	Relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text .	Relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text .	1<2	elab-addition	elab-addition
P16-1123	20-31	32-37	We propose a novel convolutional neural network architecture for this task ,	relying on two levels of attention	20-47	20-47	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	1<2	elab-addition	elab-addition
P16-1123	32-37	38-47	relying on two levels of attention	in order to better discern patterns in heterogeneous contexts .	20-47	20-47	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	1<2	enablement	enablement
P16-1123	20-31	48-57	We propose a novel convolutional neural network architecture for this task ,	This architecture enables end-to-end learning from task-specific labeled data ,	20-47	48-69	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	This architecture enables end-to-end learning from task-specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .	1<2	elab-addition	elab-addition
P16-1123	48-57	58-63	This architecture enables end-to-end learning from task-specific labeled data ,	forgoing the need for external knowledge	48-69	48-69	This architecture enables end-to-end learning from task-specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .	This architecture enables end-to-end learning from task-specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .	1<2	elab-addition	elab-addition
P16-1123	58-63	64-69	forgoing the need for external knowledge	such as explicit dependency structures .	48-69	48-69	This architecture enables end-to-end learning from task-specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .	This architecture enables end-to-end learning from task-specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .	1<2	elab-example	elab-example
P16-1123	70-71	72-79	Experiments show	that our model outperforms previous state-of-the-art methods ,	70-90	70-90	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	1>2	attribution	attribution
P16-1123	20-31	72-79	We propose a novel convolutional neural network architecture for this task ,	that our model outperforms previous state-of-the-art methods ,	20-47	70-90	We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	1<2	evaluation	evaluation
P16-1123	72-79	80-81	that our model outperforms previous state-of-the-art methods ,	including those	70-90	70-90	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	1<2	elab-example	elab-example
P16-1123	80-81	82-90	including those	relying on much richer forms of prior knowledge .	70-90	70-90	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	Experiments show that our model outperforms previous state-of-the-art methods , including those relying on much richer forms of prior knowledge .	1<2	elab-addition	elab-addition
P16-1124	1-10	18-35	Knowledge bases ( KBs ) are often greatly incomplete ,	The path ranking algorithm ( PRA ) is one of the most promising approaches to this task .	1-17	18-35	Knowledge bases ( KBs ) are often greatly incomplete , necessitating a demand for KB completion .	The path ranking algorithm ( PRA ) is one of the most promising approaches to this task .	1>2	elab-addition	elab-addition
P16-1124	1-10	11-17	Knowledge bases ( KBs ) are often greatly incomplete ,	necessitating a demand for KB completion .	1-17	1-17	Knowledge bases ( KBs ) are often greatly incomplete , necessitating a demand for KB completion .	Knowledge bases ( KBs ) are often greatly incomplete , necessitating a demand for KB completion .	1<2	elab-addition	elab-addition
P16-1124	18-35	81-91	The path ranking algorithm ( PRA ) is one of the most promising approaches to this task .	This paper proposes a novel multi-task learning framework for PRA ,	18-35	81-100	The path ranking algorithm ( PRA ) is one of the most promising approaches to this task .	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	1>2	bg-goal	bg-goal
P16-1124	36-46	61-68	Previous work on PRA usually follows a single-task learning paradigm ,	It ignores meaningful associations among certain relations ,	36-60	61-80	Previous work on PRA usually follows a single-task learning paradigm , building a prediction model for each relation independently with its own training data .	It ignores meaningful associations among certain relations , and might not get enough training data for less frequent relations .	1>2	contrast	contrast
P16-1124	36-46	47-60	Previous work on PRA usually follows a single-task learning paradigm ,	building a prediction model for each relation independently with its own training data .	36-60	36-60	Previous work on PRA usually follows a single-task learning paradigm , building a prediction model for each relation independently with its own training data .	Previous work on PRA usually follows a single-task learning paradigm , building a prediction model for each relation independently with its own training data .	1<2	elab-addition	elab-addition
P16-1124	61-68	81-91	It ignores meaningful associations among certain relations ,	This paper proposes a novel multi-task learning framework for PRA ,	61-80	81-100	It ignores meaningful associations among certain relations , and might not get enough training data for less frequent relations .	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	1>2	bg-compare	bg-compare
P16-1124	61-68	69-80	It ignores meaningful associations among certain relations ,	and might not get enough training data for less frequent relations .	61-80	61-80	It ignores meaningful associations among certain relations , and might not get enough training data for less frequent relations .	It ignores meaningful associations among certain relations , and might not get enough training data for less frequent relations .	1<2	joint	joint
P16-1124	81-91	92-100	This paper proposes a novel multi-task learning framework for PRA ,	referred to as coupled PRA ( CPRA ) .	81-100	81-100	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	1<2	elab-addition	elab-addition
P16-1124	81-91	101-107	This paper proposes a novel multi-task learning framework for PRA ,	It first devises an agglomerative clustering strategy	81-100	101-135	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	1<2	elab-addition	elab-addition
P16-1124	101-107	108-111	It first devises an agglomerative clustering strategy	to automatically discover relations	101-135	101-135	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	1<2	enablement	enablement
P16-1124	108-111	112-119	to automatically discover relations	that are highly correlated to each other ,	101-135	101-135	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	1<2	elab-addition	elab-addition
P16-1124	101-107	120-126	It first devises an agglomerative clustering strategy	and then employs a multi-task learning strategy	101-135	101-135	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	1<2	progression	progression
P16-1124	120-126	127-135	and then employs a multi-task learning strategy	to effectively couple the prediction of such relations .	101-135	101-135	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	1<2	enablement	enablement
P16-1124	101-107	136-144	It first devises an agglomerative clustering strategy	As such , CPRA takes into account relation association	101-135	136-152	It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other , and then employs a multi-task learning strategy to effectively couple the prediction of such relations .	As such , CPRA takes into account relation association and enables implicit data sharing among them .	1<2	elab-addition	elab-addition
P16-1124	136-144	145-152	As such , CPRA takes into account relation association	and enables implicit data sharing among them .	136-152	136-152	As such , CPRA takes into account relation association and enables implicit data sharing among them .	As such , CPRA takes into account relation association and enables implicit data sharing among them .	1<2	joint	joint
P16-1124	153-159	167-173	We empirically evaluate CPRA on benchmark data	that CPRA can effectively identify coherent clusters	153-163	164-180	We empirically evaluate CPRA on benchmark data created from Freebase .	Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated .	1>2	result	result
P16-1124	153-159	160-163	We empirically evaluate CPRA on benchmark data	created from Freebase .	153-163	153-163	We empirically evaluate CPRA on benchmark data created from Freebase .	We empirically evaluate CPRA on benchmark data created from Freebase .	1<2	elab-addition	elab-addition
P16-1124	164-166	167-173	Experimental results show	that CPRA can effectively identify coherent clusters	164-180	164-180	Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated .	Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated .	1>2	attribution	attribution
P16-1124	81-91	167-173	This paper proposes a novel multi-task learning framework for PRA ,	that CPRA can effectively identify coherent clusters	81-100	164-180	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated .	1<2	evaluation	evaluation
P16-1124	167-173	174-180	that CPRA can effectively identify coherent clusters	in which relations are highly correlated .	164-180	164-180	Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated .	Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated .	1<2	elab-addition	elab-addition
P16-1124	181-186	187-201	By further coupling such relations ,	CPRA significantly outperforms PRA , in terms of both predictive accuracy and model interpretability .	181-201	181-201	By further coupling such relations , CPRA significantly outperforms PRA , in terms of both predictive accuracy and model interpretability .	By further coupling such relations , CPRA significantly outperforms PRA , in terms of both predictive accuracy and model interpretability .	1>2	manner-means	manner-means
P16-1124	81-91	187-201	This paper proposes a novel multi-task learning framework for PRA ,	CPRA significantly outperforms PRA , in terms of both predictive accuracy and model interpretability .	81-100	181-201	This paper proposes a novel multi-task learning framework for PRA , referred to as coupled PRA ( CPRA ) .	By further coupling such relations , CPRA significantly outperforms PRA , in terms of both predictive accuracy and model interpretability .	1<2	evaluation	evaluation
P16-1125	1-9	10-18	In this work , we propose a novel method	to incorporate corpus-level discourse information into language modelling .	1-18	1-18	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	1<2	enablement	enablement
P16-1125	1-9	19-25	In this work , we propose a novel method	We call this larger-context language model .	1-18	19-25	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	We call this larger-context language model .	1<2	elab-addition	elab-addition
P16-1125	1-9	26-36	In this work , we propose a novel method	We introduce a late fusion approach to a recurrent language model	1-18	26-63	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	We introduce a late fusion approach to a recurrent language model based on long short term memory units ( LSTM ) , which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other .	1<2	elab-addition	elab-addition
P16-1125	26-36	37-47	We introduce a late fusion approach to a recurrent language model	based on long short term memory units ( LSTM ) ,	26-63	26-63	We introduce a late fusion approach to a recurrent language model based on long short term memory units ( LSTM ) , which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other .	We introduce a late fusion approach to a recurrent language model based on long short term memory units ( LSTM ) , which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other .	1<2	bg-general	bg-general
P16-1125	37-47	48-63	based on long short term memory units ( LSTM ) ,	which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other .	26-63	26-63	We introduce a late fusion approach to a recurrent language model based on long short term memory units ( LSTM ) , which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other .	We introduce a late fusion approach to a recurrent language model based on long short term memory units ( LSTM ) , which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other .	1<2	elab-addition	elab-addition
P16-1125	64-81	84-91	Through the evaluation on four corpora ( IMDB , BBC , Penn TreeBank , and Fil9 ) ,	that the proposed model improves perplexity significantly .	64-91	64-91	Through the evaluation on four corpora ( IMDB , BBC , Penn TreeBank , and Fil9 ) , we demonstrate that the proposed model improves perplexity significantly .	Through the evaluation on four corpora ( IMDB , BBC , Penn TreeBank , and Fil9 ) , we demonstrate that the proposed model improves perplexity significantly .	1>2	manner-means	manner-means
P16-1125	82-83	84-91	we demonstrate	that the proposed model improves perplexity significantly .	64-91	64-91	Through the evaluation on four corpora ( IMDB , BBC , Penn TreeBank , and Fil9 ) , we demonstrate that the proposed model improves perplexity significantly .	Through the evaluation on four corpora ( IMDB , BBC , Penn TreeBank , and Fil9 ) , we demonstrate that the proposed model improves perplexity significantly .	1>2	attribution	attribution
P16-1125	1-9	84-91	In this work , we propose a novel method	that the proposed model improves perplexity significantly .	1-18	64-91	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	Through the evaluation on four corpora ( IMDB , BBC , Penn TreeBank , and Fil9 ) , we demonstrate that the proposed model improves perplexity significantly .	1<2	evaluation	evaluation
P16-1125	92-100	108-109	In the experiments , we evaluate the proposed approach	and observe	92-128	92-128	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	1>2	progression	progression
P16-1125	92-100	101-107	In the experiments , we evaluate the proposed approach	while varying the number of context sentences	92-128	92-128	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	1<2	condition	condition
P16-1125	108-109	110-120	and observe	that the proposed late fusion is superior to the usual way	92-128	92-128	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	1>2	attribution	attribution
P16-1125	1-9	110-120	In this work , we propose a novel method	that the proposed late fusion is superior to the usual way	1-18	92-128	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	1<2	evaluation	evaluation
P16-1125	110-120	121-128	that the proposed late fusion is superior to the usual way	of incorporating additional inputs to the LSTM .	92-128	92-128	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	In the experiments , we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM .	1<2	elab-addition	elab-addition
P16-1125	129-136	139-142,150-159	By analyzing the trained larger-context language model ,	that content words , <*> benefit most from an increasing number of context sentences .	129-159	129-159	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	1>2	manner-means	manner-means
P16-1125	137-138	139-142,150-159	we discover	that content words , <*> benefit most from an increasing number of context sentences .	129-159	129-159	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	1>2	attribution	attribution
P16-1125	1-9	139-142,150-159	In this work , we propose a novel method	that content words , <*> benefit most from an increasing number of context sentences .	1-18	129-159	In this work , we propose a novel method to incorporate corpus-level discourse information into language modelling .	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	1<2	elab-addition	elab-addition
P16-1125	139-142,150-159	143-149	that content words , <*> benefit most from an increasing number of context sentences .	including nouns , adjectives and verbs ,	129-159	129-159	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	1<2	elab-enumember	elab-enumember
P16-1125	160-162	163-171	This analysis suggests	that larger-context language model improves the unconditional language model	160-183	160-183	This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily .	This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily .	1>2	attribution	attribution
P16-1125	139-142,150-159	163-171	that content words , <*> benefit most from an increasing number of context sentences .	that larger-context language model improves the unconditional language model	129-159	160-183	By analyzing the trained larger-context language model , we discover that content words , including nouns , adjectives and verbs , benefit most from an increasing number of context sentences .	This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily .	1<2	elab-addition	elab-addition
P16-1125	163-171	172-183	that larger-context language model improves the unconditional language model	by capturing the theme of a document better and more easily .	160-183	160-183	This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily .	This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily .	1<2	manner-means	manner-means
P16-1126	1-10	23-33	Website privacy policies are often ignored by Internet users ,	However , the significance of privacy policies greatly exceeds the attention	1-22	23-67	Website privacy policies are often ignored by Internet users , because these documents tend to be long and difficult to understand .	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	1>2	contrast	contrast
P16-1126	1-10	11-22	Website privacy policies are often ignored by Internet users ,	because these documents tend to be long and difficult to understand .	1-22	1-22	Website privacy policies are often ignored by Internet users , because these documents tend to be long and difficult to understand .	Website privacy policies are often ignored by Internet users , because these documents tend to be long and difficult to understand .	1<2	exp-reason	exp-reason
P16-1126	23-33	128-148	However , the significance of privacy policies greatly exceeds the attention	we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	23-67	123-148	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	1>2	bg-compare	bg-compare
P16-1126	23-33	34-37	However , the significance of privacy policies greatly exceeds the attention	paid to them :	23-67	23-67	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	1<2	elab-addition	elab-addition
P16-1126	23-33	38-50	However , the significance of privacy policies greatly exceeds the attention	these documents are binding legal agreements between website operators and their users ,	23-67	23-67	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	1<2	elab-addition	elab-addition
P16-1126	38-50	51-67	these documents are binding legal agreements between website operators and their users ,	and their opaqueness is a challenge not only to Internet users but also to policy regulators .	23-67	23-67	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	However , the significance of privacy policies greatly exceeds the attention paid to them : these documents are binding legal agreements between website operators and their users , and their opaqueness is a challenge not only to Internet users but also to policy regulators .	1<2	joint	joint
P16-1126	68-89	104-113	One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text ,	However , there has been a relative dearth of datasets	68-103	104-122	One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text , using a combination of crowdsourcing , natural language processing , and machine learning .	However , there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies .	1>2	contrast	contrast
P16-1126	68-89	90-103	One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text ,	using a combination of crowdsourcing , natural language processing , and machine learning .	68-103	68-103	One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text , using a combination of crowdsourcing , natural language processing , and machine learning .	One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text , using a combination of crowdsourcing , natural language processing , and machine learning .	1<2	manner-means	manner-means
P16-1126	104-113	128-148	However , there has been a relative dearth of datasets	we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	104-122	123-148	However , there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies .	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	1>2	bg-compare	bg-compare
P16-1126	104-113	114-122	However , there has been a relative dearth of datasets	appropriate for identifying data practices in privacy policies .	104-122	104-122	However , there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies .	However , there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies .	1<2	elab-addition	elab-addition
P16-1126	123-127	128-148	To remedy this problem ,	we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	123-148	123-148	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	1>2	enablement	enablement
P16-1126	128-148	149-152	we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	We describe the process	123-148	149-166	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data .	1<2	elab-process_step	elab-process_step
P16-1126	149-152	153-161	We describe the process	of using skilled annotators and a purpose-built annotation tool	149-166	149-166	We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data .	We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data .	1<2	elab-addition	elab-addition
P16-1126	153-161	162-166	of using skilled annotators and a purpose-built annotation tool	to produce the data .	149-166	149-166	We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data .	We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data .	1<2	enablement	enablement
P16-1126	128-148	167-169	we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	We provide findings	123-148	167-185	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	We provide findings based on a census of the annotations and show results toward automating the annotation procedure .	1<2	elab-process_step	elab-process_step
P16-1126	167-169	170-176	We provide findings	based on a census of the annotations	167-185	167-185	We provide findings based on a census of the annotations and show results toward automating the annotation procedure .	We provide findings based on a census of the annotations and show results toward automating the annotation procedure .	1<2	bg-general	bg-general
P16-1126	167-169	177-185	We provide findings	and show results toward automating the annotation procedure .	167-185	167-185	We provide findings based on a census of the annotations and show results toward automating the annotation procedure .	We provide findings based on a census of the annotations and show results toward automating the annotation procedure .	1<2	joint	joint
P16-1126	128-148	186-196	we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	Finally , we describe challenges and opportunities for the research community	123-148	186-210	To remedy this problem , we introduce a corpus of 115 privacy policies ( 267K words ) with manual annotations for 23K fine-grained data practices .	Finally , we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies .	1<2	elab-process_step	elab-process_step
P16-1126	186-196	197-200	Finally , we describe challenges and opportunities for the research community	to use this corpus	186-210	186-210	Finally , we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies .	Finally , we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies .	1<2	enablement	enablement
P16-1126	197-200	201-210	to use this corpus	to advance research in both privacy and language technologies .	186-210	186-210	Finally , we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies .	Finally , we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies .	1<2	enablement	enablement
P16-1127	1-7	8-13	We propose an approach for semantic parsing	that uses a recurrent neural network	1-29	1-29	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	1<2	elab-addition	elab-addition
P16-1127	8-13	14-29	that uses a recurrent neural network	to map a natural language question into a logical form representation of a KB query .	1-29	1-29	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	1<2	enablement	enablement
P16-1127	30-42	43-47,56-61	Building on recent work by ( Wang et al. , 2015 ) ,	the interpretable logical forms , <*> are enumerated by an underlying grammar	30-69	30-69	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	1>2	bg-general	bg-general
P16-1127	1-7	43-47,56-61	We propose an approach for semantic parsing	the interpretable logical forms , <*> are enumerated by an underlying grammar	1-29	30-69	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	1<2	elab-addition	elab-addition
P16-1127	43-47,56-61	48-51	the interpretable logical forms , <*> are enumerated by an underlying grammar	which are structured objects	30-69	30-69	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	1<2	elab-definition	elab-definition
P16-1127	48-51	52-55	which are structured objects	obeying certain constraints ,	30-69	30-69	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	1<2	elab-addition	elab-addition
P16-1127	56-61	62-69	are enumerated by an underlying grammar	and are paired with their canonical realizations .	30-69	30-69	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	Building on recent work by ( Wang et al. , 2015 ) , the interpretable logical forms , which are structured objects obeying certain constraints , are enumerated by an underlying grammar and are paired with their canonical realizations .	1<2	joint	joint
P16-1127	70-76	77-84	In order to use sequence prediction ,	we need to sequentialize these logical forms .	70-84	70-84	In order to use sequence prediction , we need to sequentialize these logical forms .	In order to use sequence prediction , we need to sequentialize these logical forms .	1>2	enablement	enablement
P16-1127	1-7	77-84	We propose an approach for semantic parsing	we need to sequentialize these logical forms .	1-29	70-84	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	In order to use sequence prediction , we need to sequentialize these logical forms .	1<2	elab-addition	elab-addition
P16-1127	77-84	85-89	we need to sequentialize these logical forms .	We compare three sequentializations :	70-84	85-118	In order to use sequence prediction , we need to sequentialize these logical forms .	We compare three sequentializations : a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence consisting of derivation steps relative to the underlying grammar .	1<2	elab-addition	elab-addition
P16-1127	85-89	90-108	We compare three sequentializations :	a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence	85-118	85-118	We compare three sequentializations : a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence consisting of derivation steps relative to the underlying grammar .	We compare three sequentializations : a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence consisting of derivation steps relative to the underlying grammar .	1<2	elab-enumember	elab-enumember
P16-1127	90-108	109-118	a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence	consisting of derivation steps relative to the underlying grammar .	85-118	85-118	We compare three sequentializations : a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence consisting of derivation steps relative to the underlying grammar .	We compare three sequentializations : a direct linearization of the logical form , a linearization of the associated canonical realization , and a sequence consisting of derivation steps relative to the underlying grammar .	1<2	elab-addition	elab-addition
P16-1127	119-121	122-138	We also show	how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor .	119-138	119-138	We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor .	We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor .	1>2	attribution	attribution
P16-1127	77-84	122-138	we need to sequentialize these logical forms .	how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor .	70-84	119-138	In order to use sequence prediction , we need to sequentialize these logical forms .	We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor .	1<2	elab-addition	elab-addition
P16-1127	1-7	139-151	We propose an approach for semantic parsing	Our experiments show important improvements over previous results for the same dataset ,	1-29	139-162	We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query .	Our experiments show important improvements over previous results for the same dataset , and also demonstrate the advantage of incorporating the grammatical constraints .	1<2	evaluation	evaluation
P16-1127	139-151	152-156	Our experiments show important improvements over previous results for the same dataset ,	and also demonstrate the advantage	139-162	139-162	Our experiments show important improvements over previous results for the same dataset , and also demonstrate the advantage of incorporating the grammatical constraints .	Our experiments show important improvements over previous results for the same dataset , and also demonstrate the advantage of incorporating the grammatical constraints .	1<2	joint	joint
P16-1127	152-156	157-162	and also demonstrate the advantage	of incorporating the grammatical constraints .	139-162	139-162	Our experiments show important improvements over previous results for the same dataset , and also demonstrate the advantage of incorporating the grammatical constraints .	Our experiments show important improvements over previous results for the same dataset , and also demonstrate the advantage of incorporating the grammatical constraints .	1<2	elab-addition	elab-addition
P16-1128	1-19	20-35	Word embeddings - distributed representations of words - in deep learning are beneficial for many tasks in NLP .	However , different embedding sets vary greatly in quality and characteristics of the captured information .	1-19	20-35	Word embeddings - distributed representations of words - in deep learning are beneficial for many tasks in NLP .	However , different embedding sets vary greatly in quality and characteristics of the captured information .	1>2	contrast	contrast
P16-1128	20-35	48-53	However , different embedding sets vary greatly in quality and characteristics of the captured information .	this paper proposes an ensemble approach	20-35	36-66	However , different embedding sets vary greatly in quality and characteristics of the captured information .	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	1>2	bg-goal	bg-goal
P16-1128	36-47	48-53	Instead of relying on a more advanced algorithm for embedding learning ,	this paper proposes an ensemble approach	36-66	36-66	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	1>2	contrast	contrast
P16-1128	48-53	54-66	this paper proposes an ensemble approach	of combining different public embedding sets with the aim of learning metaembeddings .	36-66	36-66	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	1<2	elab-addition	elab-addition
P16-1128	48-53	67-82	this paper proposes an ensemble approach	Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings	36-66	67-88	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets .	1<2	evaluation	evaluation
P16-1128	67-82	83-88	Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings	compared to individual embedding sets .	67-88	67-88	Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets .	Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets .	1<2	comparison	comparison
P16-1128	48-53	89-98	this paper proposes an ensemble approach	One advantage of metaembeddings is the increased vocabulary coverage .	36-66	89-98	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	One advantage of metaembeddings is the increased vocabulary coverage .	1<2	evaluation	evaluation
P16-1128	48-53	99-107	this paper proposes an ensemble approach	We release our metaembeddings publicly at http ://cistern.cis.lmu.de/meta-emb .	36-66	99-107	Instead of relying on a more advanced algorithm for embedding learning , this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings .	We release our metaembeddings publicly at http ://cistern.cis.lmu.de/meta-emb .	1<2	elab-addition	elab-addition
P16-1129	1-8	9-19	In this paper , we investigate the possibility	to automatically generate sports news from live text commentary scripts .	1-19	1-19	In this paper , we investigate the possibility to automatically generate sports news from live text commentary scripts .	In this paper , we investigate the possibility to automatically generate sports news from live text commentary scripts .	1<2	elab-addition	elab-addition
P16-1129	1-8	20-35	In this paper , we investigate the possibility	As a preliminary study , we treat this task as a special kind of document summarization	1-19	20-40	In this paper , we investigate the possibility to automatically generate sports news from live text commentary scripts .	As a preliminary study , we treat this task as a special kind of document summarization based on sentence extraction .	1<2	elab-process_step	elab-process_step
P16-1129	20-35	36-40	As a preliminary study , we treat this task as a special kind of document summarization	based on sentence extraction .	20-40	20-40	As a preliminary study , we treat this task as a special kind of document summarization based on sentence extraction .	As a preliminary study , we treat this task as a special kind of document summarization based on sentence extraction .	1<2	bg-general	bg-general
P16-1129	1-8	41-52	In this paper , we investigate the possibility	We formulate the task in a supervised learning to rank framework ,	1-19	41-67	In this paper , we investigate the possibility to automatically generate sports news from live text commentary scripts .	We formulate the task in a supervised learning to rank framework , utilizing both traditional sentence features for generic document summarization and novelly designed task-specific features .	1<2	elab-process_step	elab-process_step
P16-1129	41-52	53-67	We formulate the task in a supervised learning to rank framework ,	utilizing both traditional sentence features for generic document summarization and novelly designed task-specific features .	41-67	41-67	We formulate the task in a supervised learning to rank framework , utilizing both traditional sentence features for generic document summarization and novelly designed task-specific features .	We formulate the task in a supervised learning to rank framework , utilizing both traditional sentence features for generic document summarization and novelly designed task-specific features .	1<2	manner-means	manner-means
P16-1129	68-75	76-84	To tackle the problem of local redundancy ,	we also propose a probabilistic sentence selection algorithm .	68-84	68-84	To tackle the problem of local redundancy , we also propose a probabilistic sentence selection algorithm .	To tackle the problem of local redundancy , we also propose a probabilistic sentence selection algorithm .	1>2	enablement	enablement
P16-1129	1-8	76-84	In this paper , we investigate the possibility	we also propose a probabilistic sentence selection algorithm .	1-19	68-84	In this paper , we investigate the possibility to automatically generate sports news from live text commentary scripts .	To tackle the problem of local redundancy , we also propose a probabilistic sentence selection algorithm .	1<2	elab-process_step	elab-process_step
P16-1129	85-105	109-118	Experiments on our collected data from football live commentary scripts and corresponding sports news demonstrate the feasibility of this task .	that our methods are indeed appropriate for this task ,	85-105	106-126	Experiments on our collected data from football live commentary scripts and corresponding sports news demonstrate the feasibility of this task .	Evaluation results show that our methods are indeed appropriate for this task , outperforming several baseline methods in different aspects .	1>2	result	result
P16-1129	106-108	109-118	Evaluation results show	that our methods are indeed appropriate for this task ,	106-126	106-126	Evaluation results show that our methods are indeed appropriate for this task , outperforming several baseline methods in different aspects .	Evaluation results show that our methods are indeed appropriate for this task , outperforming several baseline methods in different aspects .	1>2	attribution	attribution
P16-1129	1-8	109-118	In this paper , we investigate the possibility	that our methods are indeed appropriate for this task ,	1-19	106-126	In this paper , we investigate the possibility to automatically generate sports news from live text commentary scripts .	Evaluation results show that our methods are indeed appropriate for this task , outperforming several baseline methods in different aspects .	1<2	evaluation	evaluation
P16-1129	109-118	119-126	that our methods are indeed appropriate for this task ,	outperforming several baseline methods in different aspects .	106-126	106-126	Evaluation results show that our methods are indeed appropriate for this task , outperforming several baseline methods in different aspects .	Evaluation results show that our methods are indeed appropriate for this task , outperforming several baseline methods in different aspects .	1<2	elab-addition	elab-addition
P16-1130	1-7	8-21	This paper presents neural probabilistic parsing models	which explore up to third order graph-based parsing with maximum likelihood training criteria .	1-21	1-21	This paper presents neural probabilistic parsing models which explore up to third order graph-based parsing with maximum likelihood training criteria .	This paper presents neural probabilistic parsing models which explore up to third order graph-based parsing with maximum likelihood training criteria .	1<2	elab-addition	elab-addition
P16-1130	1-7	22-31	This paper presents neural probabilistic parsing models	Two neural network extensions are exploited for performance improvement .	1-21	22-31	This paper presents neural probabilistic parsing models which explore up to third order graph-based parsing with maximum likelihood training criteria .	Two neural network extensions are exploited for performance improvement .	1<2	elab-addition	elab-addition
P16-1130	22-31	32-36,47-48	Two neural network extensions are exploited for performance improvement .	Firstly , a convolutional layer <*> is used	22-31	32-57	Two neural network extensions are exploited for performance improvement .	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	1<2	elab-enumember	elab-enumember
P16-1130	32-36,47-48	37-46	Firstly , a convolutional layer <*> is used	that absorbs the influences of all words in a sentence	32-57	32-57	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	1<2	elab-addition	elab-addition
P16-1130	32-36,47-48	49-57	Firstly , a convolutional layer <*> is used	so that sentence-level information can be effectively captured .	32-57	32-57	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	1<2	cause	cause
P16-1130	32-36,47-48	58-64	Firstly , a convolutional layer <*> is used	Secondly , a linear layer is added	32-57	58-76	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	1<2	joint	joint
P16-1130	58-64	65-70	Secondly , a linear layer is added	to integrate different order neural models	58-76	58-76	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	1<2	enablement	enablement
P16-1130	58-64	71-76	Secondly , a linear layer is added	and trained with perceptron method .	58-76	58-76	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	1<2	joint	joint
P16-1130	77-87	88-92	The proposed parsers are evaluated on English and Chinese Penn Treebanks	and obtain competitive accuracies .	77-92	77-92	The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies .	The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies .	1>2	progression	progression
P16-1130	1-7	88-92	This paper presents neural probabilistic parsing models	and obtain competitive accuracies .	1-21	77-92	This paper presents neural probabilistic parsing models which explore up to third order graph-based parsing with maximum likelihood training criteria .	The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies .	1<2	evaluation	evaluation
P16-1131	1-7	8-20	This paper presents neural probabilistic parsing models	which explore up to thirdorder graph-based parsing with maximum likelihood training criteria .	1-20	1-20	This paper presents neural probabilistic parsing models which explore up to thirdorder graph-based parsing with maximum likelihood training criteria .	This paper presents neural probabilistic parsing models which explore up to thirdorder graph-based parsing with maximum likelihood training criteria .	1<2	elab-addition	elab-addition
P16-1131	1-7	21-30	This paper presents neural probabilistic parsing models	Two neural network extensions are exploited for performance improvement .	1-20	21-30	This paper presents neural probabilistic parsing models which explore up to thirdorder graph-based parsing with maximum likelihood training criteria .	Two neural network extensions are exploited for performance improvement .	1<2	elab-addition	elab-addition
P16-1131	21-30	31-35,46-47	Two neural network extensions are exploited for performance improvement .	Firstly , a convolutional layer <*> is used	21-30	31-56	Two neural network extensions are exploited for performance improvement .	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	1<2	elab-enumember	elab-enumember
P16-1131	31-35,46-47	36-45	Firstly , a convolutional layer <*> is used	that absorbs the influences of all words in a sentence	31-56	31-56	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	1<2	elab-addition	elab-addition
P16-1131	31-35,46-47	48-56	Firstly , a convolutional layer <*> is used	so that sentence-level information can be effectively captured .	31-56	31-56	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured .	1<2	cause	cause
P16-1131	21-30	57-63	Two neural network extensions are exploited for performance improvement .	Secondly , a linear layer is added	21-30	57-75	Two neural network extensions are exploited for performance improvement .	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	1<2	elab-enumember	elab-enumember
P16-1131	57-63	64-69	Secondly , a linear layer is added	to integrate different order neural models	57-75	57-75	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	1<2	enablement	enablement
P16-1131	57-63	70-75	Secondly , a linear layer is added	and trained with perceptron method .	57-75	57-75	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method .	1<2	joint	joint
P16-1131	76-86	87-91	The proposed parsers are evaluated on English and Chinese Penn Treebanks	and obtain competitive accuracies .	76-91	76-91	The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies .	The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies .	1>2	progression	progression
P16-1131	1-7	87-91	This paper presents neural probabilistic parsing models	and obtain competitive accuracies .	1-20	76-91	This paper presents neural probabilistic parsing models which explore up to thirdorder graph-based parsing with maximum likelihood training criteria .	The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies .	1<2	evaluation	evaluation
P16-1132	1-6	7-14	We propose a novel reranking method	to extend a deterministic neural dependency parser .	1-14	1-14	We propose a novel reranking method to extend a deterministic neural dependency parser .	We propose a novel reranking method to extend a deterministic neural dependency parser .	1<2	elab-addition	elab-addition
P16-1132	15-20	21-27	Different to conventional k-best reranking ,	the proposed model integrates search and learning	15-52	15-52	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	1>2	comparison	comparison
P16-1132	1-6	21-27	We propose a novel reranking method	the proposed model integrates search and learning	1-14	15-52	We propose a novel reranking method to extend a deterministic neural dependency parser .	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	1<2	elab-addition	elab-addition
P16-1132	21-27	28-35	the proposed model integrates search and learning	by utilizing a dynamic action revising process ,	15-52	15-52	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	1<2	manner-means	manner-means
P16-1132	28-35	36-39	by utilizing a dynamic action revising process ,	using the reranking model	15-52	15-52	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	1<2	manner-means	manner-means
P16-1132	36-39	40-46	using the reranking model	to guide modification for the base outputs	15-52	15-52	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	1<2	enablement	enablement
P16-1132	40-46	47-52	to guide modification for the base outputs	and to rerank the candidates .	15-52	15-52	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	Different to conventional k-best reranking , the proposed model integrates search and learning by utilizing a dynamic action revising process , using the reranking model to guide modification for the base outputs and to rerank the candidates .	1<2	joint	joint
P16-1132	1-6	53-71	We propose a novel reranking method	The dynamic reranking model achieves an absolute 1.78 % accuracy improvement over the deterministic baseline parser on PTB ,	1-14	53-83	We propose a novel reranking method to extend a deterministic neural dependency parser .	The dynamic reranking model achieves an absolute 1.78 % accuracy improvement over the deterministic baseline parser on PTB , which is the highest improvement by neural rerankers in the literature .	1<2	evaluation	evaluation
P16-1132	53-71	72-83	The dynamic reranking model achieves an absolute 1.78 % accuracy improvement over the deterministic baseline parser on PTB ,	which is the highest improvement by neural rerankers in the literature .	53-83	53-83	The dynamic reranking model achieves an absolute 1.78 % accuracy improvement over the deterministic baseline parser on PTB , which is the highest improvement by neural rerankers in the literature .	The dynamic reranking model achieves an absolute 1.78 % accuracy improvement over the deterministic baseline parser on PTB , which is the highest improvement by neural rerankers in the literature .	1<2	elab-addition	elab-addition
P16-1133	1-18	19-28	Cross-lingual sentiment classification aims to adapt the sentiment resource in a resource-rich language to a resource-poor language .	In this study , we propose a representation learning approach	1-18	19-45	Cross-lingual sentiment classification aims to adapt the sentiment resource in a resource-rich language to a resource-poor language .	In this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages .	1>2	bg-goal	bg-goal
P16-1133	19-28	29-45	In this study , we propose a representation learning approach	which simultaneously learns vector representations for the texts in both the source and the target languages .	19-45	19-45	In this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages .	In this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages .	1<2	elab-addition	elab-addition
P16-1133	46-49	57-68	Different from previous research	our Bilingual Document Representation Learning model BiDRL directly learns document representations .	46-68	46-68	Different from previous research which only gets bilingual word embedding , our Bilingual Document Representation Learning model BiDRL directly learns document representations .	Different from previous research which only gets bilingual word embedding , our Bilingual Document Representation Learning model BiDRL directly learns document representations .	1>2	comparison	comparison
P16-1133	46-49	50-56	Different from previous research	which only gets bilingual word embedding ,	46-68	46-68	Different from previous research which only gets bilingual word embedding , our Bilingual Document Representation Learning model BiDRL directly learns document representations .	Different from previous research which only gets bilingual word embedding , our Bilingual Document Representation Learning model BiDRL directly learns document representations .	1<2	elab-addition	elab-addition
P16-1133	19-28	57-68	In this study , we propose a representation learning approach	our Bilingual Document Representation Learning model BiDRL directly learns document representations .	19-45	46-68	In this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages .	Different from previous research which only gets bilingual word embedding , our Bilingual Document Representation Learning model BiDRL directly learns document representations .	1<2	elab-addition	elab-addition
P16-1133	57-68	69-75	our Bilingual Document Representation Learning model BiDRL directly learns document representations .	Both semantic and sentiment correlations are utilized	46-68	69-86	Different from previous research which only gets bilingual word embedding , our Bilingual Document Representation Learning model BiDRL directly learns document representations .	Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space .	1<2	elab-addition	elab-addition
P16-1133	69-75	76-86	Both semantic and sentiment correlations are utilized	to map the bilingual texts into the same embedding space .	69-86	69-86	Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space .	Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space .	1<2	enablement	enablement
P16-1133	19-28	87-98	In this study , we propose a representation learning approach	The experiments are based on the multilingual multi-domain Amazon review dataset .	19-45	87-98	In this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages .	The experiments are based on the multilingual multi-domain Amazon review dataset .	1<2	elab-addition	elab-addition
P16-1133	87-98	99-105	The experiments are based on the multilingual multi-domain Amazon review dataset .	We use English as the source language	87-98	99-117	The experiments are based on the multilingual multi-domain Amazon review dataset .	We use English as the source language and use Japanese , German and French as the target languages .	1<2	elab-addition	elab-addition
P16-1133	99-105	106-117	We use English as the source language	and use Japanese , German and French as the target languages .	99-117	99-117	We use English as the source language and use Japanese , German and French as the target languages .	We use English as the source language and use Japanese , German and French as the target languages .	1<2	joint	joint
P16-1133	118-121	122-133	The experimental results show	that BiDRL outperforms the state-of-the-art methods for all the target languages .	118-133	118-133	The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages .	The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages .	1>2	attribution	attribution
P16-1133	19-28	122-133	In this study , we propose a representation learning approach	that BiDRL outperforms the state-of-the-art methods for all the target languages .	19-45	118-133	In this study , we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages .	The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages .	1<2	evaluation	evaluation
P16-1134	1-25	26-37	Most of the sequence tagging tasks in natural language processing require to recognize segments with certain syntactic role or semantic meaning in a sentence .	They are usually tackled with Conditional Random Fields ( CRFs ) ,	1-25	26-55	Most of the sequence tagging tasks in natural language processing require to recognize segments with certain syntactic role or semantic meaning in a sentence .	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	1>2	elab-addition	elab-addition
P16-1134	26-37	79-88	They are usually tackled with Conditional Random Fields ( CRFs ) ,	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) ,	26-55	79-106	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) , which model segments directly and automatically learn segment level features through a gated recursive convolutional neural network .	1>2	bg-compare	bg-compare
P16-1134	26-37	38-45	They are usually tackled with Conditional Random Fields ( CRFs ) ,	which do indirect word-level modeling over word-level features	26-55	26-55	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	1<2	elab-addition	elab-addition
P16-1134	38-45	46-55	which do indirect word-level modeling over word-level features	and thus cannot make full use of segment-level information .	26-55	26-55	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	1<2	progression	progression
P16-1134	56-65	66-78	Semi-Markov Conditional Random Fields ( Semi-CRFs ) model segments directly	but extracting segment-level features for Semi-CRFs is still a very challenging problem .	56-78	56-78	Semi-Markov Conditional Random Fields ( Semi-CRFs ) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem .	Semi-Markov Conditional Random Fields ( Semi-CRFs ) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem .	1>2	contrast	contrast
P16-1134	26-37	66-78	They are usually tackled with Conditional Random Fields ( CRFs ) ,	but extracting segment-level features for Semi-CRFs is still a very challenging problem .	26-55	56-78	They are usually tackled with Conditional Random Fields ( CRFs ) , which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information .	Semi-Markov Conditional Random Fields ( Semi-CRFs ) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem .	1<2	elab-addition	elab-addition
P16-1134	79-88	89-92	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) ,	which model segments directly	79-106	79-106	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) , which model segments directly and automatically learn segment level features through a gated recursive convolutional neural network .	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) , which model segments directly and automatically learn segment level features through a gated recursive convolutional neural network .	1<2	elab-addition	elab-addition
P16-1134	89-92	93-106	which model segments directly	and automatically learn segment level features through a gated recursive convolutional neural network .	79-106	79-106	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) , which model segments directly and automatically learn segment level features through a gated recursive convolutional neural network .	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) , which model segments directly and automatically learn segment level features through a gated recursive convolutional neural network .	1<2	joint	joint
P16-1134	107-119	120-127	Our experiments on text chunking and named entity recognition ( NER ) demonstrate	that grSemi-CRFs generally outperform other neural models .	107-127	107-127	Our experiments on text chunking and named entity recognition ( NER ) demonstrate that grSemi-CRFs generally outperform other neural models .	Our experiments on text chunking and named entity recognition ( NER ) demonstrate that grSemi-CRFs generally outperform other neural models .	1>2	attribution	attribution
P16-1134	79-88	120-127	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) ,	that grSemi-CRFs generally outperform other neural models .	79-106	107-127	This paper presents Gated Recursive Semi-CRFs ( grSemi-CRFs ) , which model segments directly and automatically learn segment level features through a gated recursive convolutional neural network .	Our experiments on text chunking and named entity recognition ( NER ) demonstrate that grSemi-CRFs generally outperform other neural models .	1<2	evaluation	evaluation
P16-1135	1-15	36-43	The automatic detection of causal relationships in text is important for natural language understanding .	We focus on a sub-task of this problem	1-15	36-58	The automatic detection of causal relationships in text is important for natural language understanding .	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	1>2	bg-goal	bg-goal
P16-1135	1-15	16-23	The automatic detection of causal relationships in text is important for natural language understanding .	This task has proven to be difficult ,	1-15	16-35	The automatic detection of causal relationships in text is important for natural language understanding .	This task has proven to be difficult , however , due to the need for world knowledge and inference .	1<2	elab-addition	elab-addition
P16-1135	16-23	24-35	This task has proven to be difficult ,	however , due to the need for world knowledge and inference .	16-35	16-35	This task has proven to be difficult , however , due to the need for world knowledge and inference .	This task has proven to be difficult , however , due to the need for world knowledge and inference .	1<2	exp-reason	exp-reason
P16-1135	36-43	44-58	We focus on a sub-task of this problem	where an open class set of linguistic markers can provide clues towards understanding causality .	36-58	36-58	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	1<2	elab-addition	elab-addition
P16-1135	59-67	68-76	Unlike the explicit markers , a closed class ,	these markers vary significantly in their linguistic forms .	59-76	59-76	Unlike the explicit markers , a closed class , these markers vary significantly in their linguistic forms .	Unlike the explicit markers , a closed class , these markers vary significantly in their linguistic forms .	1>2	comparison	comparison
P16-1135	44-58	68-76	where an open class set of linguistic markers can provide clues towards understanding causality .	these markers vary significantly in their linguistic forms .	36-58	59-76	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	Unlike the explicit markers , a closed class , these markers vary significantly in their linguistic forms .	1<2	elab-addition	elab-addition
P16-1135	36-43	77-81	We focus on a sub-task of this problem	We leverage parallel Wikipedia corpora	36-58	77-101	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	1<2	elab-addition	elab-addition
P16-1135	77-81	82-85	We leverage parallel Wikipedia corpora	to identify new markers	77-101	77-101	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	1<2	enablement	enablement
P16-1135	82-85	86-93	to identify new markers	that are variations on known causal phrases ,	77-101	77-101	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	1<2	elab-addition	elab-addition
P16-1135	77-81	94-97	We leverage parallel Wikipedia corpora	creating a training set	77-101	77-101	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	1<2	cause	cause
P16-1135	94-97	98-101	creating a training set	via distant supervision .	77-101	77-101	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases , creating a training set via distant supervision .	1<2	manner-means	manner-means
P16-1135	36-43	102-107	We focus on a sub-task of this problem	We also train a causal classifier	36-58	102-121	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	We also train a causal classifier using features from the open class markers and semantic features providing contextual information .	1<2	elab-addition	elab-addition
P16-1135	102-107	108-117	We also train a causal classifier	using features from the open class markers and semantic features	102-121	102-121	We also train a causal classifier using features from the open class markers and semantic features providing contextual information .	We also train a causal classifier using features from the open class markers and semantic features providing contextual information .	1<2	manner-means	manner-means
P16-1135	102-107	118-121	We also train a causal classifier	providing contextual information .	102-121	102-121	We also train a causal classifier using features from the open class markers and semantic features providing contextual information .	We also train a causal classifier using features from the open class markers and semantic features providing contextual information .	1<2	elab-addition	elab-addition
P16-1135	122-124	125-139	The results show	that our features provide an 11.05 point absolute increase over the baseline on the task	122-145	122-145	The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text .	The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text .	1>2	attribution	attribution
P16-1135	36-43	125-139	We focus on a sub-task of this problem	that our features provide an 11.05 point absolute increase over the baseline on the task	36-58	122-145	We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality .	The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text .	1<2	evaluation	evaluation
P16-1135	125-139	140-145	that our features provide an 11.05 point absolute increase over the baseline on the task	of identifying causality in text .	122-145	122-145	The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text .	The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text .	1<2	elab-addition	elab-addition
P16-1136	1-18	19-29	Modeling relation paths has offered significant gains in embedding models for knowledge base ( KB ) completion .	However , enumerating paths between two entities is very expensive ,	1-18	19-41	Modeling relation paths has offered significant gains in embedding models for knowledge base ( KB ) completion .	However , enumerating paths between two entities is very expensive , and existing approaches typically resort to approximation with a sampled subset .	1>2	elab-addition	elab-addition
P16-1136	19-29	67-78	However , enumerating paths between two entities is very expensive ,	In this paper , we propose the first exact dynamic programming algorithm	19-41	67-104	However , enumerating paths between two entities is very expensive , and existing approaches typically resort to approximation with a sampled subset .	In this paper , we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length , while modeling both relation types and intermediate nodes in the compositional path representations .	1>2	bg-goal	bg-goal
P16-1136	19-29	30-41	However , enumerating paths between two entities is very expensive ,	and existing approaches typically resort to approximation with a sampled subset .	19-41	19-41	However , enumerating paths between two entities is very expensive , and existing approaches typically resort to approximation with a sampled subset .	However , enumerating paths between two entities is very expensive , and existing approaches typically resort to approximation with a sampled subset .	1<2	joint	joint
P16-1136	19-29	42-46	However , enumerating paths between two entities is very expensive ,	This problem is particularly acute	19-41	42-66	However , enumerating paths between two entities is very expensive , and existing approaches typically resort to approximation with a sampled subset .	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	1<2	elab-addition	elab-addition
P16-1136	42-46	47-54	This problem is particularly acute	when text is jointly modeled with KB relations	42-66	42-66	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	1<2	condition	condition
P16-1136	47-54	55-56	when text is jointly modeled with KB relations	and used	42-66	42-66	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	1<2	joint	joint
P16-1136	55-56	57-62	and used	to provide direct evidence for facts	42-66	42-66	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	1<2	enablement	enablement
P16-1136	57-62	63-66	to provide direct evidence for facts	mentioned in it .	42-66	42-66	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it .	1<2	elab-addition	elab-addition
P16-1136	67-78	79-90	In this paper , we propose the first exact dynamic programming algorithm	which enables efficient incorporation of all relation paths of bounded length ,	67-104	67-104	In this paper , we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length , while modeling both relation types and intermediate nodes in the compositional path representations .	In this paper , we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length , while modeling both relation types and intermediate nodes in the compositional path representations .	1<2	elab-addition	elab-addition
P16-1136	79-90	91-104	which enables efficient incorporation of all relation paths of bounded length ,	while modeling both relation types and intermediate nodes in the compositional path representations .	67-104	67-104	In this paper , we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length , while modeling both relation types and intermediate nodes in the compositional path representations .	In this paper , we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length , while modeling both relation types and intermediate nodes in the compositional path representations .	1<2	joint	joint
P16-1136	105-117	123-130	We conduct a theoretical analysis of the efficiency gain from the approach .	that it addresses representational limitations in prior approaches	105-117	118-137	We conduct a theoretical analysis of the efficiency gain from the approach .	Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion .	1>2	result	result
P16-1136	118-122	123-130	Experiments on two datasets show	that it addresses representational limitations in prior approaches	118-137	118-137	Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion .	Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion .	1>2	attribution	attribution
P16-1136	67-78	123-130	In this paper , we propose the first exact dynamic programming algorithm	that it addresses representational limitations in prior approaches	67-104	118-137	In this paper , we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length , while modeling both relation types and intermediate nodes in the compositional path representations .	Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion .	1<2	evaluation	evaluation
P16-1136	123-130	131-137	that it addresses representational limitations in prior approaches	and improves accuracy in KB completion .	118-137	118-137	Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion .	Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion .	1<2	joint	joint
P16-1137	1-8	64-68	We enrich a curated resource of commonsense knowledge	We develop neural network models	1-22	64-89	We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion ( KBC ) .	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	1>2	elab-addition	elab-addition
P16-1137	1-8	9-22	We enrich a curated resource of commonsense knowledge	by formulating the problem as one of knowledge base completion ( KBC ) .	1-22	1-22	We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion ( KBC ) .	We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion ( KBC ) .	1<2	manner-means	manner-means
P16-1137	23-32	42-63	Most work in KBC focuses on knowledge bases like Freebase	However , the tuples in ConceptNet ( Speer and Havasi , 2012 ) define relations between an unbounded set of phrases .	23-41	42-63	Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set .	However , the tuples in ConceptNet ( Speer and Havasi , 2012 ) define relations between an unbounded set of phrases .	1>2	contrast	contrast
P16-1137	23-32	33-35	Most work in KBC focuses on knowledge bases like Freebase	that relate entities	23-41	23-41	Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set .	Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set .	1<2	elab-addition	elab-addition
P16-1137	33-35	36-41	that relate entities	drawn from a fixed set .	23-41	23-41	Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set .	Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set .	1<2	elab-addition	elab-addition
P16-1137	42-63	64-68	However , the tuples in ConceptNet ( Speer and Havasi , 2012 ) define relations between an unbounded set of phrases .	We develop neural network models	42-63	64-89	However , the tuples in ConceptNet ( Speer and Havasi , 2012 ) define relations between an unbounded set of phrases .	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	1>2	bg-goal	bg-goal
P16-1137	64-68	69-74	We develop neural network models	for scoring tuples on arbitrary phrases	64-89	64-89	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	1<2	elab-addition	elab-addition
P16-1137	64-68	75-80	We develop neural network models	and evaluate them by their ability	64-89	64-89	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	1<2	joint	joint
P16-1137	75-80	81-89	and evaluate them by their ability	to distinguish true held-out tuples from false ones .	64-89	64-89	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	1<2	enablement	enablement
P16-1137	64-68	90-97	We develop neural network models	We find strong performance from a bilinear model	64-89	90-106	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	We find strong performance from a bilinear model using a simple additive architecture to model phrases .	1<2	evaluation	evaluation
P16-1137	90-97	98-106	We find strong performance from a bilinear model	using a simple additive architecture to model phrases .	90-106	90-106	We find strong performance from a bilinear model using a simple additive architecture to model phrases .	We find strong performance from a bilinear model using a simple additive architecture to model phrases .	1<2	manner-means	manner-means
P16-1137	107-114	124-140	We manually evaluate our trained model 's ability	that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	107-140	107-140	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	1>2	result	result
P16-1137	107-114	115-122	We manually evaluate our trained model 's ability	to assign quality scores to novel tuples ,	107-140	107-140	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	1<2	enablement	enablement
P16-1137	123	124-140	finding	that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	107-140	107-140	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	1>2	attribution	attribution
P16-1137	64-68	124-140	We develop neural network models	that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	64-89	107-140	We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones .	We manually evaluate our trained model 's ability to assign quality scores to novel tuples , finding that it can propose tuples at the same quality level as medium confidence tuples from ConceptNet .	1<2	evaluation	evaluation
P16-1138	1-4	49-59	We consider the task	we perform successive projections of the full model onto simpler models	1-15	43-68	We consider the task of learning a context dependent mapping from utterances to denotations .	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	1>2	elab-addition	elab-addition
P16-1138	1-4	5-15	We consider the task	of learning a context dependent mapping from utterances to denotations .	1-15	1-15	We consider the task of learning a context dependent mapping from utterances to denotations .	We consider the task of learning a context dependent mapping from utterances to denotations .	1<2	elab-addition	elab-addition
P16-1138	16-34	49-59	With only denotations at training time , we must search over a combinatorially large space of logical forms ,	we perform successive projections of the full model onto simpler models	16-42	43-68	With only denotations at training time , we must search over a combinatorially large space of logical forms , which is even larger with context-dependent utterances .	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	1>2	bg-goal	bg-goal
P16-1138	16-34	35-42	With only denotations at training time , we must search over a combinatorially large space of logical forms ,	which is even larger with context-dependent utterances .	16-42	16-42	With only denotations at training time , we must search over a combinatorially large space of logical forms , which is even larger with context-dependent utterances .	With only denotations at training time , we must search over a combinatorially large space of logical forms , which is even larger with context-dependent utterances .	1<2	elab-addition	elab-addition
P16-1138	43-48	49-59	To cope with this challenge ,	we perform successive projections of the full model onto simpler models	43-68	43-68	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	1>2	enablement	enablement
P16-1138	49-59	60-68	we perform successive projections of the full model onto simpler models	that operate over equivalence classes of logical forms .	43-68	43-68	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	1<2	elab-addition	elab-addition
P16-1138	69-72	75-81	Though less expressive ,	that these simpler models are much faster	69-87	69-87	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	1>2	contrast	contrast
P16-1138	73-74	75-81	we find	that these simpler models are much faster	69-87	69-87	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	1>2	attribution	attribution
P16-1138	49-59	75-81	we perform successive projections of the full model onto simpler models	that these simpler models are much faster	43-68	69-87	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	1<2	evaluation	evaluation
P16-1138	75-81	82-87	that these simpler models are much faster	and can be surprisingly effective .	69-87	69-87	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	Though less expressive , we find that these simpler models are much faster and can be surprisingly effective .	1<2	joint	joint
P16-1138	49-59	88-93	we perform successive projections of the full model onto simpler models	Moreover , they can be used	43-68	88-99	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	Moreover , they can be used to bootstrap the full model .	1<2	evaluation	evaluation
P16-1138	88-93	94-99	Moreover , they can be used	to bootstrap the full model .	88-99	88-99	Moreover , they can be used to bootstrap the full model .	Moreover , they can be used to bootstrap the full model .	1<2	enablement	enablement
P16-1138	49-59	100-110	we perform successive projections of the full model onto simpler models	Finally , we collected three new contextdependent semantic parsing datasets ,	43-68	100-117	To cope with this challenge , we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms .	Finally , we collected three new contextdependent semantic parsing datasets , and develop a new left-to-right parser .	1<2	elab-addition	elab-addition
P16-1138	100-110	111-117	Finally , we collected three new contextdependent semantic parsing datasets ,	and develop a new left-to-right parser .	100-117	100-117	Finally , we collected three new contextdependent semantic parsing datasets , and develop a new left-to-right parser .	Finally , we collected three new contextdependent semantic parsing datasets , and develop a new left-to-right parser .	1<2	joint	joint
P16-1139	1-8	17-25	Tree-structured neural networks exploit valuable syntactic parse information	However , they suffer from two key technical problems	1-16	17-51	Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences .	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	1>2	contrast	contrast
P16-1139	1-8	9-16	Tree-structured neural networks exploit valuable syntactic parse information	as they interpret the meanings of sentences .	1-16	1-16	Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences .	Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences .	1<2	temporal	temporal
P16-1139	17-25	52-55	However , they suffer from two key technical problems	We address these issues	17-51	52-93	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	1>2	bg-goal	bg-goal
P16-1139	17-25	26-36	However , they suffer from two key technical problems	that make them slow and unwieldy for large-scale NLP tasks :	17-51	17-51	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	1<2	elab-addition	elab-addition
P16-1139	17-25	37-42	However , they suffer from two key technical problems	they usually operate on parsed sentences	17-51	17-51	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	1<2	elab-addition	elab-addition
P16-1139	37-42	43-51	they usually operate on parsed sentences	and they do not directly support batched computation .	17-51	17-51	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	However , they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .	1<2	joint	joint
P16-1139	52-55	56-66	We address these issues	by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) ,	52-93	52-93	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	1<2	manner-means	manner-means
P16-1139	56-66	67-77	by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) ,	which combines parsing and interpretation within a single tree-sequence hybrid model	52-93	52-93	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	1<2	elab-addition	elab-addition
P16-1139	67-77	78-93	which combines parsing and interpretation within a single tree-sequence hybrid model	by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	52-93	52-93	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	1<2	manner-means	manner-means
P16-1139	52-55	94-111	We address these issues	Our model supports batched computation for a speedup of up to 25 * over other tree-structured models ,	52-93	94-126	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	Our model supports batched computation for a speedup of up to 25 * over other tree-structured models , and its integrated parser can operate on unparsed data with little loss in accuracy .	1<2	evaluation	evaluation
P16-1139	94-111	112-126	Our model supports batched computation for a speedup of up to 25 * over other tree-structured models ,	and its integrated parser can operate on unparsed data with little loss in accuracy .	94-126	94-126	Our model supports batched computation for a speedup of up to 25 * over other tree-structured models , and its integrated parser can operate on unparsed data with little loss in accuracy .	Our model supports batched computation for a speedup of up to 25 * over other tree-structured models , and its integrated parser can operate on unparsed data with little loss in accuracy .	1<2	joint	joint
P16-1139	127-135	138-145	We evaluate it on the Stanford NLI entailment task	that it significantly outperforms other sentence-encoding models .	127-145	127-145	We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models .	We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models .	1>2	progression	progression
P16-1139	136-137	138-145	and show	that it significantly outperforms other sentence-encoding models .	127-145	127-145	We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models .	We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models .	1>2	attribution	attribution
P16-1139	52-55	138-145	We address these issues	that it significantly outperforms other sentence-encoding models .	52-93	127-145	We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift reduce parser .	We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models .	1<2	evaluation	evaluation
P16-1140	1-12	13-17	Recently , many NLP tasks have benefited from distributed word representation .	However , it remains unknown	1-12	13-35	Recently , many NLP tasks have benefited from distributed word representation .	However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture .	1>2	contrast	contrast
P16-1140	13-17	36-48	However , it remains unknown	Here we investigate three representative models on a large set of language samples	13-35	36-58	However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture .	Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space .	1>2	bg-goal	bg-goal
P16-1140	13-17	18-30	However , it remains unknown	whether embedding models are really immune to the typological diversity of languages ,	13-35	13-35	However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture .	However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture .	1<2	elab-addition	elab-addition
P16-1140	18-30	31-35	whether embedding models are really immune to the typological diversity of languages ,	despite the language-independent architecture .	13-35	13-35	However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture .	However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture .	1<2	contrast	contrast
P16-1140	36-48	49-58	Here we investigate three representative models on a large set of language samples	by mapping dense embedding to sparse linguistic property space .	36-58	36-58	Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space .	Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space .	1<2	manner-means	manner-means
P16-1140	36-48	59-67	Here we investigate three representative models on a large set of language samples	Experiment results reveal the language universal and specific properties	36-58	59-73	Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space .	Experiment results reveal the language universal and specific properties encoded in various word representation .	1<2	evaluation	evaluation
P16-1140	59-67	68-73	Experiment results reveal the language universal and specific properties	encoded in various word representation .	59-73	59-73	Experiment results reveal the language universal and specific properties encoded in various word representation .	Experiment results reveal the language universal and specific properties encoded in various word representation .	1<2	elab-addition	elab-addition
P16-1140	36-48	74-89	Here we investigate three representative models on a large set of language samples	Additionally , strong evidence supports the utility of word form , especially for inflectional languages .	36-58	74-89	Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space .	Additionally , strong evidence supports the utility of word form , especially for inflectional languages .	1<2	evaluation	evaluation
P16-1141	1-18	19-26	Understanding how words change their meanings over time is key to models of language and cultural evolution ,	but historical data on meaning is scarce ,	1-34	1-34	Understanding how words change their meanings over time is key to models of language and cultural evolution , but historical data on meaning is scarce , making theories hard to develop and test .	Understanding how words change their meanings over time is key to models of language and cultural evolution , but historical data on meaning is scarce , making theories hard to develop and test .	1>2	contrast	contrast
P16-1141	19-26	51-55	but historical data on meaning is scarce ,	We develop a robust methodology	1-34	51-75	Understanding how words change their meanings over time is key to models of language and cultural evolution , but historical data on meaning is scarce , making theories hard to develop and test .	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	1>2	bg-goal	bg-goal
P16-1141	19-26	27-34	but historical data on meaning is scarce ,	making theories hard to develop and test .	1-34	1-34	Understanding how words change their meanings over time is key to models of language and cultural evolution , but historical data on meaning is scarce , making theories hard to develop and test .	Understanding how words change their meanings over time is key to models of language and cultural evolution , but historical data on meaning is scarce , making theories hard to develop and test .	1<2	cause	cause
P16-1141	35-43	44-50	Word embeddings show promise as a diachronic tool ,	but have not been carefully evaluated .	35-50	35-50	Word embeddings show promise as a diachronic tool , but have not been carefully evaluated .	Word embeddings show promise as a diachronic tool , but have not been carefully evaluated .	1>2	contrast	contrast
P16-1141	19-26	44-50	but historical data on meaning is scarce ,	but have not been carefully evaluated .	1-34	35-50	Understanding how words change their meanings over time is key to models of language and cultural evolution , but historical data on meaning is scarce , making theories hard to develop and test .	Word embeddings show promise as a diachronic tool , but have not been carefully evaluated .	1<2	elab-addition	elab-addition
P16-1141	51-55	56-59	We develop a robust methodology	for quantifying semantic change	51-75	51-75	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	1<2	elab-addition	elab-addition
P16-1141	56-59	60-63	for quantifying semantic change	by evaluating word embeddings	51-75	51-75	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	1<2	manner-means	manner-means
P16-1141	60-63	64-70	by evaluating word embeddings	( PPMI , SVD , word2vec )	51-75	51-75	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	1<2	elab-enumember	elab-enumember
P16-1141	60-63	71-75	by evaluating word embeddings	against known historical changes .	51-75	51-75	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	1<2	elab-addition	elab-addition
P16-1141	76-80	100-107	We then use this methodology	we propose two quantitative laws of semantic change	76-88	89-154	We then use this methodology to reveal statistical laws of semantic evolution .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1>2	result	result
P16-1141	76-80	81-88	We then use this methodology	to reveal statistical laws of semantic evolution .	76-88	76-88	We then use this methodology to reveal statistical laws of semantic evolution .	We then use this methodology to reveal statistical laws of semantic evolution .	1<2	enablement	enablement
P16-1141	89-92	100-107	Using six historical corpora	we propose two quantitative laws of semantic change	89-154	89-154	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1>2	manner-means	manner-means
P16-1141	89-92	93-99	Using six historical corpora	spanning four languages and two centuries ,	89-154	89-154	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1<2	elab-addition	elab-addition
P16-1141	51-55	100-107	We develop a robust methodology	we propose two quantitative laws of semantic change	51-75	89-154	We develop a robust methodology for quantifying semantic change by evaluating word embeddings ( PPMI , SVD , word2vec ) against known historical changes .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1<2	elab-addition	elab-addition
P16-1141	100-107	108-129	we propose two quantitative laws of semantic change	: ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency	89-154	89-154	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1<2	elab-enumember	elab-enumember
P16-1141	130-142	143,148-154	; ( ii ) the law of innovation - independent of frequency ,	words <*> have higher rates of semantic change .	89-154	89-154	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1>2	condition	condition
P16-1141	100-107	143,148-154	we propose two quantitative laws of semantic change	words <*> have higher rates of semantic change .	89-154	89-154	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1<2	elab-enumember	elab-enumember
P16-1141	143,148-154	144-147	words <*> have higher rates of semantic change .	that are more polysemous	89-154	89-154	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	Using six historical corpora spanning four languages and two centuries , we propose two quantitative laws of semantic change : ( i ) the law of conformity - the rate of semantic change scales with an inverse power-law of word frequency ; ( ii ) the law of innovation - independent of frequency , words that are more polysemous have higher rates of semantic change .	1<2	elab-addition	elab-addition
P16-1142	1-9	10-14	This paper complements semantic role representations with spatial knowledge	beyond indicating plain locations .	1-14	1-14	This paper complements semantic role representations with spatial knowledge beyond indicating plain locations .	This paper complements semantic role representations with spatial knowledge beyond indicating plain locations .	1<2	elab-addition	elab-addition
P16-1142	1-9	15-28	This paper complements semantic role representations with spatial knowledge	Namely , we extract where entities are ( and are not ) located ,	1-14	15-42	This paper complements semantic role representations with spatial knowledge beyond indicating plain locations .	Namely , we extract where entities are ( and are not ) located , and for how long ( seconds , hours , days , etc. ) .	1<2	elab-addition	elab-addition
P16-1142	15-28	29-42	Namely , we extract where entities are ( and are not ) located ,	and for how long ( seconds , hours , days , etc. ) .	15-42	15-42	Namely , we extract where entities are ( and are not ) located , and for how long ( seconds , hours , days , etc. ) .	Namely , we extract where entities are ( and are not ) located , and for how long ( seconds , hours , days , etc. ) .	1<2	joint	joint
P16-1142	43-45	46-53	Crowdsourced annotations show	that this additional knowledge is intuitive to humans	43-60	43-60	Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts .	Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts .	1>2	attribution	attribution
P16-1142	1-9	46-53	This paper complements semantic role representations with spatial knowledge	that this additional knowledge is intuitive to humans	1-14	43-60	This paper complements semantic role representations with spatial knowledge beyond indicating plain locations .	Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts .	1<2	evaluation	evaluation
P16-1142	46-53	54-60	that this additional knowledge is intuitive to humans	and can be annotated by non-experts .	43-60	43-60	Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts .	Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts .	1<2	joint	joint
P16-1142	61-63	64-70	Experimental results show	that the task can be automated .	61-70	61-70	Experimental results show that the task can be automated .	Experimental results show that the task can be automated .	1>2	attribution	attribution
P16-1142	1-9	64-70	This paper complements semantic role representations with spatial knowledge	that the task can be automated .	1-14	61-70	This paper complements semantic role representations with spatial knowledge beyond indicating plain locations .	Experimental results show that the task can be automated .	1<2	evaluation	evaluation
P16-1143	1-11	26-36	There has recently been a lot of interest in unsupervised methods	This paper analyses a state-of-the-art method for sense distribution learning ,	1-25	26-50	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	This paper analyses a state-of-the-art method for sense distribution learning , and optimises it for application to the entire vocabulary of a given language .	1>2	bg-compare	bg-compare
P16-1143	1-11	12-16	There has recently been a lot of interest in unsupervised methods	for learning sense distributions ,	1-25	1-25	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	1<2	elab-addition	elab-addition
P16-1143	1-11	17-19	There has recently been a lot of interest in unsupervised methods	particularly in applications	1-25	1-25	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	1<2	elab-addition	elab-addition
P16-1143	17-19	20-25	particularly in applications	where sense distinctions are needed .	1-25	1-25	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	There has recently been a lot of interest in unsupervised methods for learning sense distributions , particularly in applications where sense distinctions are needed .	1<2	elab-addition	elab-addition
P16-1143	26-36	37-50	This paper analyses a state-of-the-art method for sense distribution learning ,	and optimises it for application to the entire vocabulary of a given language .	26-50	26-50	This paper analyses a state-of-the-art method for sense distribution learning , and optimises it for application to the entire vocabulary of a given language .	This paper analyses a state-of-the-art method for sense distribution learning , and optimises it for application to the entire vocabulary of a given language .	1<2	joint	joint
P16-1143	26-36	51-56	This paper analyses a state-of-the-art method for sense distribution learning ,	The optimised method is then used	26-50	51-92	This paper analyses a state-of-the-art method for sense distribution learning , and optimises it for application to the entire vocabulary of a given language .	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	1<2	elab-addition	elab-addition
P16-1143	51-56	57-60	The optimised method is then used	to produce LEXSEMTM :	51-92	51-92	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	1<2	enablement	enablement
P16-1143	57-60	61-70	to produce LEXSEMTM :	a sense frequency and semantic dataset of unprecedented size ,	51-92	51-92	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	1<2	elab-definition	elab-definition
P16-1143	61-70	71-81	a sense frequency and semantic dataset of unprecedented size ,	spanning approximately 88 % of polysemous , English simplex lemmas ,	51-92	51-92	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	1<2	elab-addition	elab-addition
P16-1143	71-81	82-92	spanning approximately 88 % of polysemous , English simplex lemmas ,	which is released as a public resource to the community .	51-92	51-92	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	The optimised method is then used to produce LEXSEMTM : a sense frequency and semantic dataset of unprecedented size , spanning approximately 88 % of polysemous , English simplex lemmas , which is released as a public resource to the community .	1<2	elab-addition	elab-addition
P16-1143	93-102	103-109	Finally , the quality of this data is investigated ,	and the LEXSEMTM sense distributions are shown	93-136	93-136	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	1>2	progression	progression
P16-1143	26-36	103-109	This paper analyses a state-of-the-art method for sense distribution learning ,	and the LEXSEMTM sense distributions are shown	26-50	93-136	This paper analyses a state-of-the-art method for sense distribution learning , and optimises it for application to the entire vocabulary of a given language .	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	1<2	evaluation	evaluation
P16-1143	103-109	110-114	and the LEXSEMTM sense distributions are shown	to be superior to those	93-136	93-136	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	1<2	enablement	enablement
P16-1143	110-114	115-122	to be superior to those	based on the WORDNET first sense for lemmas	93-136	93-136	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	1<2	bg-general	bg-general
P16-1143	115-122	123-126	based on the WORDNET first sense for lemmas	missing from SEMCOR ,	93-136	93-136	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	1<2	elab-addition	elab-addition
P16-1143	110-114	127-136	to be superior to those	and at least on par with SEMCOR-based distributions otherwise .	93-136	93-136	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	Finally , the quality of this data is investigated , and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR , and at least on par with SEMCOR-based distributions otherwise .	1<2	joint	joint
P16-1144	1-6	7-16	We introduce LAMBADA , a dataset	to evaluate the capabilities of computational models for text understanding	1-24	1-24	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	1<2	enablement	enablement
P16-1144	7-16	17-24	to evaluate the capabilities of computational models for text understanding	by means of a word prediction task .	1-24	1-24	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	1<2	manner-means	manner-means
P16-1144	1-6	25-31	We introduce LAMBADA , a dataset	LAMBADA is a collection of narrative passages	1-24	25-67	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	1<2	elab-definition	elab-definition
P16-1144	25-31	32-34	LAMBADA is a collection of narrative passages	sharing the characteristic	25-67	25-67	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	1<2	elab-addition	elab-addition
P16-1144	32-34	35-44	sharing the characteristic	that human subjects are able to guess their last word	25-67	25-67	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	1<2	elab-addition	elab-addition
P16-1144	35-44	45-53	that human subjects are able to guess their last word	if they are exposed to the whole passage ,	25-67	25-67	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	1<2	condition	condition
P16-1144	45-53	54-62	if they are exposed to the whole passage ,	but not if they only see the last sentence	25-67	25-67	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	1<2	contrast	contrast
P16-1144	54-62	63-67	but not if they only see the last sentence	preceding the target word .	25-67	25-67	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage , but not if they only see the last sentence preceding the target word .	1<2	elab-addition	elab-addition
P16-1144	68-72	82-95	To succeed on LAMBADA ,	but must be able to keep track of information in the broader discourse .	68-95	68-95	To succeed on LAMBADA , computational models cannot simply rely on local context , but must be able to keep track of information in the broader discourse .	To succeed on LAMBADA , computational models cannot simply rely on local context , but must be able to keep track of information in the broader discourse .	1>2	enablement	enablement
P16-1144	73-81	82-95	computational models cannot simply rely on local context ,	but must be able to keep track of information in the broader discourse .	68-95	68-95	To succeed on LAMBADA , computational models cannot simply rely on local context , but must be able to keep track of information in the broader discourse .	To succeed on LAMBADA , computational models cannot simply rely on local context , but must be able to keep track of information in the broader discourse .	1>2	progression	progression
P16-1144	1-6	82-95	We introduce LAMBADA , a dataset	but must be able to keep track of information in the broader discourse .	1-24	68-95	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	To succeed on LAMBADA , computational models cannot simply rely on local context , but must be able to keep track of information in the broader discourse .	1<2	elab-addition	elab-addition
P16-1144	96-97	98-107	We show	that LAMBADA exemplifies a wide range of linguistic phenomena ,	96-125	96-125	We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1 % on this novel benchmark .	We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1 % on this novel benchmark .	1>2	attribution	attribution
P16-1144	1-6	98-107	We introduce LAMBADA , a dataset	that LAMBADA exemplifies a wide range of linguistic phenomena ,	1-24	96-125	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1 % on this novel benchmark .	1<2	evaluation	evaluation
P16-1144	98-107	108-125	that LAMBADA exemplifies a wide range of linguistic phenomena ,	and that none of several state-of-the-art language models reaches accuracy above 1 % on this novel benchmark .	96-125	96-125	We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1 % on this novel benchmark .	We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1 % on this novel benchmark .	1<2	joint	joint
P16-1144	1-6	126-135	We introduce LAMBADA , a dataset	We thus propose LAMBADA as a challenging test set ,	1-24	126-155	We introduce LAMBADA , a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task .	We thus propose LAMBADA as a challenging test set , meant to encourage the development of new models capable of genuine understanding of broad context in natural language text .	1<2	elab-addition	elab-addition
P16-1144	126-135	136-143	We thus propose LAMBADA as a challenging test set ,	meant to encourage the development of new models	126-155	126-155	We thus propose LAMBADA as a challenging test set , meant to encourage the development of new models capable of genuine understanding of broad context in natural language text .	We thus propose LAMBADA as a challenging test set , meant to encourage the development of new models capable of genuine understanding of broad context in natural language text .	1<2	enablement	enablement
P16-1144	136-143	144-155	meant to encourage the development of new models	capable of genuine understanding of broad context in natural language text .	126-155	126-155	We thus propose LAMBADA as a challenging test set , meant to encourage the development of new models capable of genuine understanding of broad context in natural language text .	We thus propose LAMBADA as a challenging test set , meant to encourage the development of new models capable of genuine understanding of broad context in natural language text .	1<2	elab-addition	elab-addition
P16-1145	1-18	19-31	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	The task is to predict textual values from the structured knowledge base Wikidata	1-18	19-41	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles .	1<2	elab-addition	elab-addition
P16-1145	19-31	32-41	The task is to predict textual values from the structured knowledge base Wikidata	by reading the text of the corresponding Wikipedia articles .	19-41	19-41	The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles .	The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles .	1<2	manner-means	manner-means
P16-1145	1-18	42-54	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	The task contains a rich variety of challenging classification and extraction sub-tasks ,	1-18	42-69	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	The task contains a rich variety of challenging classification and extraction sub-tasks , making it well-suited for end-to-end models such as deep neural networks ( DNNs ) .	1<2	elab-addition	elab-addition
P16-1145	42-54	55-60	The task contains a rich variety of challenging classification and extraction sub-tasks ,	making it well-suited for end-to-end models	42-69	42-69	The task contains a rich variety of challenging classification and extraction sub-tasks , making it well-suited for end-to-end models such as deep neural networks ( DNNs ) .	The task contains a rich variety of challenging classification and extraction sub-tasks , making it well-suited for end-to-end models such as deep neural networks ( DNNs ) .	1<2	cause	cause
P16-1145	55-60	61-69	making it well-suited for end-to-end models	such as deep neural networks ( DNNs ) .	42-69	42-69	The task contains a rich variety of challenging classification and extraction sub-tasks , making it well-suited for end-to-end models such as deep neural networks ( DNNs ) .	The task contains a rich variety of challenging classification and extraction sub-tasks , making it well-suited for end-to-end models such as deep neural networks ( DNNs ) .	1<2	elab-example	elab-example
P16-1145	70-87	90-91,105-107	We compare various state-of-the-art DNN based architectures for document classification , information extraction , and question answering .	that models <*> perform best .	70-87	88-107	We compare various state-of-the-art DNN based architectures for document classification , information extraction , and question answering .	We find that models supporting a rich answer space , such as word or character sequences , perform best .	1>2	result	result
P16-1145	88-89	90-91,105-107	We find	that models <*> perform best .	88-107	88-107	We find that models supporting a rich answer space , such as word or character sequences , perform best .	We find that models supporting a rich answer space , such as word or character sequences , perform best .	1>2	attribution	attribution
P16-1145	1-18	90-91,105-107	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	that models <*> perform best .	1-18	88-107	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	We find that models supporting a rich answer space , such as word or character sequences , perform best .	1<2	evaluation	evaluation
P16-1145	90-91,105-107	92-97	that models <*> perform best .	supporting a rich answer space ,	88-107	88-107	We find that models supporting a rich answer space , such as word or character sequences , perform best .	We find that models supporting a rich answer space , such as word or character sequences , perform best .	1<2	elab-addition	elab-addition
P16-1145	92-97	98-104	supporting a rich answer space ,	such as word or character sequences ,	88-107	88-107	We find that models supporting a rich answer space , such as word or character sequences , perform best .	We find that models supporting a rich answer space , such as word or character sequences , perform best .	1<2	elab-example	elab-example
P16-1145	1-18	108-120,126-132	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	Our best-performing model , a word-level sequence to sequence model with a mechanism <*> obtains an accuracy of 71.8 % .	1-18	108-132	We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances .	Our best-performing model , a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words , obtains an accuracy of 71.8 % .	1<2	evaluation	evaluation
P16-1145	108-120,126-132	121-125	Our best-performing model , a word-level sequence to sequence model with a mechanism <*> obtains an accuracy of 71.8 % .	to copy out-of-vocabulary words ,	108-132	108-132	Our best-performing model , a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words , obtains an accuracy of 71.8 % .	Our best-performing model , a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words , obtains an accuracy of 71.8 % .	1<2	elab-addition	elab-addition
P16-1146	1-5	6-12	We describe a search algorithm	for optimizing the number of latent states	1-20	1-20	We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods .	We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods .	1<2	elab-addition	elab-addition
P16-1146	6-12	13-20	for optimizing the number of latent states	when estimating latent-variable PCFGs with spectral methods .	1-20	1-20	We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods .	We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods .	1<2	temporal	temporal
P16-1146	21-23	51-54	Our results show	parsing results significantly improve	21-77	21-77	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	1>2	attribution	attribution
P16-1146	24-29	51-54	that contrary to the common belief	parsing results significantly improve	21-77	21-77	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	1>2	comparison	comparison
P16-1146	24-29	30-50	that contrary to the common belief	that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods ,	21-77	21-77	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	1<2	elab-addition	elab-addition
P16-1146	1-5	51-54	We describe a search algorithm	parsing results significantly improve	1-20	21-77	We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods .	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	1<2	evaluation	evaluation
P16-1146	51-54	55-67	parsing results significantly improve	if the number of latent states for each nonterminal is globally optimized ,	21-77	21-77	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	1<2	condition	condition
P16-1146	55-67	68-77	if the number of latent states for each nonterminal is globally optimized ,	while taking into account interactions between the different nonterminals .	21-77	21-77	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods , parsing results significantly improve if the number of latent states for each nonterminal is globally optimized , while taking into account interactions between the different nonterminals .	1<2	joint	joint
P16-1146	78-94	114-129	In addition , we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages :	that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	78-110	111-129	In addition , we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages : Basque , French , German , Hebrew , Hungarian , Korean , Polish and Swedish .	Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	1>2	result	result
P16-1146	78-94	95-110	In addition , we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages :	Basque , French , German , Hebrew , Hungarian , Korean , Polish and Swedish .	78-110	78-110	In addition , we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages : Basque , French , German , Hebrew , Hungarian , Korean , Polish and Swedish .	In addition , we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages : Basque , French , German , Hebrew , Hungarian , Korean , Polish and Swedish .	1<2	elab-enumember	elab-enumember
P16-1146	111-113	114-129	Our results show	that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	111-129	111-129	Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	1>2	attribution	attribution
P16-1146	1-5	114-129	We describe a search algorithm	that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	1-20	111-129	We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods .	Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages .	1<2	evaluation	evaluation
